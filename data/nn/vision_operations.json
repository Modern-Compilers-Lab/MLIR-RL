[["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<2240xf32>) outs(%6 : tensor<666x1x1x2240xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x2240xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<2240xf32>) outs(%6 : tensor<666x1x1x2240xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x2240xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<2240xf32>, %6: tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<2240xf32>) outs(%6 : tensor<666x1x1x2240xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x2240xf32>\n  return %ret : tensor<666x1x1x2240xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<2240xf32>, %arg1: tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<2240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x2240xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 2240 {\n        %2 = affine.load %0[%arg3] : memref<2240xf32>\n        affine.store %2, %alloc[%arg2, %arg3] : memref<666x2240xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x2240xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1, 2], [3]] : tensor<666x2240xf32> into tensor<666x1x1x2240xf32>\n    return %expanded : tensor<666x1x1x2240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x2240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<2240xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<2240xf32>) -> tensor<2240xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x1x1x2240xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<2240xf32>) outs(%6 : tensor<666x1x1x2240xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x2240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x2240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x2240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 2240, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1x1x224xf32>, %3: tensor<2240x1x1x224xf32>, %7: tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>\n  return %ret : tensor<666x1x1x2240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x1x1x224xf32>, %arg1: tensor<2240x1x1x224xf32>, %arg2: tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<2240x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x1x1x2240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x1x1x2240xf32>\n    memref.copy %2, %alloc : memref<666x1x1x2240xf32> to memref<666x1x1x2240xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 2240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<2240x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x1x1x2240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x1x1x2240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x1x1x2240xf32>\n    return %3 : tensor<666x1x1x2240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x2240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1x1x224xf32>) -> tensor<666x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<2240x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<2240x1x1x224xf32>) -> tensor<2240x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x1x1x2240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<666x1x1x2240xf32>) -> tensor<666x1x1x2240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x2240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x2240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 2240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x26xf32>) outs(%2 : tensor<666x1x1x26xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x1x1x26xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x26xf32>) outs(%2 : tensor<666x1x1x26xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1x1x26xf32>, %2: tensor<666x1x1x26xf32>) -> tensor<666x1x1x26xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x26xf32>) outs(%2 : tensor<666x1x1x26xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x1x1x26xf32>\n  return %ret : tensor<666x1x1x26xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x1x1x26xf32>, %arg1: tensor<666x1x1x26xf32>) -> tensor<666x1x1x26xf32> {\n    %cst = arith.constant 3.40282347E+38 : f32\n    %cst_0 = arith.constant 0.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1, 2], [3]] : tensor<666x1x1x26xf32> into tensor<666x26xf32>\n    %0 = bufferization.to_memref %collapsed : memref<666x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x26xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 26 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<666x26xf32>\n        %3 = arith.minf %2, %cst : f32\n        %4 = arith.maxf %3, %cst_0 : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<666x26xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x26xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1, 2], [3]] : tensor<666x26xf32> into tensor<666x1x1x26xf32>\n    return %expanded : tensor<666x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1x1x26xf32>) -> tensor<666x1x1x26xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x1x1x26xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x1x1x26xf32>) -> tensor<666x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x26xf32>) outs(%2 : tensor<666x1x1x26xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 26, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x1728xf32>, tensor<1x1x1x1728xf32>) outs(%4 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x1728xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x1728xf32>, tensor<1x1x1x1728xf32>) outs(%4 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x1728xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x7x7x1728xf32>, %3: tensor<1x1x1x1728xf32>, %4: tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x1728xf32>, tensor<1x1x1x1728xf32>) outs(%4 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x1728xf32>\n  return %ret : tensor<666x7x7x1728xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x7x7x1728xf32>, %arg1: tensor<1x1x1x1728xf32>, %arg2: tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x7x7x1728xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x1728xf32> into tensor<1728xf32>\n    %1 = bufferization.to_memref %collapsed : memref<1728xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x1728xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1728 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x1728xf32>\n            %4 = affine.load %1[%arg6] : memref<1728xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x1728xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x7x7x1728xf32>\n    return %2 : tensor<666x7x7x1728xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x1728xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x7x7x1728xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x1728xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x1728xf32>) -> tensor<1x1x1x1728xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x7x7x1728xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x1728xf32>, tensor<1x1x1x1728xf32>) outs(%4 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x1728xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x1728xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x1728xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1728, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x1xf32>) outs(%2 : tensor<666x56x56x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x56x56x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x1xf32>) outs(%2 : tensor<666x56x56x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x56x56x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x1xf32>, %2: tensor<666x56x56x1xf32>) -> tensor<666x56x56x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x1xf32>) outs(%2 : tensor<666x56x56x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x56x56x1xf32>\n  return %ret : tensor<666x56x56x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x1xf32>, %arg1: tensor<666x56x56x1xf32>) -> tensor<666x56x56x1xf32> {\n    %collapsed = tensor.collapse_shape %arg0 [[0], [1], [2, 3]] : tensor<666x56x56x1xf32> into tensor<666x56x56xf32>\n    %0 = bufferization.to_memref %collapsed : memref<666x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 56 {\n        affine.for %arg4 = 0 to 56 {\n          %2 = affine.load %0[%arg2, %arg3, %arg4] : memref<666x56x56xf32>\n          %3 = math.rsqrt %2 : f32\n          affine.store %3, %alloc[%arg2, %arg3, %arg4] : memref<666x56x56xf32>\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x56x56xf32>\n    %expanded = tensor.expand_shape %1 [[0], [1], [2, 3]] : tensor<666x56x56xf32> into tensor<666x56x56x1xf32>\n    return %expanded : tensor<666x56x56x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x1xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x1xf32>) -> tensor<666x56x56x1xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x56x56x1xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x56x56x1xf32>) -> tensor<666x56x56x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, 0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x1xf32>) outs(%2 : tensor<666x56x56x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x56x56x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 56, 1], ["%arg4", 0, 56, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4"]], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<512xf32>) outs(%6 : tensor<666x28x28x512xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x512xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<512xf32>) outs(%6 : tensor<666x28x28x512xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x512xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<512xf32>, %6: tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<512xf32>) outs(%6 : tensor<666x28x28x512xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x512xf32>\n  return %ret : tensor<666x28x28x512xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<512xf32>, %arg1: tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x512xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 28 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 512 {\n            %2 = affine.load %0[%arg5] : memref<512xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x28x28x512xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x28x28x512xf32>\n    return %1 : tensor<666x28x28x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<512xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<512xf32>) -> tensor<512xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x28x28x512xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<512xf32>) outs(%6 : tensor<666x28x28x512xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 512, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x28x28x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x28x28x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x28x28x512xf32>, %3: tensor<512x1x1x512xf32>, %7: tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x28x28x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>\n  return %ret : tensor<666x28x28x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x28x28x512xf32>, %arg1: tensor<512x1x1x512xf32>, %arg2: tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x28x28x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x28x28x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x512xf32>\n    memref.copy %2, %alloc : memref<666x28x28x512xf32> to memref<666x28x28x512xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x28x28x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x28x28x512xf32>\n    return %3 : tensor<666x28x28x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x28x28x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x28x28x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x28x28x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<666x28x28x512xf32>) -> tensor<666x28x28x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x42x42x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x168xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x42x42x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x42x42x168xf32>, %3: tensor<1x1x1x168xf32>, %4: tensor<666x42x42x168xf32>) -> tensor<666x42x42x168xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x42x42x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x168xf32>\n  return %ret : tensor<666x42x42x168xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x42x42x168xf32>, %arg1: tensor<1x1x1x168xf32>, %arg2: tensor<666x42x42x168xf32>) -> tensor<666x42x42x168xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x42x42x168xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x168xf32> into tensor<168xf32>\n    %1 = bufferization.to_memref %collapsed : memref<168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x42x42x168xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 168 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x42x42x168xf32>\n            %4 = affine.load %1[%arg6] : memref<168xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x42x42x168xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x42x42x168xf32>\n    return %2 : tensor<666x42x42x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x42x42x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x42x42x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x42x42x168xf32>) -> tensor<666x42x42x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x168xf32>) -> tensor<1x1x1x168xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x42x42x168xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x42x42x168xf32>) -> tensor<666x42x42x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x42x42x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x42x42x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x42x42x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x11xf32>, tensor<666x56x56x11xf32>) outs(%4 : tensor<666x56x56x11xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x11xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x11xf32>, tensor<666x56x56x11xf32>) outs(%4 : tensor<666x56x56x11xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x11xf32>, %3: tensor<666x56x56x11xf32>, %4: tensor<666x56x56x11xf32>) -> tensor<666x56x56x11xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x11xf32>, tensor<666x56x56x11xf32>) outs(%4 : tensor<666x56x56x11xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x11xf32>\n  return %ret : tensor<666x56x56x11xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x11xf32>, %arg1: tensor<666x56x56x11xf32>, %arg2: tensor<666x56x56x11xf32>) -> tensor<666x56x56x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<666x56x56x11xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x56x56x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x11xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            %3 = affine.load %1[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x11xf32>\n            %4 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x11xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x11xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x56x56x11xf32>\n    return %2 : tensor<666x56x56x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x11xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x11xf32>) -> tensor<666x56x56x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<666x56x56x11xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<666x56x56x11xf32>) -> tensor<666x56x56x11xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x56x56x11xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x56x56x11xf32>) -> tensor<666x56x56x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x11xf32>, tensor<666x56x56x11xf32>) outs(%4 : tensor<666x56x56x11xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1536xf32>, tensor<666x1536xf32>) outs(%4 : tensor<666x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1536xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1536xf32>, tensor<666x1536xf32>) outs(%4 : tensor<666x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1536xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1536xf32>, %3: tensor<666x1536xf32>, %4: tensor<666x1536xf32>) -> tensor<666x1536xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1536xf32>, tensor<666x1536xf32>) outs(%4 : tensor<666x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1536xf32>\n  return %ret : tensor<666x1536xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x1536xf32>, %arg1: tensor<666x1536xf32>, %arg2: tensor<666x1536xf32>) -> tensor<666x1536xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<666x1536xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x1536xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x1536xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 1536 {\n        %3 = affine.load %1[%arg3, %arg4] : memref<666x1536xf32>\n        %4 = affine.load %0[%arg3, %arg4] : memref<666x1536xf32>\n        %5 = arith.addf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<666x1536xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x1536xf32>\n    return %2 : tensor<666x1536xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1536xf32>) -> tensor<666x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<666x1536xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<666x1536xf32>) -> tensor<666x1536xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x1536xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x1536xf32>) -> tensor<666x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1536xf32>, tensor<666x1536xf32>) outs(%4 : tensor<666x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1536xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<12xf32>) outs(%6 : tensor<666x1x1x12xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x12xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<12xf32>) outs(%6 : tensor<666x1x1x12xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<12xf32>, %6: tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<12xf32>) outs(%6 : tensor<666x1x1x12xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x12xf32>\n  return %ret : tensor<666x1x1x12xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<12xf32>, %arg1: tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x12xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 12 {\n        %2 = affine.load %0[%arg3] : memref<12xf32>\n        affine.store %2, %alloc[%arg2, %arg3] : memref<666x12xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x12xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1, 2], [3]] : tensor<666x12xf32> into tensor<666x1x1x12xf32>\n    return %expanded : tensor<666x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<12xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<12xf32>) -> tensor<12xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x1x1x12xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<12xf32>) outs(%6 : tensor<666x1x1x12xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 12, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1x1x48xf32>, %3: tensor<12x1x1x48xf32>, %7: tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>\n  return %ret : tensor<666x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x1x1x48xf32>, %arg1: tensor<12x1x1x48xf32>, %arg2: tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x1x1x12xf32>\n    memref.copy %2, %alloc : memref<666x1x1x12xf32> to memref<666x1x1x12xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x1x1x12xf32>\n    return %3 : tensor<666x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1x1x48xf32>) -> tensor<666x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x48xf32>) -> tensor<12x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<666x1x1x12xf32>) -> tensor<666x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x2048xf32>, tensor<1x1xf32>) outs(%4 : tensor<666x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x2048xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x2048xf32>, tensor<1x1xf32>) outs(%4 : tensor<666x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x2048xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x2048xf32>, %3: tensor<1x1xf32>, %4: tensor<666x2048xf32>) -> tensor<666x2048xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x2048xf32>, tensor<1x1xf32>) outs(%4 : tensor<666x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x2048xf32>\n  return %ret : tensor<666x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x2048xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<666x2048xf32>) -> tensor<666x2048xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x2048xf32>\n    %collapsed = tensor.collapse_shape %arg1 [] : tensor<1x1xf32> into tensor<f32>\n    %1 = bufferization.to_memref %collapsed : memref<f32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x2048xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 2048 {\n        %3 = affine.load %0[%arg3, %arg4] : memref<666x2048xf32>\n        %4 = affine.load %1[] : memref<f32>\n        %5 = arith.mulf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<666x2048xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x2048xf32>\n    return %2 : tensor<666x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x2048xf32>) -> tensor<666x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x2048xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x2048xf32>) -> tensor<666x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x2048xf32>, tensor<1x1xf32>) outs(%4 : tensor<666x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 2048, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], []], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x120xf32>, tensor<1x1x1x120xf32>) outs(%4 : tensor<666x56x56x120xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x120xf32>, tensor<1x1x1x120xf32>) outs(%4 : tensor<666x56x56x120xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x120xf32>, %3: tensor<1x1x1x120xf32>, %4: tensor<666x56x56x120xf32>) -> tensor<666x56x56x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x120xf32>, tensor<1x1x1x120xf32>) outs(%4 : tensor<666x56x56x120xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x120xf32>\n  return %ret : tensor<666x56x56x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x120xf32>, %arg1: tensor<1x1x1x120xf32>, %arg2: tensor<666x56x56x120xf32>) -> tensor<666x56x56x120xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x56x56x120xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x120xf32> into tensor<120xf32>\n    %1 = bufferization.to_memref %collapsed : memref<120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x120xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 120 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x120xf32>\n            %4 = affine.load %1[%arg6] : memref<120xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x120xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x56x56x120xf32>\n    return %2 : tensor<666x56x56x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x120xf32>) -> tensor<666x56x56x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x120xf32>) -> tensor<1x1x1x120xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x56x56x120xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x56x56x120xf32>) -> tensor<666x56x56x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x120xf32>, tensor<1x1x1x120xf32>) outs(%4 : tensor<666x56x56x120xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x28x28x896xf32>, tensor<1x1x1x896xf32>) outs(%4 : tensor<666x28x28x896xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x28x28x896xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x28x28x896xf32>, tensor<1x1x1x896xf32>) outs(%4 : tensor<666x28x28x896xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x28x28x896xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x28x28x896xf32>, %3: tensor<1x1x1x896xf32>, %4: tensor<666x28x28x896xf32>) -> tensor<666x28x28x896xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x28x28x896xf32>, tensor<1x1x1x896xf32>) outs(%4 : tensor<666x28x28x896xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x28x28x896xf32>\n  return %ret : tensor<666x28x28x896xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x28x28x896xf32>, %arg1: tensor<1x1x1x896xf32>, %arg2: tensor<666x28x28x896xf32>) -> tensor<666x28x28x896xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x28x28x896xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x896xf32> into tensor<896xf32>\n    %1 = bufferization.to_memref %collapsed : memref<896xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x896xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 896 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x896xf32>\n            %4 = affine.load %1[%arg6] : memref<896xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x896xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x28x28x896xf32>\n    return %2 : tensor<666x28x28x896xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x896xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x28x28x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x28x28x896xf32>) -> tensor<666x28x28x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x896xf32>) -> tensor<1x1x1x896xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x28x28x896xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x28x28x896xf32>) -> tensor<666x28x28x896xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x28x28x896xf32>, tensor<1x1x1x896xf32>) outs(%4 : tensor<666x28x28x896xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x28x28x896xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x896xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x896xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x35x35x256xf32>, tensor<1x1x1x256xf32>) outs(%4 : tensor<666x35x35x256xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x35x35x256xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x35x35x256xf32>, tensor<1x1x1x256xf32>) outs(%4 : tensor<666x35x35x256xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x35x35x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x35x35x256xf32>, %3: tensor<1x1x1x256xf32>, %4: tensor<666x35x35x256xf32>) -> tensor<666x35x35x256xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x35x35x256xf32>, tensor<1x1x1x256xf32>) outs(%4 : tensor<666x35x35x256xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x35x35x256xf32>\n  return %ret : tensor<666x35x35x256xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x35x35x256xf32>, %arg1: tensor<1x1x1x256xf32>, %arg2: tensor<666x35x35x256xf32>) -> tensor<666x35x35x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x35x35x256xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x256xf32> into tensor<256xf32>\n    %1 = bufferization.to_memref %collapsed : memref<256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x35x35x256xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 256 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x35x35x256xf32>\n            %4 = affine.load %1[%arg6] : memref<256xf32>\n            %5 = arith.mulf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x35x35x256xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x35x35x256xf32>\n    return %2 : tensor<666x35x35x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x35x35x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x35x35x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x35x35x256xf32>) -> tensor<666x35x35x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x256xf32>) -> tensor<1x1x1x256xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x35x35x256xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x35x35x256xf32>) -> tensor<666x35x35x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x35x35x256xf32>, tensor<1x1x1x256xf32>) outs(%4 : tensor<666x35x35x256xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x35x35x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x35x35x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x35x35x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 256, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x112xf32>, tensor<1x1x1x112xf32>) outs(%4 : tensor<666x56x56x112xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x112xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x112xf32>, tensor<1x1x1x112xf32>) outs(%4 : tensor<666x56x56x112xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x112xf32>, %3: tensor<1x1x1x112xf32>, %4: tensor<666x56x56x112xf32>) -> tensor<666x56x56x112xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x112xf32>, tensor<1x1x1x112xf32>) outs(%4 : tensor<666x56x56x112xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x112xf32>\n  return %ret : tensor<666x56x56x112xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x112xf32>, %arg1: tensor<1x1x1x112xf32>, %arg2: tensor<666x56x56x112xf32>) -> tensor<666x56x56x112xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x56x56x112xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x112xf32> into tensor<112xf32>\n    %1 = bufferization.to_memref %collapsed : memref<112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x112xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 112 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x112xf32>\n            %4 = affine.load %1[%arg6] : memref<112xf32>\n            %5 = arith.subf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x112xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x56x56x112xf32>\n    return %2 : tensor<666x56x56x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x112xf32>) -> tensor<666x56x56x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x112xf32>) -> tensor<1x1x1x112xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x56x56x112xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x56x56x112xf32>) -> tensor<666x56x56x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x112xf32>, tensor<1x1x1x112xf32>) outs(%4 : tensor<666x56x56x112xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 112, 1]], "op_count": {"+": 0, "-": 1, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x28x28x2048xf32>) outs(%2 : tensor<666x28x28x2048xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.erf %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x28x28x2048xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x28x28x2048xf32>) outs(%2 : tensor<666x28x28x2048xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.erf %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x28x28x2048xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x28x28x2048xf32>, %2: tensor<666x28x28x2048xf32>) -> tensor<666x28x28x2048xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x28x28x2048xf32>) outs(%2 : tensor<666x28x28x2048xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.erf %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x28x28x2048xf32>\n  return %ret : tensor<666x28x28x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x28x28x2048xf32>, %arg1: tensor<666x28x28x2048xf32>) -> tensor<666x28x28x2048xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x28x28x2048xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x2048xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 28 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 2048 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<666x28x28x2048xf32>\n            %3 = math.erf %2 : f32\n            affine.store %3, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x28x28x2048xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x28x28x2048xf32>\n    return %1 : tensor<666x28x28x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x28x28x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x28x28x2048xf32>) -> tensor<666x28x28x2048xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x28x28x2048xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x28x28x2048xf32>) -> tensor<666x28x28x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x28x28x2048xf32>) outs(%2 : tensor<666x28x28x2048xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.erf %in : f32\n  linalg.yield %4 : f32\n} -> tensor<666x28x28x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 2048, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<42xf32>) outs(%6 : tensor<666x83x83x42xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x83x83x42xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<42xf32>) outs(%6 : tensor<666x83x83x42xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<42xf32>, %6: tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<42xf32>) outs(%6 : tensor<666x83x83x42xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x83x83x42xf32>\n  return %ret : tensor<666x83x83x42xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<42xf32>, %arg1: tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x83x83x42xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 83 {\n        affine.for %arg4 = 0 to 83 {\n          affine.for %arg5 = 0 to 42 {\n            %2 = affine.load %0[%arg5] : memref<42xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x83x83x42xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x83x83x42xf32>\n    return %1 : tensor<666x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<42xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<42xf32>) -> tensor<42xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x83x83x42xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<42xf32>) outs(%6 : tensor<666x83x83x42xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 83, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 42, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x83x83x42xf32>, %3: tensor<42x1x1x42xf32>, %7: tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>\n  return %ret : tensor<666x83x83x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x83x83x42xf32>, %arg1: tensor<42x1x1x42xf32>, %arg2: tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x83x83x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x83x83x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x83x83x42xf32>\n    memref.copy %2, %alloc : memref<666x83x83x42xf32> to memref<666x83x83x42xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x83x83x42xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x83x83x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x83x83x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x83x83x42xf32>\n    return %3 : tensor<666x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x83x83x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x42xf32>) -> tensor<42x1x1x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x83x83x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<666x83x83x42xf32>) -> tensor<666x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1024xf32>, tensor<666x1024xf32>) outs(%4 : tensor<666x1024xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1024xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1024xf32>, tensor<666x1024xf32>) outs(%4 : tensor<666x1024xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1024xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1024xf32>, %3: tensor<666x1024xf32>, %4: tensor<666x1024xf32>) -> tensor<666x1024xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1024xf32>, tensor<666x1024xf32>) outs(%4 : tensor<666x1024xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1024xf32>\n  return %ret : tensor<666x1024xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x1024xf32>, %arg1: tensor<666x1024xf32>, %arg2: tensor<666x1024xf32>) -> tensor<666x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<666x1024xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x1024xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 1024 {\n        %3 = affine.load %1[%arg3, %arg4] : memref<666x1024xf32>\n        %4 = affine.load %0[%arg3, %arg4] : memref<666x1024xf32>\n        %5 = arith.mulf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<666x1024xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x1024xf32>\n    return %2 : tensor<666x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1024xf32>) -> tensor<666x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<666x1024xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<666x1024xf32>) -> tensor<666x1024xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x1024xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x1024xf32>) -> tensor<666x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x1024xf32>, tensor<666x1024xf32>) outs(%4 : tensor<666x1024xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 1024, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x7x7x1728xf32>) outs(%2 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x7x7x1728xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x7x7x1728xf32>) outs(%2 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x7x7x1728xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x7x7x1728xf32>, %2: tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x7x7x1728xf32>) outs(%2 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x7x7x1728xf32>\n  return %ret : tensor<666x7x7x1728xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x7x7x1728xf32>, %arg1: tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %cst_0 = arith.constant 3.40282347E+38 : f32\n    %0 = bufferization.to_memref %arg0 : memref<666x7x7x1728xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x1728xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 7 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 1728 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<666x7x7x1728xf32>\n            %3 = arith.minf %2, %cst_0 : f32\n            %4 = arith.maxf %3, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x7x7x1728xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x7x7x1728xf32>\n    return %1 : tensor<666x7x7x1728xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x1728xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x7x7x1728xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x7x7x1728xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x7x7x1728xf32>) -> tensor<666x7x7x1728xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x7x7x1728xf32>) outs(%2 : tensor<666x7x7x1728xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x7x7x1728xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x1728xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x1728xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 7, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 1728, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.fill ins(%cst_1 : f32) outs(%6 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>", {"operation": "linalg.fill ins(%cst_1 : f32) outs(%6 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%cst_1: f32, %6: tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32> {\n  %ret = linalg.fill ins(%cst_1 : f32) outs(%6 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n  return %ret : tensor<666x7x7x176x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x176x1xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 7 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 176 {\n            affine.for %arg6 = 0 to 1 {\n              affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5, %arg6] : memref<666x7x7x176x1xf32>\n            }\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<666x7x7x176x1xf32>\n    return %0 : tensor<666x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_1 = arith.constant 2.00000e+00 : f32\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x7x7x176x1xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_1 : f32) outs(%6 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 7, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 176, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<666x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<666x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<666x11x11x176xf32>, %3: tensor<5x5x176x1xf32>, %7: tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<666x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n  return %ret : tensor<666x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x11x11x176xf32>, %arg1: tensor<5x5x176x1xf32>, %arg2: tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x11x11x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<666x7x7x176x1xf32> to memref<666x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<666x11x11x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<666x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<666x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x7x7x176x1xf32>\n    return %3 : tensor<666x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<666x11x11x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<666x11x11x176xf32>) -> tensor<666x11x11x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x176x1xf32>) -> tensor<5x5x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<666x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<666x7x7x176x1xf32>) -> tensor<666x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<176xf32>, tensor<666x7x7x176xf32>) outs(%8 : tensor<666x7x7x176xf32>) {\n^bb0(%in: f32, %in_2: f32, %out: f32):\n  %11 = arith.addf %in, %in_2 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x7x7x176xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<176xf32>, tensor<666x7x7x176xf32>) outs(%8 : tensor<666x7x7x176xf32>) {\n^bb0(%in: f32, %in_2: f32, %out: f32):\n  %11 = arith.addf %in, %in_2 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<176xf32>, %collapsed: tensor<666x7x7x176xf32>, %8: tensor<666x7x7x176xf32>) -> tensor<666x7x7x176xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<176xf32>, tensor<666x7x7x176xf32>) outs(%8 : tensor<666x7x7x176xf32>) {\n^bb0(%in: f32, %in_2: f32, %out: f32):\n  %11 = arith.addf %in, %in_2 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x7x7x176xf32>\n  return %ret : tensor<666x7x7x176xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<176xf32>, %arg1: tensor<666x7x7x176xf32>, %arg2: tensor<666x7x7x176xf32>) -> tensor<666x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<666x7x7x176xf32>\n    %1 = bufferization.to_memref %arg0 : memref<176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x176xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            %3 = affine.load %1[%arg6] : memref<176xf32>\n            %4 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x176xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x176xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x7x7x176xf32>\n    return %2 : tensor<666x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<176xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<176xf32>) -> tensor<176xf32>\n%tmp_collapsed = bufferization.alloc_tensor() : tensor<666x7x7x176xf32>\n%collapsed = linalg.fill ins(%val : f32) outs(%tmp_collapsed : tensor<666x7x7x176xf32>) -> tensor<666x7x7x176xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<666x7x7x176xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<666x7x7x176xf32>) -> tensor<666x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<176xf32>, tensor<666x7x7x176xf32>) outs(%8 : tensor<666x7x7x176xf32>) {\n^bb0(%in: f32, %in_2: f32, %out: f32):\n  %11 = arith.addf %in, %in_2 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %6: tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n  return %ret : tensor<666x56x56x96x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x96x1xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 56 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 96 {\n            affine.for %arg6 = 0 to 1 {\n              affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5, %arg6] : memref<666x56x56x96x1xf32>\n            }\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<666x56x56x96x1xf32>\n    return %0 : tensor<666x56x56x96x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x96x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x56x56x96x1xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x96x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x96x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 56, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 96, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x113x113x96xf32>, %3: tensor<3x3x96x1xf32>, %7: tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n  return %ret : tensor<666x56x56x96x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x113x113x96xf32>, %arg1: tensor<3x3x96x1xf32>, %arg2: tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x96x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x113x113x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x56x56x96x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x96x1xf32>\n    memref.copy %2, %alloc : memref<666x56x56x96x1xf32> to memref<666x56x56x96x1xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<666x113x113x96xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x96x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<666x56x56x96x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<666x56x56x96x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x56x56x96x1xf32>\n    return %3 : tensor<666x56x56x96x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x96x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x113x113x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x113x113x96xf32>) -> tensor<666x113x113x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x96x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x96x1xf32>) -> tensor<3x3x96x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x56x56x96x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<666x56x56x96x1xf32>) -> tensor<666x56x56x96x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x96x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x96x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<96xf32>, tensor<666x56x56x96xf32>) outs(%8 : tensor<666x56x56x96xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %11 = arith.addf %in, %in_1 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x56x56x96xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<96xf32>, tensor<666x56x56x96xf32>) outs(%8 : tensor<666x56x56x96xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %11 = arith.addf %in, %in_1 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<96xf32>, %collapsed: tensor<666x56x56x96xf32>, %8: tensor<666x56x56x96xf32>) -> tensor<666x56x56x96xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<96xf32>, tensor<666x56x56x96xf32>) outs(%8 : tensor<666x56x56x96xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %11 = arith.addf %in, %in_1 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x56x56x96xf32>\n  return %ret : tensor<666x56x56x96xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<96xf32>, %arg1: tensor<666x56x56x96xf32>, %arg2: tensor<666x56x56x96xf32>) -> tensor<666x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<666x56x56x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x96xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            %3 = affine.load %1[%arg6] : memref<96xf32>\n            %4 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x96xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x96xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x56x56x96xf32>\n    return %2 : tensor<666x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<96xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<96xf32>) -> tensor<96xf32>\n%tmp_collapsed = bufferization.alloc_tensor() : tensor<666x56x56x96xf32>\n%collapsed = linalg.fill ins(%val : f32) outs(%tmp_collapsed : tensor<666x56x56x96xf32>) -> tensor<666x56x56x96xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<666x56x56x96xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<666x56x56x96xf32>) -> tensor<666x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %collapsed : tensor<96xf32>, tensor<666x56x56x96xf32>) outs(%8 : tensor<666x56x56x96xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %11 = arith.addf %in, %in_1 : f32\n  linalg.yield %11 : f32\n} -> tensor<666x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x104xf32>) outs(%2 : tensor<666x56x56x104xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x56x56x104xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x104xf32>) outs(%2 : tensor<666x56x56x104xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x56x56x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x104xf32>, %2: tensor<666x56x56x104xf32>) -> tensor<666x56x56x104xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x104xf32>) outs(%2 : tensor<666x56x56x104xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x56x56x104xf32>\n  return %ret : tensor<666x56x56x104xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x104xf32>, %arg1: tensor<666x56x56x104xf32>) -> tensor<666x56x56x104xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %cst_0 = arith.constant 3.40282347E+38 : f32\n    %0 = bufferization.to_memref %arg0 : memref<666x56x56x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x104xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 56 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 104 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<666x56x56x104xf32>\n            %3 = arith.minf %2, %cst_0 : f32\n            %4 = arith.maxf %3, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x56x56x104xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x56x56x104xf32>\n    return %1 : tensor<666x56x56x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x104xf32>) -> tensor<666x56x56x104xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x56x56x104xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x56x56x104xf32>) -> tensor<666x56x56x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x56x56x104xf32>) outs(%2 : tensor<666x56x56x104xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 0.000000e+00 : f32\n  %cst_1 = arith.constant 3.40282347E+38 : f32\n  %4 = arith.minf %in, %cst_1 : f32\n  %5 = arith.maxf %4, %cst_0 : f32\n  linalg.yield %5 : f32\n} -> tensor<666x56x56x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 56, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 104, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<128xf32>, %6: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>\n  return %ret : tensor<666x14x14x128xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128xf32>, %arg1: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x14x14x128xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 14 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 128 {\n            %2 = affine.load %0[%arg5] : memref<128xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x14x14x128xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x14x14x128xf32>\n    return %1 : tensor<666x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<128xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<128xf32>) -> tensor<128xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x14x14x128xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 14, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 128, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x14x14x480xf32>, %3: tensor<128x1x1x480xf32>, %7: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n  return %ret : tensor<666x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x14x14x480xf32>, %arg1: tensor<128x1x1x480xf32>, %arg2: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x480xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x14x14x480xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x14x14x128xf32>\n    memref.copy %2, %alloc : memref<666x14x14x128xf32> to memref<666x14x14x128xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 480 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x14x14x480xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x480xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x14x14x128xf32>\n    return %3 : tensor<666x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x14x14x480xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x14x14x480xf32>) -> tensor<666x14x14x480xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x480xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x480xf32>) -> tensor<128x1x1x480xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 480, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<672xf32>) outs(%6 : tensor<666x56x56x672xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x56x56x672xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<672xf32>) outs(%6 : tensor<666x56x56x672xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x56x56x672xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<672xf32>, %6: tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<672xf32>) outs(%6 : tensor<666x56x56x672xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x56x56x672xf32>\n  return %ret : tensor<666x56x56x672xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<672xf32>, %arg1: tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<672xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x672xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 56 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 672 {\n            %2 = affine.load %0[%arg5] : memref<672xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x56x56x672xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x56x56x672xf32>\n    return %1 : tensor<666x56x56x672xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x672xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<672xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<672xf32>) -> tensor<672xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x56x56x672xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<672xf32>) outs(%6 : tensor<666x56x56x672xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x56x56x672xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x672xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x672xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 56, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 672, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x336xf32>, tensor<672x1x1x336xf32>) outs(%7 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x336xf32>, tensor<672x1x1x336xf32>) outs(%7 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x336xf32>, %3: tensor<672x1x1x336xf32>, %7: tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x336xf32>, tensor<672x1x1x336xf32>) outs(%7 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>\n  return %ret : tensor<666x56x56x672xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x56x56x336xf32>, %arg1: tensor<672x1x1x336xf32>, %arg2: tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<672x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x56x56x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x56x56x672xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x672xf32>\n    memref.copy %2, %alloc : memref<666x56x56x672xf32> to memref<666x56x56x672xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x56x56x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<672x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x672xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x672xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x56x56x672xf32>\n    return %3 : tensor<666x56x56x672xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x672xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x336xf32>) -> tensor<666x56x56x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<672x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<672x1x1x336xf32>) -> tensor<672x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x56x56x672xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x336xf32>, tensor<672x1x1x336xf32>) outs(%7 : tensor<666x56x56x672xf32>) -> tensor<666x56x56x672xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x672xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x672xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x14x14x88xf32>, tensor<1x1x1x88xf32>) outs(%4 : tensor<666x14x14x88xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x14x14x88xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x14x14x88xf32>, tensor<1x1x1x88xf32>) outs(%4 : tensor<666x14x14x88xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x14x14x88xf32>, %3: tensor<1x1x1x88xf32>, %4: tensor<666x14x14x88xf32>) -> tensor<666x14x14x88xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x14x14x88xf32>, tensor<1x1x1x88xf32>) outs(%4 : tensor<666x14x14x88xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x14x14x88xf32>\n  return %ret : tensor<666x14x14x88xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x14x14x88xf32>, %arg1: tensor<1x1x1x88xf32>, %arg2: tensor<666x14x14x88xf32>) -> tensor<666x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x14x14x88xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x88xf32> into tensor<88xf32>\n    %1 = bufferization.to_memref %collapsed : memref<88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x14x14x88xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x88xf32>\n            %4 = affine.load %1[%arg6] : memref<88xf32>\n            %5 = arith.subf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x88xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x14x14x88xf32>\n    return %2 : tensor<666x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x14x14x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x14x14x88xf32>) -> tensor<666x14x14x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x88xf32>) -> tensor<1x1x1x88xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x14x14x88xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x14x14x88xf32>) -> tensor<666x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x14x14x88xf32>, tensor<1x1x1x88xf32>) outs(%4 : tensor<666x14x14x88xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1]], "op_count": {"+": 0, "-": 1, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<384xf32>) outs(%6 : tensor<666x28x28x384xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x384xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<384xf32>) outs(%6 : tensor<666x28x28x384xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x384xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<384xf32>, %6: tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<384xf32>) outs(%6 : tensor<666x28x28x384xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x384xf32>\n  return %ret : tensor<666x28x28x384xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<384xf32>, %arg1: tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x384xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 28 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 384 {\n            %2 = affine.load %0[%arg5] : memref<384xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x28x28x384xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x28x28x384xf32>\n    return %1 : tensor<666x28x28x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<384xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<384xf32>) -> tensor<384xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x28x28x384xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<384xf32>) outs(%6 : tensor<666x28x28x384xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x28x28x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 384, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x192xf32>, tensor<384x2x2x192xf32>) outs(%7 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x192xf32>, tensor<384x2x2x192xf32>) outs(%7 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x192xf32>, %3: tensor<384x2x2x192xf32>, %7: tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x192xf32>, tensor<384x2x2x192xf32>) outs(%7 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>\n  return %ret : tensor<666x28x28x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x56x56x192xf32>, %arg1: tensor<384x2x2x192xf32>, %arg2: tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x2x2x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x56x56x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x28x28x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x28x28x384xf32>\n    memref.copy %2, %alloc : memref<666x28x28x384xf32> to memref<666x28x28x384xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x56x56x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x2x2x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x28x28x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x28x28x384xf32>\n    return %3 : tensor<666x28x28x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x28x28x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x192xf32>) -> tensor<666x56x56x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x2x2x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x2x2x192xf32>) -> tensor<384x2x2x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x28x28x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<666x56x56x192xf32>, tensor<384x2x2x192xf32>) outs(%7 : tensor<666x28x28x384xf32>) -> tensor<666x28x28x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x28x28x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x28x28x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x8x8x1536xf32>, tensor<1x1x1x1536xf32>) outs(%4 : tensor<666x8x8x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x8x8x1536xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x8x8x1536xf32>, tensor<1x1x1x1536xf32>) outs(%4 : tensor<666x8x8x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x8x8x1536xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x8x8x1536xf32>, %3: tensor<1x1x1x1536xf32>, %4: tensor<666x8x8x1536xf32>) -> tensor<666x8x8x1536xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x8x8x1536xf32>, tensor<1x1x1x1536xf32>) outs(%4 : tensor<666x8x8x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x8x8x1536xf32>\n  return %ret : tensor<666x8x8x1536xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x8x8x1536xf32>, %arg1: tensor<1x1x1x1536xf32>, %arg2: tensor<666x8x8x1536xf32>) -> tensor<666x8x8x1536xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x8x8x1536xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x1536xf32> into tensor<1536xf32>\n    %1 = bufferization.to_memref %collapsed : memref<1536xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x8x8x1536xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 1536 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x8x8x1536xf32>\n            %4 = affine.load %1[%arg6] : memref<1536xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x8x8x1536xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x8x8x1536xf32>\n    return %2 : tensor<666x8x8x1536xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x8x8x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x8x8x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x8x8x1536xf32>) -> tensor<666x8x8x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x1536xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x1536xf32>) -> tensor<1x1x1x1536xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x8x8x1536xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x8x8x1536xf32>) -> tensor<666x8x8x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x8x8x1536xf32>, tensor<1x1x1x1536xf32>) outs(%4 : tensor<666x8x8x1536xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x8x8x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x8x8x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x8x8x1536xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x2048xf32>, tensor<1x1x1x2048xf32>) outs(%4 : tensor<666x7x7x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x2048xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x2048xf32>, tensor<1x1x1x2048xf32>) outs(%4 : tensor<666x7x7x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x2048xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x7x7x2048xf32>, %3: tensor<1x1x1x2048xf32>, %4: tensor<666x7x7x2048xf32>) -> tensor<666x7x7x2048xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x2048xf32>, tensor<1x1x1x2048xf32>) outs(%4 : tensor<666x7x7x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x2048xf32>\n  return %ret : tensor<666x7x7x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x7x7x2048xf32>, %arg1: tensor<1x1x1x2048xf32>, %arg2: tensor<666x7x7x2048xf32>) -> tensor<666x7x7x2048xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x7x7x2048xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x2048xf32> into tensor<2048xf32>\n    %1 = bufferization.to_memref %collapsed : memref<2048xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x7x7x2048xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 2048 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x2048xf32>\n            %4 = affine.load %1[%arg6] : memref<2048xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x7x7x2048xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x7x7x2048xf32>\n    return %2 : tensor<666x7x7x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x7x7x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x7x7x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x7x7x2048xf32>) -> tensor<666x7x7x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x2048xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x2048xf32>) -> tensor<1x1x1x2048xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x7x7x2048xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x7x7x2048xf32>) -> tensor<666x7x7x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x7x7x2048xf32>, tensor<1x1x1x2048xf32>) outs(%4 : tensor<666x7x7x2048xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.addf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x7x7x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x7x7x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x7x7x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 2048, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x56x56x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x168xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x56x56x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x56x56x168xf32>, %3: tensor<1x1x1x168xf32>, %4: tensor<666x56x56x168xf32>) -> tensor<666x56x56x168xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x56x56x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x168xf32>\n  return %ret : tensor<666x56x56x168xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x56x56x168xf32>, %arg1: tensor<1x1x1x168xf32>, %arg2: tensor<666x56x56x168xf32>) -> tensor<666x56x56x168xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x56x56x168xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x168xf32> into tensor<168xf32>\n    %1 = bufferization.to_memref %collapsed : memref<168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x56x56x168xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 168 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x168xf32>\n            %4 = affine.load %1[%arg6] : memref<168xf32>\n            %5 = arith.subf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x56x56x168xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x56x56x168xf32>\n    return %2 : tensor<666x56x56x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x56x56x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x56x56x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x56x56x168xf32>) -> tensor<666x56x56x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x168xf32>) -> tensor<1x1x1x168xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x56x56x168xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x56x56x168xf32>) -> tensor<666x56x56x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x56x56x168xf32>, tensor<1x1x1x168xf32>) outs(%4 : tensor<666x56x56x168xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.subf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x56x56x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x56x56x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x56x56x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 168, 1]], "op_count": {"+": 0, "-": 1, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x336xf32>, tensor<1x1x1x336xf32>) outs(%4 : tensor<666x42x42x336xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x336xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x336xf32>, tensor<1x1x1x336xf32>) outs(%4 : tensor<666x42x42x336xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x42x42x336xf32>, %3: tensor<1x1x1x336xf32>, %4: tensor<666x42x42x336xf32>) -> tensor<666x42x42x336xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x336xf32>, tensor<1x1x1x336xf32>) outs(%4 : tensor<666x42x42x336xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x336xf32>\n  return %ret : tensor<666x42x42x336xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x42x42x336xf32>, %arg1: tensor<1x1x1x336xf32>, %arg2: tensor<666x42x42x336xf32>) -> tensor<666x42x42x336xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x42x42x336xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x336xf32> into tensor<336xf32>\n    %1 = bufferization.to_memref %collapsed : memref<336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x42x42x336xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 336 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x42x42x336xf32>\n            %4 = affine.load %1[%arg6] : memref<336xf32>\n            %5 = arith.mulf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x42x42x336xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x42x42x336xf32>\n    return %2 : tensor<666x42x42x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x42x42x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x42x42x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x42x42x336xf32>) -> tensor<666x42x42x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x336xf32>) -> tensor<1x1x1x336xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x42x42x336xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x42x42x336xf32>) -> tensor<666x42x42x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x42x42x336xf32>, tensor<1x1x1x336xf32>) outs(%4 : tensor<666x42x42x336xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x42x42x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x42x42x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x42x42x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 336, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%1 : tensor<696xf32>) outs(%2 : tensor<696xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<696xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%1 : tensor<696xf32>) outs(%2 : tensor<696xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<696xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<696xf32>, %2: tensor<696xf32>) -> tensor<696xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%1 : tensor<696xf32>) outs(%2 : tensor<696xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<696xf32>\n  return %ret : tensor<696xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<696xf32>, %arg1: tensor<696xf32>) -> tensor<696xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<696xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<696xf32>\n    affine.for %arg2 = 0 to 696 {\n      %2 = affine.load %0[%arg2] : memref<696xf32>\n      %3 = math.rsqrt %2 : f32\n      affine.store %3, %alloc[%arg2] : memref<696xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<696xf32>\n    return %1 : tensor<696xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<696xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<696xf32>) -> tensor<696xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<696xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<696xf32>) -> tensor<696xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%1 : tensor<696xf32>) outs(%2 : tensor<696xf32>) {\n^bb0(%in: f32, %out: f32):\n  %4 = math.rsqrt %in : f32\n  linalg.yield %4 : f32\n} -> tensor<696xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<696xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<696xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 696, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x21x21x672xf32>, tensor<1x1x1x672xf32>) outs(%4 : tensor<666x21x21x672xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x21x21x672xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x21x21x672xf32>, tensor<1x1x1x672xf32>) outs(%4 : tensor<666x21x21x672xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x21x21x672xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x21x21x672xf32>, %3: tensor<1x1x1x672xf32>, %4: tensor<666x21x21x672xf32>) -> tensor<666x21x21x672xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x21x21x672xf32>, tensor<1x1x1x672xf32>) outs(%4 : tensor<666x21x21x672xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x21x21x672xf32>\n  return %ret : tensor<666x21x21x672xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x21x21x672xf32>, %arg1: tensor<1x1x1x672xf32>, %arg2: tensor<666x21x21x672xf32>) -> tensor<666x21x21x672xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<666x21x21x672xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1, 2, 3]] : tensor<1x1x1x672xf32> into tensor<672xf32>\n    %1 = bufferization.to_memref %collapsed : memref<672xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x21x21x672xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 672 {\n            %3 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<666x21x21x672xf32>\n            %4 = affine.load %1[%arg6] : memref<672xf32>\n            %5 = arith.mulf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x21x21x672xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<666x21x21x672xf32>\n    return %2 : tensor<666x21x21x672xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x21x21x672xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x21x21x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x21x21x672xf32>) -> tensor<666x21x21x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1x672xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1x672xf32>) -> tensor<1x1x1x672xf32>\n%tmp_4 = bufferization.alloc_tensor() : tensor<666x21x21x672xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<666x21x21x672xf32>) -> tensor<666x21x21x672xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1, %3 : tensor<666x21x21x672xf32>, tensor<1x1x1x672xf32>) outs(%4 : tensor<666x21x21x672xf32>) {\n^bb0(%in: f32, %in_0: f32, %out: f32):\n  %6 = arith.mulf %in, %in_0 : f32\n  linalg.yield %6 : f32\n} -> tensor<666x21x21x672xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x21x21x672xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x21x21x672xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 672, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg6"], ["%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<128xf32>, %6: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>\n  return %ret : tensor<666x14x14x128xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128xf32>, %arg1: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x14x14x128xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 14 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 128 {\n            %2 = affine.load %0[%arg5] : memref<128xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<666x14x14x128xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x14x14x128xf32>\n    return %1 : tensor<666x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<128xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<128xf32>) -> tensor<128xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<666x14x14x128xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5 : tensor<128xf32>) outs(%6 : tensor<666x14x14x128xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<666x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 14, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 128, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x14x14x1312xf32>, %3: tensor<128x1x1x1312xf32>, %7: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n  return %ret : tensor<666x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<666x14x14x1312xf32>, %arg1: tensor<128x1x1x1312xf32>, %arg2: tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1312xf32>\n    %1 = bufferization.to_memref %arg0 : memref<666x14x14x1312xf32>\n    %2 = bufferization.to_memref %arg2 : memref<666x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x14x14x128xf32>\n    memref.copy %2, %alloc : memref<666x14x14x128xf32> to memref<666x14x14x128xf32>\n    affine.for %arg3 = 0 to 666 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1312 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<666x14x14x1312xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1312xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<666x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<666x14x14x128xf32>\n    return %3 : tensor<666x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x14x14x1312xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x14x14x1312xf32>) -> tensor<666x14x14x1312xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1312xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1312xf32>) -> tensor<128x1x1x1312xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<666x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<666x14x14x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<666x14x14x128xf32>) -> tensor<666x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 666, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1312, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x368xf32>) outs(%2 : tensor<666x1x1x368xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 1.000000e+00 : f32\n  %4 = arith.negf %in : f32\n  %5 = math.exp %4 : f32\n  %6 = arith.addf %5, %cst_0 : f32\n  %7 = arith.divf %cst_0, %6 : f32\n  linalg.yield %7 : f32\n} -> tensor<666x1x1x368xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x368xf32>) outs(%2 : tensor<666x1x1x368xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 1.000000e+00 : f32\n  %4 = arith.negf %in : f32\n  %5 = math.exp %4 : f32\n  %6 = arith.addf %5, %cst_0 : f32\n  %7 = arith.divf %cst_0, %6 : f32\n  linalg.yield %7 : f32\n} -> tensor<666x1x1x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<666x1x1x368xf32>, %2: tensor<666x1x1x368xf32>) -> tensor<666x1x1x368xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x368xf32>) outs(%2 : tensor<666x1x1x368xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 1.000000e+00 : f32\n  %4 = arith.negf %in : f32\n  %5 = math.exp %4 : f32\n  %6 = arith.addf %5, %cst_0 : f32\n  %7 = arith.divf %cst_0, %6 : f32\n  linalg.yield %7 : f32\n} -> tensor<666x1x1x368xf32>\n  return %ret : tensor<666x1x1x368xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<666x1x1x368xf32>, %arg1: tensor<666x1x1x368xf32>) -> tensor<666x1x1x368xf32> {\n    %cst = arith.constant 1.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1, 2], [3]] : tensor<666x1x1x368xf32> into tensor<666x368xf32>\n    %0 = bufferization.to_memref %collapsed : memref<666x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<666x368xf32>\n    affine.for %arg2 = 0 to 666 {\n      affine.for %arg3 = 0 to 368 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<666x368xf32>\n        %3 = arith.negf %2 : f32\n        %4 = math.exp %3 : f32\n        %5 = arith.addf %4, %cst : f32\n        %6 = arith.divf %cst, %5 : f32\n        affine.store %6, %alloc[%arg2, %arg3] : memref<666x368xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<666x368xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1, 2], [3]] : tensor<666x368xf32> into tensor<666x1x1x368xf32>\n    return %expanded : tensor<666x1x1x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<666x1x1x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<666x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<666x1x1x368xf32>) -> tensor<666x1x1x368xf32>\n%tmp_2 = bufferization.alloc_tensor() : tensor<666x1x1x368xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<666x1x1x368xf32>) -> tensor<666x1x1x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, 0, 0, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<666x1x1x368xf32>) outs(%2 : tensor<666x1x1x368xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_0 = arith.constant 1.000000e+00 : f32\n  %4 = arith.negf %in : f32\n  %5 = math.exp %4 : f32\n  %6 = arith.addf %5, %cst_0 : f32\n  %7 = arith.divf %cst_0, %6 : f32\n  linalg.yield %7 : f32\n} -> tensor<666x1x1x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<666x1x1x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<666x1x1x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 666, 1], ["%arg3", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 1, "exp": 1}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}]]