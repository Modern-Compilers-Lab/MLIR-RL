[["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x64x112x112xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x64x112x112xf32>", "wrapped_operation": "func.func @func_call(%2: tensor<64xf32>, %11: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x64x112x112xf32>\n  return %ret : tensor<32x64x112x112xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<64xf32>, %arg1: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x112x112xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%arg3] : memref<64xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x64x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x64x112x112xf32>\n    return %1 : tensor<32x64x112x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_2 = bufferization.alloc_tensor() : tensor<64xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<64xf32>) -> tensor<64xf32>\n%tmp_11 = bufferization.alloc_tensor() : tensor<32x64x112x112xf32>\n%11 = linalg.fill ins(%val : f32) outs(%tmp_11 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x112x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 64, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 17632744.0}], ["linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>", {"operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>", "wrapped_operation": "func.func @func_call(%0: tensor<32x3x230x230xf32>, %1: tensor<64x3x7x7xf32>, %12: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n  return %ret : tensor<32x64x112x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x3x230x230xf32>, %arg1: tensor<64x3x7x7xf32>, %arg2: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x3x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x3x230x230xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x112x112xf32>\n    memref.copy %2, %alloc : memref<32x64x112x112xf32> to memref<32x64x112x112xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<32x3x230x230xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x3x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x112x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x112x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64x112x112xf32>\n    return %3 : tensor<32x64x112x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_0 = bufferization.alloc_tensor() : tensor<32x3x230x230xf32>\n%0 = linalg.fill ins(%val : f32) outs(%tmp_0 : tensor<32x3x230x230xf32>) -> tensor<32x3x230x230xf32>\n%tmp_1 = bufferization.alloc_tensor() : tensor<64x3x7x7xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<64x3x7x7xf32>) -> tensor<64x3x7x7xf32>\n%tmp_12 = bufferization.alloc_tensor() : tensor<32x64x112x112xf32>\n%12 = linalg.fill ins(%val : f32) outs(%tmp_12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x112x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg7", "%arg5 * 2 + %arg8", "%arg6 * 2 + %arg9"], ["%arg4", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13560142681.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x64x112x112xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x64x112x112xf32>", "wrapped_operation": "func.func @func_call(%13: tensor<32x64x112x112xf32>, %11: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x64x112x112xf32>\n  return %ret : tensor<32x64x112x112xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x64x112x112xf32>, %arg1: tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<32x64x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x112x112xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<32x64x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x64x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x64x112x112xf32>\n    return %1 : tensor<32x64x112x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_13 = bufferization.alloc_tensor() : tensor<32x64x112x112xf32>\n%13 = linalg.fill ins(%val : f32) outs(%tmp_13 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n%tmp_11 = bufferization.alloc_tensor() : tensor<32x64x112x112xf32>\n%11 = linalg.fill ins(%val : f32) outs(%tmp_11 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x112x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 64, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 24161533.0}], ["linalg.fill ins(%cst : f32) outs(%15 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>", {"operation": "linalg.fill ins(%cst : f32) outs(%15 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>", "wrapped_operation": "func.func @func_call(%cst: f32, %15: tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32> {\n  %ret = linalg.fill ins(%cst : f32) outs(%15 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n  return %ret : tensor<32x64x56x56xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x56x56xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x64x56x56xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<32x64x56x56xf32>\n    return %0 : tensor<32x64x56x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst = arith.constant 2.00000e+00 : f32\n%tmp_15 = bufferization.alloc_tensor() : tensor<32x64x56x56xf32>\n%15 = linalg.fill ins(%val : f32) outs(%tmp_15 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst : f32) outs(%15 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x56x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 64, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 3779284.5}], ["linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%14, %17 : tensor<32x64x112x112xf32>, tensor<2x2xf32>) outs(%16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>", {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%14, %17 : tensor<32x64x112x112xf32>, tensor<2x2xf32>) outs(%16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>", "wrapped_operation": "func.func @func_call(%14: tensor<32x64x112x112xf32>, %17: tensor<2x2xf32>, %16: tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%14, %17 : tensor<32x64x112x112xf32>, tensor<2x2xf32>) outs(%16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n  return %ret : tensor<32x64x56x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x112x112xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x64x112x112xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x64x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x56x56xf32>\n    memref.copy %1, %alloc : memref<32x64x56x56xf32> to memref<32x64x56x56xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x64x112x112xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x56x56xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x56x56xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x64x56x56xf32>\n    return %2 : tensor<32x64x56x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_14 = bufferization.alloc_tensor() : tensor<32x64x112x112xf32>\n%14 = linalg.fill ins(%val : f32) outs(%tmp_14 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n%tmp_17 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_16 = bufferization.alloc_tensor() : tensor<32x64x56x56xf32>\n%16 = linalg.fill ins(%val : f32) outs(%tmp_16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%14, %17 : tensor<32x64x112x112xf32>, tensor<2x2xf32>) outs(%16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x56x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 72947384.5}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x16x52x52xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x16x52x52xf32>", "wrapped_operation": "func.func @func_call(%4: tensor<16xf32>, %19: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x16x52x52xf32>\n  return %ret : tensor<32x16x52x52xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16xf32>, %arg1: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x52x52xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 52 {\n          affine.for %arg5 = 0 to 52 {\n            %2 = affine.load %0[%arg3] : memref<16xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x16x52x52xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x16x52x52xf32>\n    return %1 : tensor<32x16x52x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x52x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_4 = bufferization.alloc_tensor() : tensor<16xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<16xf32>) -> tensor<16xf32>\n%tmp_19 = bufferization.alloc_tensor() : tensor<32x16x52x52xf32>\n%19 = linalg.fill ins(%val : f32) outs(%tmp_19 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<32x16x52x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x52x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x52x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 52, 1], ["%arg5", 0, 52, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 1167705.5}], ["linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>", {"operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>", "wrapped_operation": "func.func @func_call(%18: tensor<32x64x56x56xf32>, %3: tensor<16x64x5x5xf32>, %20: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n  return %ret : tensor<32x16x52x52xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x56x56xf32>, %arg1: tensor<16x64x5x5xf32>, %arg2: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x64x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x16x52x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x52x52xf32>\n    memref.copy %2, %alloc : memref<32x16x52x52xf32> to memref<32x16x52x52xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 52 {\n          affine.for %arg6 = 0 to 52 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<32x64x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<16x64x5x5xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x52x52xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x52x52xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x16x52x52xf32>\n    return %3 : tensor<32x16x52x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x52x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_18 = bufferization.alloc_tensor() : tensor<32x64x56x56xf32>\n%18 = linalg.fill ins(%val : f32) outs(%tmp_18 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<16x64x5x5xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<16x64x5x5xf32>) -> tensor<16x64x5x5xf32>\n%tmp_20 = bufferization.alloc_tensor() : tensor<32x16x52x52xf32>\n%20 = linalg.fill ins(%val : f32) outs(%tmp_20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x52x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x52x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 52, 1], ["%arg6", 0, 52, 1], ["%arg7", 0, 64, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg7", "%arg5 + %arg8", "%arg6 + %arg9"], ["%arg4", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8356379721.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21 : tensor<32x16x52x52xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x16x52x52xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21 : tensor<32x16x52x52xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x16x52x52xf32>", "wrapped_operation": "func.func @func_call(%21: tensor<32x16x52x52xf32>, %19: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21 : tensor<32x16x52x52xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x16x52x52xf32>\n  return %ret : tensor<32x16x52x52xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x16x52x52xf32>, %arg1: tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<32x16x52x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x52x52xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 52 {\n          affine.for %arg5 = 0 to 52 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<32x16x52x52xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x16x52x52xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x16x52x52xf32>\n    return %1 : tensor<32x16x52x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x52x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_21 = bufferization.alloc_tensor() : tensor<32x16x52x52xf32>\n%21 = linalg.fill ins(%val : f32) outs(%tmp_21 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n%tmp_19 = bufferization.alloc_tensor() : tensor<32x16x52x52xf32>\n%19 = linalg.fill ins(%val : f32) outs(%tmp_19 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21 : tensor<32x16x52x52xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x16x52x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x52x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x52x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 52, 1], ["%arg5", 0, 52, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 1291713.0}], ["linalg.fill ins(%cst : f32) outs(%23 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>", {"operation": "linalg.fill ins(%cst : f32) outs(%23 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>", "wrapped_operation": "func.func @func_call(%cst: f32, %23: tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32> {\n  %ret = linalg.fill ins(%cst : f32) outs(%23 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n  return %ret : tensor<32x16x26x26xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x26x26xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 26 {\n          affine.for %arg5 = 0 to 26 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<32x16x26x26xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<32x16x26x26xf32>\n    return %0 : tensor<32x16x26x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x26x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst = arith.constant 2.00000e+00 : f32\n%tmp_23 = bufferization.alloc_tensor() : tensor<32x16x26x26xf32>\n%23 = linalg.fill ins(%val : f32) outs(%tmp_23 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst : f32) outs(%23 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x26x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x26x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 26, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 184091.0}], ["linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%22, %17 : tensor<32x16x52x52xf32>, tensor<2x2xf32>) outs(%24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>", {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%22, %17 : tensor<32x16x52x52xf32>, tensor<2x2xf32>) outs(%24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>", "wrapped_operation": "func.func @func_call(%22: tensor<32x16x52x52xf32>, %17: tensor<2x2xf32>, %24: tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%22, %17 : tensor<32x16x52x52xf32>, tensor<2x2xf32>) outs(%24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n  return %ret : tensor<32x16x26x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x16x52x52xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x16x52x52xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x16x26x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x26x26xf32>\n    memref.copy %1, %alloc : memref<32x16x26x26xf32> to memref<32x16x26x26xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x16x52x52xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x26x26xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x26x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x16x26x26xf32>\n    return %2 : tensor<32x16x26x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x26x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_22 = bufferization.alloc_tensor() : tensor<32x16x52x52xf32>\n%22 = linalg.fill ins(%val : f32) outs(%tmp_22 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n%tmp_17 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_24 = bufferization.alloc_tensor() : tensor<32x16x26x26xf32>\n%24 = linalg.fill ins(%val : f32) outs(%tmp_24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%22, %17 : tensor<32x16x52x52xf32>, tensor<2x2xf32>) outs(%24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x26x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x26x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4064276.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<10816x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<10816x120xf32>", "wrapped_operation": "func.func @func_call(%5: tensor<120x10816xf32>, %26: tensor<10816x120xf32>) -> tensor<10816x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<10816x120xf32>\n  return %ret : tensor<10816x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<120x10816xf32>, %arg1: tensor<10816x120xf32>) -> tensor<10816x120xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<120x10816xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<10816x120xf32>\n    affine.for %arg2 = 0 to 120 {\n      affine.for %arg3 = 0 to 10816 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<120x10816xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<10816x120xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<10816x120xf32>\n    return %1 : tensor<10816x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<10816x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_5 = bufferization.alloc_tensor() : tensor<120x10816xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<120x10816xf32>) -> tensor<120x10816xf32>\n%tmp_26 = bufferization.alloc_tensor() : tensor<10816x120xf32>\n%26 = linalg.fill ins(%val : f32) outs(%tmp_26 : tensor<10816x120xf32>) -> tensor<10816x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<10816x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<10816x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<10816x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 120, 1], ["%arg3", 0, 10816, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": 4656410.0}], ["linalg.fill ins(%cst_0 : f32) outs(%28 : tensor<32x120xf32>) -> tensor<32x120xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%28 : tensor<32x120xf32>) -> tensor<32x120xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %28: tensor<32x120xf32>) -> tensor<32x120xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%28 : tensor<32x120xf32>) -> tensor<32x120xf32>\n  return %ret : tensor<32x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<32x120xf32>) -> tensor<32x120xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x120xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 120 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<32x120xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<32x120xf32>\n    return %0 : tensor<32x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_28 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<32x120xf32>) -> tensor<32x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%28 : tensor<32x120xf32>) -> tensor<32x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": 1943.0}], ["linalg.matmul ins(%collapsed, %27 : tensor<32x10816xf32>, tensor<10816x120xf32>) outs(%29 : tensor<32x120xf32>) -> tensor<32x120xf32>", {"operation": "linalg.matmul ins(%collapsed, %27 : tensor<32x10816xf32>, tensor<10816x120xf32>) outs(%29 : tensor<32x120xf32>) -> tensor<32x120xf32>", "wrapped_operation": "func.func @func_call(%collapsed: tensor<32x10816xf32>, %27: tensor<10816x120xf32>, %29: tensor<32x120xf32>) -> tensor<32x120xf32> {\n  %ret = linalg.matmul ins(%collapsed, %27 : tensor<32x10816xf32>, tensor<10816x120xf32>) outs(%29 : tensor<32x120xf32>) -> tensor<32x120xf32>\n  return %ret : tensor<32x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x10816xf32>, %arg1: tensor<10816x120xf32>, %arg2: tensor<32x120xf32>) -> tensor<32x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<10816x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x10816xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x120xf32>\n    memref.copy %2, %alloc : memref<32x120xf32> to memref<32x120xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 120 {\n        affine.for %arg5 = 0 to 10816 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<32x10816xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<10816x120xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<32x120xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<32x120xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x120xf32>\n    return %3 : tensor<32x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_collapsed = bufferization.alloc_tensor() : tensor<32x10816xf32>\n%collapsed = linalg.fill ins(%val : f32) outs(%tmp_collapsed : tensor<32x10816xf32>) -> tensor<32x10816xf32>\n%tmp_27 = bufferization.alloc_tensor() : tensor<10816x120xf32>\n%27 = linalg.fill ins(%val : f32) outs(%tmp_27 : tensor<10816x120xf32>) -> tensor<10816x120xf32>\n%tmp_29 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%29 = linalg.fill ins(%val : f32) outs(%tmp_29 : tensor<32x120xf32>) -> tensor<32x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%collapsed, %27 : tensor<32x10816xf32>, tensor<10816x120xf32>) outs(%29 : tensor<32x120xf32>) -> tensor<32x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 120, 1], ["%arg5", 0, 10816, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 158641869.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x120xf32>", "wrapped_operation": "func.func @func_call(%30: tensor<32x120xf32>, %6: tensor<120xf32>, %28: tensor<32x120xf32>) -> tensor<32x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x120xf32>\n  return %ret : tensor<32x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x120xf32>, %arg1: tensor<120xf32>, %arg2: tensor<32x120xf32>) -> tensor<32x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x120xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 120 {\n        %3 = affine.load %1[%arg3, %arg4] : memref<32x120xf32>\n        %4 = affine.load %0[%arg4] : memref<120xf32>\n        %5 = arith.addf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<32x120xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x120xf32>\n    return %2 : tensor<32x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_30 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%30 = linalg.fill ins(%val : f32) outs(%tmp_30 : tensor<32x120xf32>) -> tensor<32x120xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<120xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<120xf32>) -> tensor<120xf32>\n%tmp_28 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<32x120xf32>) -> tensor<32x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 3229.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x120xf32>", "wrapped_operation": "func.func @func_call(%31: tensor<32x120xf32>, %28: tensor<32x120xf32>) -> tensor<32x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x120xf32>\n  return %ret : tensor<32x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x120xf32>, %arg1: tensor<32x120xf32>) -> tensor<32x120xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<32x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x120xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 120 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<32x120xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<32x120xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x120xf32>\n    return %1 : tensor<32x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_31 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%31 = linalg.fill ins(%val : f32) outs(%tmp_31 : tensor<32x120xf32>) -> tensor<32x120xf32>\n%tmp_28 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<32x120xf32>) -> tensor<32x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": 3024.5}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>", "wrapped_operation": "func.func @func_call(%7: tensor<84x120xf32>, %33: tensor<120x84xf32>) -> tensor<120x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>\n  return %ret : tensor<120x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<84x120xf32>, %arg1: tensor<120x84xf32>) -> tensor<120x84xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<84x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<120x84xf32>\n    affine.for %arg2 = 0 to 84 {\n      affine.for %arg3 = 0 to 120 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<84x120xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<120x84xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<120x84xf32>\n    return %1 : tensor<120x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<120x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_7 = bufferization.alloc_tensor() : tensor<84x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<84x120xf32>) -> tensor<84x120xf32>\n%tmp_33 = bufferization.alloc_tensor() : tensor<120x84xf32>\n%33 = linalg.fill ins(%val : f32) outs(%tmp_33 : tensor<120x84xf32>) -> tensor<120x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<120x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<120x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 84, 1], ["%arg3", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": 12623.5}], ["linalg.fill ins(%cst_0 : f32) outs(%35 : tensor<32x84xf32>) -> tensor<32x84xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%35 : tensor<32x84xf32>) -> tensor<32x84xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %35: tensor<32x84xf32>) -> tensor<32x84xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%35 : tensor<32x84xf32>) -> tensor<32x84xf32>\n  return %ret : tensor<32x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<32x84xf32>) -> tensor<32x84xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x84xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 84 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<32x84xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<32x84xf32>\n    return %0 : tensor<32x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_35 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<32x84xf32>) -> tensor<32x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%35 : tensor<32x84xf32>) -> tensor<32x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": 1460.5}], ["linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>", {"operation": "linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>", "wrapped_operation": "func.func @func_call(%32: tensor<32x120xf32>, %34: tensor<120x84xf32>, %36: tensor<32x84xf32>) -> tensor<32x84xf32> {\n  %ret = linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>\n  return %ret : tensor<32x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x120xf32>, %arg1: tensor<120x84xf32>, %arg2: tensor<32x84xf32>) -> tensor<32x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x84xf32>\n    memref.copy %2, %alloc : memref<32x84xf32> to memref<32x84xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 84 {\n        affine.for %arg5 = 0 to 120 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<32x120xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<120x84xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<32x84xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<32x84xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x84xf32>\n    return %3 : tensor<32x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_32 = bufferization.alloc_tensor() : tensor<32x120xf32>\n%32 = linalg.fill ins(%val : f32) outs(%tmp_32 : tensor<32x120xf32>) -> tensor<32x120xf32>\n%tmp_34 = bufferization.alloc_tensor() : tensor<120x84xf32>\n%34 = linalg.fill ins(%val : f32) outs(%tmp_34 : tensor<120x84xf32>) -> tensor<120x84xf32>\n%tmp_36 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%36 = linalg.fill ins(%val : f32) outs(%tmp_36 : tensor<32x84xf32>) -> tensor<32x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 84, 1], ["%arg5", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 1080776.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x84xf32>", "wrapped_operation": "func.func @func_call(%37: tensor<32x84xf32>, %8: tensor<84xf32>, %35: tensor<32x84xf32>) -> tensor<32x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x84xf32>\n  return %ret : tensor<32x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x84xf32>, %arg1: tensor<84xf32>, %arg2: tensor<32x84xf32>) -> tensor<32x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x84xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 84 {\n        %3 = affine.load %1[%arg3, %arg4] : memref<32x84xf32>\n        %4 = affine.load %0[%arg4] : memref<84xf32>\n        %5 = arith.addf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<32x84xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x84xf32>\n    return %2 : tensor<32x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_37 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%37 = linalg.fill ins(%val : f32) outs(%tmp_37 : tensor<32x84xf32>) -> tensor<32x84xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<84xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<84xf32>) -> tensor<84xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<32x84xf32>) -> tensor<32x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 2383.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x84xf32>", "wrapped_operation": "func.func @func_call(%38: tensor<32x84xf32>, %35: tensor<32x84xf32>) -> tensor<32x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x84xf32>\n  return %ret : tensor<32x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x84xf32>, %arg1: tensor<32x84xf32>) -> tensor<32x84xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<32x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x84xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 84 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<32x84xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<32x84xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<32x84xf32>\n    return %1 : tensor<32x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<32x84xf32>) -> tensor<32x84xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<32x84xf32>) -> tensor<32x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_1 = arith.constant 0.000000e+00 : f32\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n  %47 = arith.select %46, %in, %cst_1 : f32\n  linalg.yield %47 : f32\n} -> tensor<32x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": 2294.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>", "wrapped_operation": "func.func @func_call(%9: tensor<10x84xf32>, %40: tensor<84x10xf32>) -> tensor<84x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>\n  return %ret : tensor<84x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<10x84xf32>, %arg1: tensor<84x10xf32>) -> tensor<84x10xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<10x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<84x10xf32>\n    affine.for %arg2 = 0 to 10 {\n      affine.for %arg3 = 0 to 84 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<10x84xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<84x10xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<84x10xf32>\n    return %1 : tensor<84x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<84x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_9 = bufferization.alloc_tensor() : tensor<10x84xf32>\n%9 = linalg.fill ins(%val : f32) outs(%tmp_9 : tensor<10x84xf32>) -> tensor<10x84xf32>\n%tmp_40 = bufferization.alloc_tensor() : tensor<84x10xf32>\n%40 = linalg.fill ins(%val : f32) outs(%tmp_40 : tensor<84x10xf32>) -> tensor<84x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<84x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<84x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 10, 1], ["%arg3", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": 676.5}], ["linalg.fill ins(%cst_0 : f32) outs(%42 : tensor<32x10xf32>) -> tensor<32x10xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%42 : tensor<32x10xf32>) -> tensor<32x10xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %42: tensor<32x10xf32>) -> tensor<32x10xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%42 : tensor<32x10xf32>) -> tensor<32x10xf32>\n  return %ret : tensor<32x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<32x10xf32>) -> tensor<32x10xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x10xf32>\n    affine.for %arg2 = 0 to 32 {\n      affine.for %arg3 = 0 to 10 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<32x10xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<32x10xf32>\n    return %0 : tensor<32x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_42 = bufferization.alloc_tensor() : tensor<32x10xf32>\n%42 = linalg.fill ins(%val : f32) outs(%tmp_42 : tensor<32x10xf32>) -> tensor<32x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%42 : tensor<32x10xf32>) -> tensor<32x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 32, 1], ["%arg3", 0, 10, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": 195.5}], ["linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>", {"operation": "linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>", "wrapped_operation": "func.func @func_call(%39: tensor<32x84xf32>, %41: tensor<84x10xf32>, %43: tensor<32x10xf32>) -> tensor<32x10xf32> {\n  %ret = linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>\n  return %ret : tensor<32x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x84xf32>, %arg1: tensor<84x10xf32>, %arg2: tensor<32x10xf32>) -> tensor<32x10xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x10xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x10xf32>\n    memref.copy %2, %alloc : memref<32x10xf32> to memref<32x10xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 10 {\n        affine.for %arg5 = 0 to 84 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<32x84xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<84x10xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<32x10xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<32x10xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x10xf32>\n    return %3 : tensor<32x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_39 = bufferization.alloc_tensor() : tensor<32x84xf32>\n%39 = linalg.fill ins(%val : f32) outs(%tmp_39 : tensor<32x84xf32>) -> tensor<32x84xf32>\n%tmp_41 = bufferization.alloc_tensor() : tensor<84x10xf32>\n%41 = linalg.fill ins(%val : f32) outs(%tmp_41 : tensor<84x10xf32>) -> tensor<84x10xf32>\n%tmp_43 = bufferization.alloc_tensor() : tensor<32x10xf32>\n%43 = linalg.fill ins(%val : f32) outs(%tmp_43 : tensor<32x10xf32>) -> tensor<32x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 10, 1], ["%arg5", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 84347.5}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x10xf32>", "wrapped_operation": "func.func @func_call(%44: tensor<32x10xf32>, %10: tensor<10xf32>, %42: tensor<32x10xf32>) -> tensor<32x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x10xf32>\n  return %ret : tensor<32x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<32x10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<32x10xf32>) -> tensor<32x10xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<10xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x10xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 10 {\n        %3 = affine.load %1[%arg3, %arg4] : memref<32x10xf32>\n        %4 = affine.load %0[%arg4] : memref<10xf32>\n        %5 = arith.addf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<32x10xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x10xf32>\n    return %2 : tensor<32x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_44 = bufferization.alloc_tensor() : tensor<32x10xf32>\n%44 = linalg.fill ins(%val : f32) outs(%tmp_44 : tensor<32x10xf32>) -> tensor<32x10xf32>\n%tmp_10 = bufferization.alloc_tensor() : tensor<10xf32>\n%10 = linalg.fill ins(%val : f32) outs(%tmp_10 : tensor<10xf32>) -> tensor<10xf32>\n%tmp_42 = bufferization.alloc_tensor() : tensor<32x10xf32>\n%42 = linalg.fill ins(%val : f32) outs(%tmp_42 : tensor<32x10xf32>) -> tensor<32x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\n^bb0(%in: f32, %in_1: f32, %out: f32):\n  %46 = arith.addf %in, %in_1 : f32\n  linalg.yield %46 : f32\n} -> tensor<32x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 10, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 363.0}]]