{"linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x512x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32>\n  return %ret : tensor<64x32x253x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x512x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x253x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x253x125xf32>\n    memref.copy %1, %alloc : memref<64x32x253x125xf32> to memref<64x32x253x125xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x253x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x253x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x253x125xf32>\n    return %2 : tensor<64x32x253x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x253x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x512x256xf32>) -> tensor<64x32x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x253x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x253x125xf32>) -> tensor<64x32x253x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x253x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x253x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 11883106883}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x64x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>\n  return %ret : tensor<8x512x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x64x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x64x128xf32>\n    memref.copy %1, %alloc : memref<8x512x64x128xf32> to memref<8x512x64x128xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x64x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x64x128xf32>\n    return %2 : tensor<8x512x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x64x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x64x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x128xf32>) -> tensor<8x512x64x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x64x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x64x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 37719196}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x8x128x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32>\n  return %ret : tensor<16x8x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x8x128x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x8x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x8x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x8x122x58xf32>\n    memref.copy %1, %alloc : memref<16x8x122x58xf32> to memref<16x8x122x58xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x8x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x8x122x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x8x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x8x122x58xf32>\n    return %2 : tensor<16x8x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x8x122x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x8x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x8x128x64xf32>) -> tensor<16x8x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x8x122x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x8x122x58xf32>) -> tensor<16x8x122x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x8x122x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x8x122x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 166054480}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x512x64x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32>\n  return %ret : tensor<4x512x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x512x64x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x512x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x512x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x512x62x254xf32>\n    memref.copy %1, %alloc : memref<4x512x62x254xf32> to memref<4x512x62x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x512x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x62x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x512x62x254xf32>\n    return %2 : tensor<4x512x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x512x62x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x512x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x512x64x256xf32>) -> tensor<4x512x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x512x62x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x62x254xf32>) -> tensor<4x512x62x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x512x62x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x512x62x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 575700282}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x256x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32>\n  return %ret : tensor<32x32x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x256x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x32x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x32x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x250x122xf32>\n    memref.copy %1, %alloc : memref<32x32x250x122xf32> to memref<32x32x250x122xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x32x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x250x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x32x250x122xf32>\n    return %2 : tensor<32x32x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x32x250x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x32x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x32x256x128xf32>) -> tensor<32x32x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x32x250x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x250x122xf32>) -> tensor<32x32x250x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x32x250x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x32x250x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5728044262}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x256x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32>\n  return %ret : tensor<64x256x127x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x256x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x256x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x256x127x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x127x63xf32>\n    memref.copy %1, %alloc : memref<64x256x127x63xf32> to memref<64x256x127x63xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x256x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x127x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x127x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x256x127x63xf32>\n    return %2 : tensor<64x256x127x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x256x127x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x256x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x256x256x128xf32>) -> tensor<64x256x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x256x127x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x127x63xf32>) -> tensor<64x256x127x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x256x127x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x256x127x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2494222870}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x512x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x512x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x8x512x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x512x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32>\n  return %ret : tensor<32x8x510x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x8x512x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x8x512x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x8x510x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x8x510x510xf32>\n    memref.copy %1, %alloc : memref<32x8x510x510xf32> to memref<32x8x510x510xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 510 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x8x512x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x510x510xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x510x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x8x510x510xf32>\n    return %2 : tensor<32x8x510x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x8x510x510xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x8x512x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x8x512x512xf32>) -> tensor<32x8x512x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x8x510x510xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x512x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x8x510x510xf32>) -> tensor<32x8x510x510xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x8x510x510xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x8x510x510xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 510, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1204285903}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x512x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32>\n  return %ret : tensor<8x128x254x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x512x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x254x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x254x14xf32>\n    memref.copy %1, %alloc : memref<8x128x254x14xf32> to memref<8x128x254x14xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x254x14xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x254x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x254x14xf32>\n    return %2 : tensor<8x128x254x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x254x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x512x32xf32>) -> tensor<8x128x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x254x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x254x14xf32>) -> tensor<8x128x254x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x254x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x254x14xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 270101028}}