{
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x14x14xf32>, tensor<288x128x7x7xf32>) outs (%init: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x14x14xf32>, tensor<288x128x7x7xf32>) outs (%init: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x128x14x14xf32>, %filter: tensor<288x128x7x7xf32>, %init: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x14x14xf32>, tensor<288x128x7x7xf32>) outs (%init: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32>\n  return %ret : tensor<256x288x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x14x14xf32>, %arg1: tensor<288x128x7x7xf32>, %arg2: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x128x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x288x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x4x4xf32>\n    memref.copy %2, %alloc : memref<256x288x4x4xf32> to memref<256x288x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 288 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x128x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<288x128x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x288x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x288x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x288x4x4xf32>\n    return %3 : tensor<256x288x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<288x128x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<288x128x7x7xf32>) -> tensor<288x128x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x288x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x14x14xf32>, tensor<288x128x7x7xf32>) outs (%init: tensor<256x288x4x4xf32>) -> tensor<256x288x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          288,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 27979376689
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<384x288x3x3xf32>) outs (%init: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<384x288x3x3xf32>) outs (%init: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x7x7xf32>, %filter: tensor<384x288x3x3xf32>, %init: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<384x288x3x3xf32>) outs (%init: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32>\n  return %ret : tensor<256x384x3x3xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x7x7xf32>, %arg1: tensor<384x288x3x3xf32>, %arg2: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x288x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x3x3xf32>\n    memref.copy %2, %alloc : memref<256x384x3x3xf32> to memref<256x384x3x3xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x288x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x3x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x3x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x3x3xf32>\n    return %3 : tensor<256x384x3x3xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x7x7xf32>) -> tensor<256x288x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x288x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x288x3x3xf32>) -> tensor<384x288x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x3x3xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<384x288x3x3xf32>) outs (%init: tensor<256x384x3x3xf32>) -> tensor<256x384x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x3x3xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          3,
          1
        ],
        [
          "%arg6",
          0,
          3,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 8663228661
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x7x7xf32>, tensor<288x240x3x3xf32>) outs (%init: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x7x7xf32>, tensor<288x240x3x3xf32>) outs (%init: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x240x7x7xf32>, %filter: tensor<288x240x3x3xf32>, %init: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x7x7xf32>, tensor<288x240x3x3xf32>) outs (%init: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32>\n  return %ret : tensor<128x288x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x240x7x7xf32>, %arg1: tensor<288x240x3x3xf32>, %arg2: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x240x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x240x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x288x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x5x5xf32>\n    memref.copy %2, %alloc : memref<128x288x5x5xf32> to memref<128x288x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 288 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x240x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<288x240x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x288x5x5xf32>\n    return %3 : tensor<128x288x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x240x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x240x7x7xf32>) -> tensor<128x240x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<288x240x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<288x240x3x3xf32>) -> tensor<288x240x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x288x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x7x7xf32>, tensor<288x240x3x3xf32>) outs (%init: tensor<128x288x5x5xf32>) -> tensor<128x288x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          288,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 7517577376
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x15x15xf32>, tensor<64x64x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x15x15xf32>, tensor<64x64x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x64x15x15xf32>, %filter: tensor<64x64x7x7xf32>, %init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x15x15xf32>, tensor<64x64x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n  return %ret : tensor<256x64x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x15x15xf32>, %arg1: tensor<64x64x7x7xf32>, %arg2: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x64x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x5x5xf32>\n    memref.copy %2, %alloc : memref<256x64x5x5xf32> to memref<256x64x5x5xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x64x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x64x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x5x5xf32>\n    return %3 : tensor<256x64x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x15x15xf32>) -> tensor<256x64x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x64x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x64x7x7xf32>) -> tensor<64x64x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x15x15xf32>, tensor<64x64x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4851051422
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x7x7xf32>, tensor<128x384x3x3xf32>) outs (%init: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x7x7xf32>, tensor<128x384x3x3xf32>) outs (%init: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x7x7xf32>, %filter: tensor<128x384x3x3xf32>, %init: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x7x7xf32>, tensor<128x384x3x3xf32>) outs (%init: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32>\n  return %ret : tensor<128x128x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x7x7xf32>, %arg1: tensor<128x384x3x3xf32>, %arg2: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x384x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x5x5xf32>\n    memref.copy %2, %alloc : memref<128x128x5x5xf32> to memref<128x128x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x384x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x5x5xf32>\n    return %3 : tensor<128x128x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x7x7xf32>) -> tensor<128x384x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x384x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x384x3x3xf32>) -> tensor<128x384x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x7x7xf32>, tensor<128x384x3x3xf32>) outs (%init: tensor<128x128x5x5xf32>) -> tensor<128x128x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5349129858
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x240x14x14xf32>, %filter: tensor<128x240x7x7xf32>, %init: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32>\n  return %ret : tensor<256x128x8x8xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x240x14x14xf32>, %arg1: tensor<128x240x7x7xf32>, %arg2: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x240x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x240x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x8x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x8x8xf32>\n    memref.copy %2, %alloc : memref<256x128x8x8xf32> to memref<256x128x8x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x240x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x240x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x8x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x8x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x8x8xf32>\n    return %3 : tensor<256x128x8x8xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x8x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x240x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x240x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x240x7x7xf32>) -> tensor<128x240x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x8x8xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<256x128x8x8xf32>) -> tensor<256x128x8x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x8x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x8x8xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          8,
          1
        ],
        [
          "%arg6",
          0,
          8,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 93306020627
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<384x96x7x7xf32>) outs (%init: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<384x96x7x7xf32>) outs (%init: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x15x15xf32>, %filter: tensor<384x96x7x7xf32>, %init: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<384x96x7x7xf32>) outs (%init: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32>\n  return %ret : tensor<128x384x9x9xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x15x15xf32>, %arg1: tensor<384x96x7x7xf32>, %arg2: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x96x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x384x9x9xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x9x9xf32>\n    memref.copy %2, %alloc : memref<128x384x9x9xf32> to memref<128x384x9x9xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 9 {\n          affine.for %arg6 = 0 to 9 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x96x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x9x9xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x9x9xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x384x9x9xf32>\n    return %3 : tensor<128x384x9x9xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x9x9xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x96x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x96x7x7xf32>) -> tensor<384x96x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x384x9x9xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<384x96x7x7xf32>) outs (%init: tensor<128x384x9x9xf32>) -> tensor<128x384x9x9xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x9x9xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x9x9xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          9,
          1
        ],
        [
          "%arg6",
          0,
          9,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 70743305929
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x15x15xf32>, tensor<240x96x7x7xf32>) outs (%init: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x15x15xf32>, tensor<240x96x7x7xf32>) outs (%init: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x96x15x15xf32>, %filter: tensor<240x96x7x7xf32>, %init: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x15x15xf32>, tensor<240x96x7x7xf32>) outs (%init: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32>\n  return %ret : tensor<256x240x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x96x15x15xf32>, %arg1: tensor<240x96x7x7xf32>, %arg2: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x96x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x96x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x240x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x5x5xf32>\n    memref.copy %2, %alloc : memref<256x240x5x5xf32> to memref<256x240x5x5xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x96x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x96x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x240x5x5xf32>\n    return %3 : tensor<256x240x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x96x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x96x15x15xf32>) -> tensor<256x96x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x96x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x96x7x7xf32>) -> tensor<240x96x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x240x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x15x15xf32>, tensor<240x96x7x7xf32>) outs (%init: tensor<256x240x5x5xf32>) -> tensor<256x240x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 27285175811
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x7x7xf32>, %filter: tensor<512x64x1x1xf32>, %init: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32>\n  return %ret : tensor<128x512x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x7x7xf32>, %arg1: tensor<512x64x1x1xf32>, %arg2: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x4x4xf32>\n    memref.copy %2, %alloc : memref<128x512x4x4xf32> to memref<128x512x4x4xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x4x4xf32>\n    return %3 : tensor<128x512x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x64x1x1xf32>) -> tensor<512x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x512x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<128x512x4x4xf32>) -> tensor<128x512x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 192142636
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x14x14xf32>, tensor<96x288x7x7xf32>) outs (%init: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x14x14xf32>, tensor<96x288x7x7xf32>) outs (%init: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x14x14xf32>, %filter: tensor<96x288x7x7xf32>, %init: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x14x14xf32>, tensor<96x288x7x7xf32>) outs (%init: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32>\n  return %ret : tensor<256x96x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x14x14xf32>, %arg1: tensor<96x288x7x7xf32>, %arg2: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x288x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x4x4xf32>\n    memref.copy %2, %alloc : memref<256x96x4x4xf32> to memref<256x96x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x288x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x4x4xf32>\n    return %3 : tensor<256x96x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x14x14xf32>) -> tensor<256x288x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x288x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x288x7x7xf32>) -> tensor<96x288x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x14x14xf32>, tensor<96x288x7x7xf32>) outs (%init: tensor<256x96x4x4xf32>) -> tensor<256x96x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 20989956151
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x130x130xf32>, tensor<192x384x1x1xf32>) outs (%init: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x130x130xf32>, tensor<192x384x1x1xf32>) outs (%init: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x130x130xf32>, %filter: tensor<192x384x1x1xf32>, %init: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x130x130xf32>, tensor<192x384x1x1xf32>) outs (%init: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32>\n  return %ret : tensor<128x192x65x65xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x130x130xf32>, %arg1: tensor<192x384x1x1xf32>, %arg2: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x384x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x130x130xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x65x65xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x65x65xf32>\n    memref.copy %2, %alloc : memref<128x192x65x65xf32> to memref<128x192x65x65xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 65 {\n          affine.for %arg6 = 0 to 65 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x130x130xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x384x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x65x65xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x65x65xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x65x65xf32>\n    return %3 : tensor<128x192x65x65xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x65x65xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x130x130xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x130x130xf32>) -> tensor<128x384x130x130xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x384x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x384x1x1xf32>) -> tensor<192x384x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x65x65xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x130x130xf32>, tensor<192x384x1x1xf32>) outs (%init: tensor<128x192x65x65xf32>) -> tensor<128x192x65x65xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x65x65xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x65x65xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          65,
          1
        ],
        [
          "%arg6",
          0,
          65,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 146026727700
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x56x56xf32>, tensor<256x128x1x1xf32>) outs (%init: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x56x56xf32>, tensor<256x128x1x1xf32>) outs (%init: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x128x56x56xf32>, %filter: tensor<256x128x1x1xf32>, %init: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x56x56xf32>, tensor<256x128x1x1xf32>) outs (%init: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>\n  return %ret : tensor<128x256x56x56xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x56x56xf32>, %arg1: tensor<256x128x1x1xf32>, %arg2: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x56x56xf32>\n    memref.copy %2, %alloc : memref<128x256x56x56xf32> to memref<128x256x56x56xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x128x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x56x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x56x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x56x56xf32>\n    return %3 : tensor<128x256x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x128x56x56xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x128x56x56xf32>) -> tensor<128x128x56x56xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x128x1x1xf32>) -> tensor<256x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x256x56x56xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x56x56xf32>, tensor<256x128x1x1xf32>) outs (%init: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ],
        [
          "%arg6",
          0,
          56,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 45634658152
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<96x48x3x3xf32>) outs (%init: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<96x48x3x3xf32>) outs (%init: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x15x15xf32>, %filter: tensor<96x48x3x3xf32>, %init: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<96x48x3x3xf32>) outs (%init: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32>\n  return %ret : tensor<128x96x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x15x15xf32>, %arg1: tensor<96x48x3x3xf32>, %arg2: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x48x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x96x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x13x13xf32>\n    memref.copy %2, %alloc : memref<128x96x13x13xf32> to memref<128x96x13x13xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x48x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x96x13x13xf32>\n    return %3 : tensor<128x96x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x48x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x48x3x3xf32>) -> tensor<96x48x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x96x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<96x48x3x3xf32>) outs (%init: tensor<128x96x13x13xf32>) -> tensor<128x96x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3358548484
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x120x120xf32>, tensor<32x512x1x1xf32>) outs (%init: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x120x120xf32>, tensor<32x512x1x1xf32>) outs (%init: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x512x120x120xf32>, %filter: tensor<32x512x1x1xf32>, %init: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x120x120xf32>, tensor<32x512x1x1xf32>) outs (%init: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32>\n  return %ret : tensor<256x32x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x120x120xf32>, %arg1: tensor<32x512x1x1xf32>, %arg2: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x512x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x60x60xf32>\n    memref.copy %2, %alloc : memref<256x32x60x60xf32> to memref<256x32x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x512x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x512x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x60x60xf32>\n    return %3 : tensor<256x32x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x512x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x512x120x120xf32>) -> tensor<256x512x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x512x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x512x1x1xf32>) -> tensor<32x512x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x120x120xf32>, tensor<32x512x1x1xf32>) outs (%init: tensor<256x32x60x60xf32>) -> tensor<256x32x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 58737607690
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x7x7xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x7x7xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x7x7xf32>, %filter: tensor<64x32x1x1xf32>, %init: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x7x7xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n  return %ret : tensor<128x64x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x7x7xf32>, %arg1: tensor<64x32x1x1xf32>, %arg2: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x7x7xf32>\n    memref.copy %2, %alloc : memref<128x64x7x7xf32> to memref<128x64x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x7x7xf32>\n    return %3 : tensor<128x64x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x7x7xf32>) -> tensor<128x32x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x32x1x1xf32>) -> tensor<64x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x7x7xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 29368096
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x228x228xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x228x228xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x228x228xf32>, %filter: tensor<64x32x1x1xf32>, %init: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x228x228xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32>\n  return %ret : tensor<128x64x114x114xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x228x228xf32>, %arg1: tensor<64x32x1x1xf32>, %arg2: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x228x228xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x114x114xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x114x114xf32>\n    memref.copy %2, %alloc : memref<128x64x114x114xf32> to memref<128x64x114x114xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 114 {\n          affine.for %arg6 = 0 to 114 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x228x228xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x114x114xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x114x114xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x114x114xf32>\n    return %3 : tensor<128x64x114x114xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x114x114xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x228x228xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x228x228xf32>) -> tensor<128x32x228x228xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x32x1x1xf32>) -> tensor<64x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x64x114x114xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x228x228xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<128x64x114x114xf32>) -> tensor<128x64x114x114xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x114x114xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x114x114xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          114,
          1
        ],
        [
          "%arg6",
          0,
          114,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 8242681240
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x7x7xf32>, tensor<96x96x3x3xf32>) outs (%init: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x7x7xf32>, tensor<96x96x3x3xf32>) outs (%init: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x7x7xf32>, %filter: tensor<96x96x3x3xf32>, %init: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x7x7xf32>, tensor<96x96x3x3xf32>) outs (%init: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32>\n  return %ret : tensor<128x96x3x3xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x7x7xf32>, %arg1: tensor<96x96x3x3xf32>, %arg2: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x96x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x96x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x3x3xf32>\n    memref.copy %2, %alloc : memref<128x96x3x3xf32> to memref<128x96x3x3xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x96x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x3x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x3x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x96x3x3xf32>\n    return %3 : tensor<128x96x3x3xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x96x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x96x3x3xf32>) -> tensor<96x96x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x96x3x3xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x7x7xf32>, tensor<96x96x3x3xf32>) outs (%init: tensor<128x96x3x3xf32>) -> tensor<128x96x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x3x3xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          3,
          1
        ],
        [
          "%arg6",
          0,
          3,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 359352803
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x56x56xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x56x56xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x56x56xf32>, %filter: tensor<384x32x1x1xf32>, %init: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x56x56xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>\n  return %ret : tensor<256x384x28x28xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x56x56xf32>, %arg1: tensor<384x32x1x1xf32>, %arg2: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x28x28xf32>\n    memref.copy %2, %alloc : memref<256x384x28x28xf32> to memref<256x384x28x28xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x28x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x28x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x28x28xf32>\n    return %3 : tensor<256x384x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x56x56xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x56x56xf32>) -> tensor<256x32x56x56xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x32x1x1xf32>) -> tensor<384x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x28x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x56x56xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ],
        [
          "%arg6",
          0,
          28,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5818664641
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x15x15xf32>, %filter: tensor<32x48x1x1xf32>, %init: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32>\n  return %ret : tensor<128x32x15x15xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x15x15xf32>, %arg1: tensor<32x48x1x1xf32>, %arg2: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x15x15xf32>\n    memref.copy %2, %alloc : memref<128x32x15x15xf32> to memref<128x32x15x15xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 15 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x15x15xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x15x15xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x15x15xf32>\n    return %3 : tensor<128x32x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x48x1x1xf32>) -> tensor<32x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x15x15xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x15x15xf32>) -> tensor<128x32x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ],
        [
          "%arg6",
          0,
          15,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 112582954
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<240x256x1x1xf32>) outs (%init: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<240x256x1x1xf32>) outs (%init: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x256x7x7xf32>, %filter: tensor<240x256x1x1xf32>, %init: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<240x256x1x1xf32>) outs (%init: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32>\n  return %ret : tensor<256x240x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x7x7xf32>, %arg1: tensor<240x256x1x1xf32>, %arg2: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x256x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x240x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x4x4xf32>\n    memref.copy %2, %alloc : memref<256x240x4x4xf32> to memref<256x240x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x256x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x256x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x240x4x4xf32>\n    return %3 : tensor<256x240x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x256x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x256x7x7xf32>) -> tensor<256x256x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x256x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x256x1x1xf32>) -> tensor<240x256x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x240x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<240x256x1x1xf32>) outs (%init: tensor<256x240x4x4xf32>) -> tensor<256x240x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 895192747
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x7x7xf32>, tensor<192x512x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x7x7xf32>, tensor<192x512x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x512x7x7xf32>, %filter: tensor<192x512x1x1xf32>, %init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x7x7xf32>, tensor<192x512x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n  return %ret : tensor<128x192x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x7x7xf32>, %arg1: tensor<192x512x1x1xf32>, %arg2: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x512x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x7x7xf32>\n    memref.copy %2, %alloc : memref<128x192x7x7xf32> to memref<128x192x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x512x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x512x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x7x7xf32>\n    return %3 : tensor<128x192x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x512x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x512x7x7xf32>) -> tensor<128x512x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x512x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x512x1x1xf32>) -> tensor<192x512x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x7x7xf32>, tensor<192x512x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2266508980
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x130x130xf32>, tensor<32x192x1x1xf32>) outs (%init: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x130x130xf32>, tensor<32x192x1x1xf32>) outs (%init: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x192x130x130xf32>, %filter: tensor<32x192x1x1xf32>, %init: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x130x130xf32>, tensor<32x192x1x1xf32>) outs (%init: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32>\n  return %ret : tensor<128x32x65x65xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x192x130x130xf32>, %arg1: tensor<32x192x1x1xf32>, %arg2: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x192x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x192x130x130xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x65x65xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x65x65xf32>\n    memref.copy %2, %alloc : memref<128x32x65x65xf32> to memref<128x32x65x65xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 65 {\n          affine.for %arg6 = 0 to 65 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x192x130x130xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x192x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x65x65xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x65x65xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x65x65xf32>\n    return %3 : tensor<128x32x65x65xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x65x65xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x192x130x130xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x192x130x130xf32>) -> tensor<128x192x130x130xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x192x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x192x1x1xf32>) -> tensor<32x192x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x65x65xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x130x130xf32>, tensor<32x192x1x1xf32>) outs (%init: tensor<128x32x65x65xf32>) -> tensor<128x32x65x65xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x65x65xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x65x65xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          65,
          1
        ],
        [
          "%arg6",
          0,
          65,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 11823229770
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x14x14xf32>, tensor<32x128x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x14x14xf32>, tensor<32x128x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x128x14x14xf32>, %filter: tensor<32x128x1x1xf32>, %init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x14x14xf32>, tensor<32x128x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n  return %ret : tensor<128x32x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x14x14xf32>, %arg1: tensor<32x128x1x1xf32>, %arg2: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x14x14xf32>\n    memref.copy %2, %alloc : memref<128x32x14x14xf32> to memref<128x32x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x128x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x14x14xf32>\n    return %3 : tensor<128x32x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x128x1x1xf32>) -> tensor<32x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x14x14xf32>, tensor<32x128x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 342048539
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<96x64x3x3xf32>) outs (%init: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<96x64x3x3xf32>) outs (%init: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x64x7x7xf32>, %filter: tensor<96x64x3x3xf32>, %init: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<96x64x3x3xf32>) outs (%init: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32>\n  return %ret : tensor<256x96x3x3xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x7x7xf32>, %arg1: tensor<96x64x3x3xf32>, %arg2: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x64x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x3x3xf32>\n    memref.copy %2, %alloc : memref<256x96x3x3xf32> to memref<256x96x3x3xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x64x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x3x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x3x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x3x3xf32>\n    return %3 : tensor<256x96x3x3xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x7x7xf32>) -> tensor<256x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x64x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x64x3x3xf32>) -> tensor<96x64x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x3x3xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<96x64x3x3xf32>) outs (%init: tensor<256x96x3x3xf32>) -> tensor<256x96x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x3x3xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          3,
          1
        ],
        [
          "%arg6",
          0,
          3,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 477692588
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x384x14x14xf32>, tensor<96x384x3x3xf32>) outs (%init: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x384x14x14xf32>, tensor<96x384x3x3xf32>) outs (%init: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x384x14x14xf32>, %filter: tensor<96x384x3x3xf32>, %init: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x384x14x14xf32>, tensor<96x384x3x3xf32>) outs (%init: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32>\n  return %ret : tensor<256x96x12x12xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x384x14x14xf32>, %arg1: tensor<96x384x3x3xf32>, %arg2: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x384x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x384x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x12x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x12x12xf32>\n    memref.copy %2, %alloc : memref<256x96x12x12xf32> to memref<256x96x12x12xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 12 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x384x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x384x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x12x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x12x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x12x12xf32>\n    return %3 : tensor<256x96x12x12xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x12x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x384x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x384x14x14xf32>) -> tensor<256x384x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x384x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x384x3x3xf32>) -> tensor<96x384x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x12x12xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x384x14x14xf32>, tensor<96x384x3x3xf32>) outs (%init: tensor<256x96x12x12xf32>) -> tensor<256x96x12x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x12x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x12x12xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          12,
          1
        ],
        [
          "%arg6",
          0,
          12,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 46296927988
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x15x15xf32>, tensor<512x64x7x7xf32>) outs (%init: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x15x15xf32>, tensor<512x64x7x7xf32>) outs (%init: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x15x15xf32>, %filter: tensor<512x64x7x7xf32>, %init: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x15x15xf32>, tensor<512x64x7x7xf32>) outs (%init: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32>\n  return %ret : tensor<128x512x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x15x15xf32>, %arg1: tensor<512x64x7x7xf32>, %arg2: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x64x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x5x5xf32>\n    memref.copy %2, %alloc : memref<128x512x5x5xf32> to memref<128x512x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x64x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x5x5xf32>\n    return %3 : tensor<128x512x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x15x15xf32>) -> tensor<128x64x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x64x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x64x7x7xf32>) -> tensor<512x64x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x512x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x15x15xf32>, tensor<512x64x7x7xf32>) outs (%init: tensor<128x512x5x5xf32>) -> tensor<128x512x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 19399393054
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<256x512x3x3xf32>) outs (%init: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<256x512x3x3xf32>) outs (%init: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x512x14x14xf32>, %filter: tensor<256x512x3x3xf32>, %init: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<256x512x3x3xf32>) outs (%init: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32>\n  return %ret : tensor<128x256x12x12xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x14x14xf32>, %arg1: tensor<256x512x3x3xf32>, %arg2: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x512x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x12x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x12x12xf32>\n    memref.copy %2, %alloc : memref<128x256x12x12xf32> to memref<128x256x12x12xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 12 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x512x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x512x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x12x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x12x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x12x12xf32>\n    return %3 : tensor<128x256x12x12xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x12x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x512x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x512x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x512x3x3xf32>) -> tensor<256x512x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x256x12x12xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<256x512x3x3xf32>) outs (%init: tensor<128x256x12x12xf32>) -> tensor<128x256x12x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x12x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x12x12xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          12,
          1
        ],
        [
          "%arg6",
          0,
          12,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 82371576721
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x228x228xf32>, tensor<128x96x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x228x228xf32>, tensor<128x96x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x96x228x228xf32>, %filter: tensor<128x96x1x1xf32>, %init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x228x228xf32>, tensor<128x96x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n  return %ret : tensor<256x128x114x114xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x96x228x228xf32>, %arg1: tensor<128x96x1x1xf32>, %arg2: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x96x228x228xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x114x114xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x114x114xf32>\n    memref.copy %2, %alloc : memref<256x128x114x114xf32> to memref<256x128x114x114xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 114 {\n          affine.for %arg6 = 0 to 114 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x96x228x228xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x114x114xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x114x114xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x114x114xf32>\n    return %3 : tensor<256x128x114x114xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x114x114xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x96x228x228xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x96x228x228xf32>) -> tensor<256x96x228x228xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x96x1x1xf32>) -> tensor<128x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x114x114xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x228x228xf32>, tensor<128x96x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x114x114xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x114x114xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          114,
          1
        ],
        [
          "%arg6",
          0,
          114,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 131551874320
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x120x120xf32>, tensor<384x48x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x120x120xf32>, tensor<384x48x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x48x120x120xf32>, %filter: tensor<384x48x1x1xf32>, %init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x120x120xf32>, tensor<384x48x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n  return %ret : tensor<256x384x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x120x120xf32>, %arg1: tensor<384x48x1x1xf32>, %arg2: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x60x60xf32>\n    memref.copy %2, %alloc : memref<256x384x60x60xf32> to memref<256x384x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x48x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x60x60xf32>\n    return %3 : tensor<256x384x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x48x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x48x120x120xf32>) -> tensor<256x48x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x48x1x1xf32>) -> tensor<384x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x120x120xf32>, tensor<384x48x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 44543376566
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<240x128x1x1xf32>) outs (%init: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<240x128x1x1xf32>) outs (%init: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x128x120x120xf32>, %filter: tensor<240x128x1x1xf32>, %init: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<240x128x1x1xf32>) outs (%init: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32>\n  return %ret : tensor<256x240x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x120x120xf32>, %arg1: tensor<240x128x1x1xf32>, %arg2: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x240x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x60x60xf32>\n    memref.copy %2, %alloc : memref<256x240x60x60xf32> to memref<256x240x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x128x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x240x60x60xf32>\n    return %3 : tensor<256x240x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x120x120xf32>) -> tensor<256x128x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x128x1x1xf32>) -> tensor<240x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x240x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<240x128x1x1xf32>) outs (%init: tensor<256x240x60x60xf32>) -> tensor<256x240x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 94894231858
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x28x28xf32>, tensor<128x192x3x3xf32>) outs (%init: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x28x28xf32>, tensor<128x192x3x3xf32>) outs (%init: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x192x28x28xf32>, %filter: tensor<128x192x3x3xf32>, %init: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x28x28xf32>, tensor<128x192x3x3xf32>) outs (%init: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32>\n  return %ret : tensor<256x128x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x192x28x28xf32>, %arg1: tensor<128x192x3x3xf32>, %arg2: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x192x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x192x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x13x13xf32>\n    memref.copy %2, %alloc : memref<256x128x13x13xf32> to memref<256x128x13x13xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x192x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x192x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x13x13xf32>\n    return %3 : tensor<256x128x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x192x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x192x28x28xf32>) -> tensor<256x192x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x192x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x192x3x3xf32>) -> tensor<128x192x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x28x28xf32>, tensor<128x192x3x3xf32>) outs (%init: tensor<256x128x13x13xf32>) -> tensor<256x128x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 36218410976
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x64x7x7xf32>, %filter: tensor<512x64x1x1xf32>, %init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n  return %ret : tensor<256x512x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x7x7xf32>, %arg1: tensor<512x64x1x1xf32>, %arg2: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x4x4xf32>\n    memref.copy %2, %alloc : memref<256x512x4x4xf32> to memref<256x512x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x4x4xf32>\n    return %3 : tensor<256x512x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x7x7xf32>) -> tensor<256x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x64x1x1xf32>) -> tensor<512x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x7x7xf32>, tensor<512x64x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 384425856
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<96x128x1x1xf32>) outs (%init: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<96x128x1x1xf32>) outs (%init: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x128x120x120xf32>, %filter: tensor<96x128x1x1xf32>, %init: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<96x128x1x1xf32>) outs (%init: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32>\n  return %ret : tensor<256x96x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x120x120xf32>, %arg1: tensor<96x128x1x1xf32>, %arg2: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x60x60xf32>\n    memref.copy %2, %alloc : memref<256x96x60x60xf32> to memref<256x96x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x128x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x60x60xf32>\n    return %3 : tensor<256x96x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x120x120xf32>) -> tensor<256x128x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x128x1x1xf32>) -> tensor<96x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x120x120xf32>, tensor<96x128x1x1xf32>) outs (%init: tensor<256x96x60x60xf32>) -> tensor<256x96x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 37997019799
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<48x48x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<48x48x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x28x28xf32>, %filter: tensor<48x48x3x3xf32>, %init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<48x48x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n  return %ret : tensor<128x48x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x28x28xf32>, %arg1: tensor<48x48x3x3xf32>, %arg2: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x48x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x48x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x13x13xf32>\n    memref.copy %2, %alloc : memref<128x48x13x13xf32> to memref<128x48x13x13xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x48x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x48x13x13xf32>\n    return %3 : tensor<128x48x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x28x28xf32>) -> tensor<128x48x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x48x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x48x3x3xf32>) -> tensor<48x48x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x48x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<48x48x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1680292768
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x28x28xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x28x28xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x28x28xf32>, %filter: tensor<128x384x1x1xf32>, %init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x28x28xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n  return %ret : tensor<128x128x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x28x28xf32>, %arg1: tensor<128x384x1x1xf32>, %arg2: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x384x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x14x14xf32>\n    memref.copy %2, %alloc : memref<128x128x14x14xf32> to memref<128x128x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x384x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x14x14xf32>\n    return %3 : tensor<128x128x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x28x28xf32>) -> tensor<128x384x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x384x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x384x1x1xf32>) -> tensor<128x384x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x28x28xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4532193945
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x120x120xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x120x120xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x120x120xf32>, %filter: tensor<32x48x1x1xf32>, %init: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x120x120xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n  return %ret : tensor<128x32x120x120xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x120x120xf32>, %arg1: tensor<32x48x1x1xf32>, %arg2: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x120x120xf32>\n    memref.copy %2, %alloc : memref<128x32x120x120xf32> to memref<128x32x120x120xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 120 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x120x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x120x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x120x120xf32>\n    return %3 : tensor<128x32x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x120x120xf32>) -> tensor<128x48x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x48x1x1xf32>) -> tensor<32x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x120x120xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x120x120xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ],
        [
          "%arg6",
          0,
          120,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 8326190632
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x14x14xf32>, %filter: tensor<128x384x1x1xf32>, %init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n  return %ret : tensor<128x128x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x14x14xf32>, %arg1: tensor<128x384x1x1xf32>, %arg2: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x384x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x14x14xf32>\n    memref.copy %2, %alloc : memref<128x128x14x14xf32> to memref<128x128x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x384x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x14x14xf32>\n    return %3 : tensor<128x128x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x384x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x384x1x1xf32>) -> tensor<128x384x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<128x384x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4494276456
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x224x224xf32>, %filter: tensor<64x32x1x1xf32>, %init: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>\n  return %ret : tensor<256x64x112x112xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x224x224xf32>, %arg1: tensor<64x32x1x1xf32>, %arg2: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x224x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x112x112xf32>\n    memref.copy %2, %alloc : memref<256x64x112x112xf32> to memref<256x64x112x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x224x224xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x112x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x112x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x112x112xf32>\n    return %3 : tensor<256x64x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x224x224xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x32x1x1xf32>) -> tensor<64x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x112x112xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<64x32x1x1xf32>) outs (%init: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ],
        [
          "%arg6",
          0,
          112,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 15614700447
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<256x48x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<256x48x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x15x15xf32>, %filter: tensor<256x48x7x7xf32>, %init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<256x48x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n  return %ret : tensor<128x256x9x9xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x15x15xf32>, %arg1: tensor<256x48x7x7xf32>, %arg2: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x48x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x9x9xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x9x9xf32>\n    memref.copy %2, %alloc : memref<128x256x9x9xf32> to memref<128x256x9x9xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 9 {\n          affine.for %arg6 = 0 to 9 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x48x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x9x9xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x9x9xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x9x9xf32>\n    return %3 : tensor<128x256x9x9xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x9x9xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x48x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x48x7x7xf32>) -> tensor<256x48x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x256x9x9xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x15x15xf32>, tensor<256x48x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x9x9xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x9x9xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          9,
          1
        ],
        [
          "%arg6",
          0,
          9,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 23557236415
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<256x32x3x3xf32>) outs (%init: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<256x32x3x3xf32>) outs (%init: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x120x120xf32>, %filter: tensor<256x32x3x3xf32>, %init: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<256x32x3x3xf32>) outs (%init: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32>\n  return %ret : tensor<128x256x59x59xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x120x120xf32>, %arg1: tensor<256x32x3x3xf32>, %arg2: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x32x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x59x59xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x59x59xf32>\n    memref.copy %2, %alloc : memref<128x256x59x59xf32> to memref<128x256x59x59xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 59 {\n          affine.for %arg6 = 0 to 59 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x32x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x59x59xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x59x59xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x59x59xf32>\n    return %3 : tensor<128x256x59x59xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x59x59xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x32x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x32x3x3xf32>) -> tensor<256x32x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x256x59x59xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<256x32x3x3xf32>) outs (%init: tensor<128x256x59x59xf32>) -> tensor<128x256x59x59xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x59x59xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x59x59xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          59,
          1
        ],
        [
          "%arg6",
          0,
          59,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 121803465730
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x228x228xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x228x228xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x48x228x228xf32>, %filter: tensor<128x48x1x1xf32>, %init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x228x228xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n  return %ret : tensor<256x128x114x114xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x228x228xf32>, %arg1: tensor<128x48x1x1xf32>, %arg2: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x228x228xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x114x114xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x114x114xf32>\n    memref.copy %2, %alloc : memref<256x128x114x114xf32> to memref<256x128x114x114xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 114 {\n          affine.for %arg6 = 0 to 114 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x48x228x228xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x114x114xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x114x114xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x114x114xf32>\n    return %3 : tensor<256x128x114x114xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x114x114xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x48x228x228xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x48x228x228xf32>) -> tensor<256x48x228x228xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x48x1x1xf32>) -> tensor<128x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x114x114xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x228x228xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<256x128x114x114xf32>) -> tensor<256x128x114x114xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x114x114xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x114x114xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          114,
          1
        ],
        [
          "%arg6",
          0,
          114,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 53852217352
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x228x228xf32>, tensor<32x64x3x3xf32>) outs (%init: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x228x228xf32>, tensor<32x64x3x3xf32>) outs (%init: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x228x228xf32>, %filter: tensor<32x64x3x3xf32>, %init: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x228x228xf32>, tensor<32x64x3x3xf32>) outs (%init: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32>\n  return %ret : tensor<128x32x113x113xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x228x228xf32>, %arg1: tensor<32x64x3x3xf32>, %arg2: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x64x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x228x228xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x113x113xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x113x113xf32>\n    memref.copy %2, %alloc : memref<128x32x113x113xf32> to memref<128x32x113x113xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 113 {\n          affine.for %arg6 = 0 to 113 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x228x228xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x64x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x113x113xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x113x113xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x113x113xf32>\n    return %3 : tensor<128x32x113x113xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x113x113xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x228x228xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x228x228xf32>) -> tensor<128x64x228x228xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x64x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x64x3x3xf32>) -> tensor<32x64x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x113x113xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x228x228xf32>, tensor<32x64x3x3xf32>) outs (%init: tensor<128x32x113x113xf32>) -> tensor<128x32x113x113xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x113x113xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x113x113xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          113,
          1
        ],
        [
          "%arg6",
          0,
          113,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 113454647773
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x7x7xf32>, %filter: tensor<128x64x1x1xf32>, %init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n  return %ret : tensor<128x128x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x7x7xf32>, %arg1: tensor<128x64x1x1xf32>, %arg2: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x4x4xf32>\n    memref.copy %2, %alloc : memref<128x128x4x4xf32> to memref<128x128x4x4xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x4x4xf32>\n    return %3 : tensor<128x128x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x64x1x1xf32>) -> tensor<128x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 48030274
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x7x7xf32>, tensor<192x32x1x1xf32>) outs (%init: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x7x7xf32>, tensor<192x32x1x1xf32>) outs (%init: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x7x7xf32>, %filter: tensor<192x32x1x1xf32>, %init: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x7x7xf32>, tensor<192x32x1x1xf32>) outs (%init: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32>\n  return %ret : tensor<256x192x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x7x7xf32>, %arg1: tensor<192x32x1x1xf32>, %arg2: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x192x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x4x4xf32>\n    memref.copy %2, %alloc : memref<256x192x4x4xf32> to memref<256x192x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x192x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x192x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x192x4x4xf32>\n    return %3 : tensor<256x192x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x7x7xf32>) -> tensor<256x32x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x32x1x1xf32>) -> tensor<192x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x192x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x7x7xf32>, tensor<192x32x1x1xf32>) outs (%init: tensor<256x192x4x4xf32>) -> tensor<256x192x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 58409543
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x14x14xf32>, tensor<192x32x3x3xf32>) outs (%init: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x14x14xf32>, tensor<192x32x3x3xf32>) outs (%init: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x14x14xf32>, %filter: tensor<192x32x3x3xf32>, %init: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x14x14xf32>, tensor<192x32x3x3xf32>) outs (%init: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32>\n  return %ret : tensor<128x192x6x6xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x14x14xf32>, %arg1: tensor<192x32x3x3xf32>, %arg2: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x32x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x6x6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x6x6xf32>\n    memref.copy %2, %alloc : memref<128x192x6x6xf32> to memref<128x192x6x6xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 6 {\n          affine.for %arg6 = 0 to 6 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x32x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x6x6xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x6x6xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x6x6xf32>\n    return %3 : tensor<128x192x6x6xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x6x6xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x32x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x32x3x3xf32>) -> tensor<192x32x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x6x6xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x14x14xf32>, tensor<192x32x3x3xf32>) outs (%init: tensor<128x192x6x6xf32>) -> tensor<128x192x6x6xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x6x6xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x6x6xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          6,
          1
        ],
        [
          "%arg6",
          0,
          6,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 948514874
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<96x288x3x3xf32>) outs (%init: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<96x288x3x3xf32>) outs (%init: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x7x7xf32>, %filter: tensor<96x288x3x3xf32>, %init: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<96x288x3x3xf32>) outs (%init: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32>\n  return %ret : tensor<256x96x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x7x7xf32>, %arg1: tensor<96x288x3x3xf32>, %arg2: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x288x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x5x5xf32>\n    memref.copy %2, %alloc : memref<256x96x5x5xf32> to memref<256x96x5x5xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x288x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x5x5xf32>\n    return %3 : tensor<256x96x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x7x7xf32>) -> tensor<256x288x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x288x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x288x3x3xf32>) -> tensor<96x288x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<96x288x3x3xf32>) outs (%init: tensor<256x96x5x5xf32>) -> tensor<256x96x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 6013653742
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x150x150xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x150x150xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x256x150x150xf32>, %filter: tensor<32x256x1x1xf32>, %init: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x150x150xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>\n  return %ret : tensor<128x32x150x150xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x150x150xf32>, %arg1: tensor<32x256x1x1xf32>, %arg2: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x256x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x150x150xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x150x150xf32>\n    memref.copy %2, %alloc : memref<128x32x150x150xf32> to memref<128x32x150x150xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 150 {\n          affine.for %arg6 = 0 to 150 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x256x150x150xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x256x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x150x150xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x150x150xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x150x150xf32>\n    return %3 : tensor<128x32x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x256x150x150xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x256x150x150xf32>) -> tensor<128x256x150x150xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x256x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x256x1x1xf32>) -> tensor<32x256x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x150x150xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x150x150xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ],
        [
          "%arg6",
          0,
          150,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 90581438306
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x15x15xf32>, tensor<192x288x1x1xf32>) outs (%init: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x15x15xf32>, tensor<192x288x1x1xf32>) outs (%init: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x15x15xf32>, %filter: tensor<192x288x1x1xf32>, %init: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x15x15xf32>, tensor<192x288x1x1xf32>) outs (%init: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>\n  return %ret : tensor<256x192x15x15xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x15x15xf32>, %arg1: tensor<192x288x1x1xf32>, %arg2: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x288x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x192x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x15x15xf32>\n    memref.copy %2, %alloc : memref<256x192x15x15xf32> to memref<256x192x15x15xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 15 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x288x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x192x15x15xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x192x15x15xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x192x15x15xf32>\n    return %3 : tensor<256x192x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x15x15xf32>) -> tensor<256x288x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x288x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x288x1x1xf32>) -> tensor<192x288x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x192x15x15xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x15x15xf32>, tensor<192x288x1x1xf32>) outs (%init: tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ],
        [
          "%arg6",
          0,
          15,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 11415431571
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x120x120xf32>, tensor<384x64x1x1xf32>) outs (%init: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x120x120xf32>, tensor<384x64x1x1xf32>) outs (%init: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x120x120xf32>, %filter: tensor<384x64x1x1xf32>, %init: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x120x120xf32>, tensor<384x64x1x1xf32>) outs (%init: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32>\n  return %ret : tensor<128x384x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x120x120xf32>, %arg1: tensor<384x64x1x1xf32>, %arg2: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x384x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x60x60xf32>\n    memref.copy %2, %alloc : memref<128x384x60x60xf32> to memref<128x384x60x60xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x384x60x60xf32>\n    return %3 : tensor<128x384x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x120x120xf32>) -> tensor<128x64x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x64x1x1xf32>) -> tensor<384x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x384x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x120x120xf32>, tensor<384x64x1x1xf32>) outs (%init: tensor<128x384x60x60xf32>) -> tensor<128x384x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 32978972900
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<128x512x1x1xf32>) outs (%init: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<128x512x1x1xf32>) outs (%init: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x512x14x14xf32>, %filter: tensor<128x512x1x1xf32>, %init: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<128x512x1x1xf32>) outs (%init: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n  return %ret : tensor<128x128x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x14x14xf32>, %arg1: tensor<128x512x1x1xf32>, %arg2: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x512x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x7x7xf32>\n    memref.copy %2, %alloc : memref<128x128x7x7xf32> to memref<128x128x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x512x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x512x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x7x7xf32>\n    return %3 : tensor<128x128x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x512x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x512x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x512x1x1xf32>) -> tensor<128x512x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x14x14xf32>, tensor<128x512x1x1xf32>) outs (%init: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1524026793
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<64x512x3x3xf32>) outs (%init: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<64x512x3x3xf32>) outs (%init: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x512x15x15xf32>, %filter: tensor<64x512x3x3xf32>, %init: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<64x512x3x3xf32>) outs (%init: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32>\n  return %ret : tensor<128x64x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x15x15xf32>, %arg1: tensor<64x512x3x3xf32>, %arg2: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x512x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x13x13xf32>\n    memref.copy %2, %alloc : memref<128x64x13x13xf32> to memref<128x64x13x13xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x512x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x512x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x13x13xf32>\n    return %3 : tensor<128x64x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x512x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x512x15x15xf32>) -> tensor<128x512x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x512x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x512x3x3xf32>) -> tensor<64x512x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x64x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<64x512x3x3xf32>) outs (%init: tensor<128x64x13x13xf32>) -> tensor<128x64x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 24212143624
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<32x48x7x7xf32>) outs (%init: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<32x48x7x7xf32>) outs (%init: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x48x28x28xf32>, %filter: tensor<32x48x7x7xf32>, %init: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<32x48x7x7xf32>) outs (%init: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32>\n  return %ret : tensor<256x32x22x22xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x28x28xf32>, %arg1: tensor<32x48x7x7xf32>, %arg2: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x48x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x22x22xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x22x22xf32>\n    memref.copy %2, %alloc : memref<256x32x22x22xf32> to memref<256x32x22x22xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 22 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x48x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x48x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x22x22xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x22x22xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x22x22xf32>\n    return %3 : tensor<256x32x22x22xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x22x22xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x48x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x48x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x48x7x7xf32>) -> tensor<32x48x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x22x22xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<32x48x7x7xf32>) outs (%init: tensor<256x32x22x22xf32>) -> tensor<256x32x22x22xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x22x22xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x22x22xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          22,
          1
        ],
        [
          "%arg6",
          0,
          22,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 35193384502
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x224x224xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x224x224xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x224x224xf32>, %filter: tensor<512x32x1x1xf32>, %init: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x224x224xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>\n  return %ret : tensor<128x512x112x112xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x224x224xf32>, %arg1: tensor<512x32x1x1xf32>, %arg2: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x224x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x112x112xf32>\n    memref.copy %2, %alloc : memref<128x512x112x112xf32> to memref<128x512x112x112xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x224x224xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x112x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x112x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x112x112xf32>\n    return %3 : tensor<128x512x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x224x224xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x224x224xf32>) -> tensor<128x32x224x224xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x32x1x1xf32>) -> tensor<512x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x512x112x112xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x224x224xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ],
        [
          "%arg6",
          0,
          112,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 62336630786
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<48x512x1x1xf32>) outs (%init: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<48x512x1x1xf32>) outs (%init: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x512x15x15xf32>, %filter: tensor<48x512x1x1xf32>, %init: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<48x512x1x1xf32>) outs (%init: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n  return %ret : tensor<128x48x15x15xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x15x15xf32>, %arg1: tensor<48x512x1x1xf32>, %arg2: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x512x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x48x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x15x15xf32>\n    memref.copy %2, %alloc : memref<128x48x15x15xf32> to memref<128x48x15x15xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 15 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x512x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x512x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x15x15xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x15x15xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x48x15x15xf32>\n    return %3 : tensor<128x48x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x512x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x512x15x15xf32>) -> tensor<128x512x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x512x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x512x1x1xf32>) -> tensor<48x512x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x512x15x15xf32>, tensor<48x512x1x1xf32>) outs (%init: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ],
        [
          "%arg6",
          0,
          15,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2627698278
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x28x28xf32>, tensor<64x64x3x3xf32>) outs (%init: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x28x28xf32>, tensor<64x64x3x3xf32>) outs (%init: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x64x28x28xf32>, %filter: tensor<64x64x3x3xf32>, %init: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x28x28xf32>, tensor<64x64x3x3xf32>) outs (%init: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32>\n  return %ret : tensor<256x64x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x28x28xf32>, %arg1: tensor<64x64x3x3xf32>, %arg2: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x64x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x13x13xf32>\n    memref.copy %2, %alloc : memref<256x64x13x13xf32> to memref<256x64x13x13xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x64x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x64x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x13x13xf32>\n    return %3 : tensor<256x64x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x28x28xf32>) -> tensor<256x64x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x64x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x64x3x3xf32>) -> tensor<64x64x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x28x28xf32>, tensor<64x64x3x3xf32>) outs (%init: tensor<256x64x13x13xf32>) -> tensor<256x64x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5988985959
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<192x64x3x3xf32>) outs (%init: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<192x64x3x3xf32>) outs (%init: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x7x7xf32>, %filter: tensor<192x64x3x3xf32>, %init: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<192x64x3x3xf32>) outs (%init: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32>\n  return %ret : tensor<128x192x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x7x7xf32>, %arg1: tensor<192x64x3x3xf32>, %arg2: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x64x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x5x5xf32>\n    memref.copy %2, %alloc : memref<128x192x5x5xf32> to memref<128x192x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x64x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x5x5xf32>\n    return %3 : tensor<128x192x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x64x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x64x3x3xf32>) -> tensor<192x64x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<192x64x3x3xf32>) outs (%init: tensor<128x192x5x5xf32>) -> tensor<128x192x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1327707841
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x120x120xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x120x120xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x120x120xf32>, %filter: tensor<384x32x1x1xf32>, %init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x120x120xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n  return %ret : tensor<256x384x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x120x120xf32>, %arg1: tensor<384x32x1x1xf32>, %arg2: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x60x60xf32>\n    memref.copy %2, %alloc : memref<256x384x60x60xf32> to memref<256x384x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x60x60xf32>\n    return %3 : tensor<256x384x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x120x120xf32>) -> tensor<256x32x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x32x1x1xf32>) -> tensor<384x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x120x120xf32>, tensor<384x32x1x1xf32>) outs (%init: tensor<256x384x60x60xf32>) -> tensor<256x384x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 27244424288
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x28x28xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x28x28xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x256x28x28xf32>, %filter: tensor<32x256x1x1xf32>, %init: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x28x28xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n  return %ret : tensor<256x32x28x28xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x28x28xf32>, %arg1: tensor<32x256x1x1xf32>, %arg2: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x256x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x28x28xf32>\n    memref.copy %2, %alloc : memref<256x32x28x28xf32> to memref<256x32x28x28xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x256x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x256x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x28x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x28x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x28x28xf32>\n    return %3 : tensor<256x32x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x256x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x256x28x28xf32>) -> tensor<256x256x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x256x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x256x1x1xf32>) -> tensor<32x256x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x28x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x28x28xf32>, tensor<32x256x1x1xf32>) outs (%init: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ],
        [
          "%arg6",
          0,
          28,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5898737416
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x28x28xf32>, tensor<128x32x1x1xf32>) outs (%init: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x28x28xf32>, tensor<128x32x1x1xf32>) outs (%init: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x28x28xf32>, %filter: tensor<128x32x1x1xf32>, %init: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x28x28xf32>, tensor<128x32x1x1xf32>) outs (%init: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>\n  return %ret : tensor<256x128x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x28x28xf32>, %arg1: tensor<128x32x1x1xf32>, %arg2: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x14x14xf32>\n    memref.copy %2, %alloc : memref<256x128x14x14xf32> to memref<256x128x14x14xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x14x14xf32>\n    return %3 : tensor<256x128x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x32x1x1xf32>) -> tensor<128x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x28x28xf32>, tensor<128x32x1x1xf32>) outs (%init: tensor<256x128x14x14xf32>) -> tensor<256x128x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 473636137
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x7x7xf32>, tensor<384x192x1x1xf32>) outs (%init: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x7x7xf32>, tensor<384x192x1x1xf32>) outs (%init: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x192x7x7xf32>, %filter: tensor<384x192x1x1xf32>, %init: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x7x7xf32>, tensor<384x192x1x1xf32>) outs (%init: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32>\n  return %ret : tensor<256x384x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x192x7x7xf32>, %arg1: tensor<384x192x1x1xf32>, %arg2: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x192x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x192x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x4x4xf32>\n    memref.copy %2, %alloc : memref<256x384x4x4xf32> to memref<256x384x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x192x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x192x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x4x4xf32>\n    return %3 : tensor<256x384x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x192x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x192x7x7xf32>) -> tensor<256x192x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x192x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x192x1x1xf32>) -> tensor<384x192x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x7x7xf32>, tensor<384x192x1x1xf32>) outs (%init: tensor<256x384x4x4xf32>) -> tensor<256x384x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1050265240
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x14x14xf32>, tensor<512x256x1x1xf32>) outs (%init: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x14x14xf32>, tensor<512x256x1x1xf32>) outs (%init: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x256x14x14xf32>, %filter: tensor<512x256x1x1xf32>, %init: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x14x14xf32>, tensor<512x256x1x1xf32>) outs (%init: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32>\n  return %ret : tensor<256x512x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x14x14xf32>, %arg1: tensor<512x256x1x1xf32>, %arg2: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x256x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x7x7xf32>\n    memref.copy %2, %alloc : memref<256x512x7x7xf32> to memref<256x512x7x7xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x256x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x256x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x7x7xf32>\n    return %3 : tensor<256x512x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x256x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x256x14x14xf32>) -> tensor<256x256x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x256x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x256x1x1xf32>) -> tensor<512x256x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x14x14xf32>, tensor<512x256x1x1xf32>) outs (%init: tensor<256x512x7x7xf32>) -> tensor<256x512x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5847316987
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x7x7xf32>, tensor<64x240x1x1xf32>) outs (%init: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x7x7xf32>, tensor<64x240x1x1xf32>) outs (%init: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x240x7x7xf32>, %filter: tensor<64x240x1x1xf32>, %init: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x7x7xf32>, tensor<64x240x1x1xf32>) outs (%init: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32>\n  return %ret : tensor<256x64x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x240x7x7xf32>, %arg1: tensor<64x240x1x1xf32>, %arg2: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x240x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x240x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x4x4xf32>\n    memref.copy %2, %alloc : memref<256x64x4x4xf32> to memref<256x64x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x240x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x240x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x4x4xf32>\n    return %3 : tensor<256x64x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x240x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x240x7x7xf32>) -> tensor<256x240x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x240x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x240x1x1xf32>) -> tensor<64x240x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x7x7xf32>, tensor<64x240x1x1xf32>) outs (%init: tensor<256x64x4x4xf32>) -> tensor<256x64x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 222949055
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x56x56xf32>, tensor<96x512x1x1xf32>) outs (%init: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x56x56xf32>, tensor<96x512x1x1xf32>) outs (%init: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x512x56x56xf32>, %filter: tensor<96x512x1x1xf32>, %init: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x56x56xf32>, tensor<96x512x1x1xf32>) outs (%init: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>\n  return %ret : tensor<256x96x56x56xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x56x56xf32>, %arg1: tensor<96x512x1x1xf32>, %arg2: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x512x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x56x56xf32>\n    memref.copy %2, %alloc : memref<256x96x56x56xf32> to memref<256x96x56x56xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 512 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x512x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x512x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x56x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x56x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x56x56xf32>\n    return %3 : tensor<256x96x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x512x56x56xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x512x56x56xf32>) -> tensor<256x512x56x56xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x512x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x512x1x1xf32>) -> tensor<96x512x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x56x56xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x56x56xf32>, tensor<96x512x1x1xf32>) outs (%init: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ],
        [
          "%arg6",
          0,
          56,
          1
        ],
        [
          "%arg7",
          0,
          512,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 147036483449
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x15x15xf32>, tensor<256x256x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x15x15xf32>, tensor<256x256x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x256x15x15xf32>, %filter: tensor<256x256x7x7xf32>, %init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x15x15xf32>, tensor<256x256x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n  return %ret : tensor<128x256x9x9xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x15x15xf32>, %arg1: tensor<256x256x7x7xf32>, %arg2: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x256x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x9x9xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x9x9xf32>\n    memref.copy %2, %alloc : memref<128x256x9x9xf32> to memref<128x256x9x9xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 9 {\n          affine.for %arg6 = 0 to 9 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x256x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x256x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x9x9xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x256x9x9xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x9x9xf32>\n    return %3 : tensor<128x256x9x9xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x9x9xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x256x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x256x15x15xf32>) -> tensor<128x256x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x256x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x256x7x7xf32>) -> tensor<256x256x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x256x9x9xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x15x15xf32>, tensor<256x256x7x7xf32>) outs (%init: tensor<128x256x9x9xf32>) -> tensor<128x256x9x9xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x9x9xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x9x9xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          9,
          1
        ],
        [
          "%arg6",
          0,
          9,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 125904286959
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<240x128x3x3xf32>) outs (%init: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<240x128x3x3xf32>) outs (%init: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x128x7x7xf32>, %filter: tensor<240x128x3x3xf32>, %init: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<240x128x3x3xf32>) outs (%init: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32>\n  return %ret : tensor<128x240x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x7x7xf32>, %arg1: tensor<240x128x3x3xf32>, %arg2: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x128x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x240x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x5x5xf32>\n    memref.copy %2, %alloc : memref<128x240x5x5xf32> to memref<128x240x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x128x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x128x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x240x5x5xf32>\n    return %3 : tensor<128x240x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x128x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x128x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x128x3x3xf32>) -> tensor<240x128x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x240x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<240x128x3x3xf32>) outs (%init: tensor<128x240x5x5xf32>) -> tensor<128x240x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3331816577
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<192x240x7x7xf32>) outs (%init: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<192x240x7x7xf32>) outs (%init: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x240x14x14xf32>, %filter: tensor<192x240x7x7xf32>, %init: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<192x240x7x7xf32>) outs (%init: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32>\n  return %ret : tensor<128x192x8x8xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x240x14x14xf32>, %arg1: tensor<192x240x7x7xf32>, %arg2: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x240x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x240x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x8x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x8x8xf32>\n    memref.copy %2, %alloc : memref<128x192x8x8xf32> to memref<128x192x8x8xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x240x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x240x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x8x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x8x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x8x8xf32>\n    return %3 : tensor<128x192x8x8xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x8x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x240x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x240x14x14xf32>) -> tensor<128x240x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x240x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x240x7x7xf32>) -> tensor<192x240x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x8x8xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<192x240x7x7xf32>) outs (%init: tensor<128x192x8x8xf32>) -> tensor<128x192x8x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x8x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x8x8xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          8,
          1
        ],
        [
          "%arg6",
          0,
          8,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 69952649953
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x130x130xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x130x130xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x130x130xf32>, %filter: tensor<512x32x1x1xf32>, %init: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x130x130xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32>\n  return %ret : tensor<256x512x65x65xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x130x130xf32>, %arg1: tensor<512x32x1x1xf32>, %arg2: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x130x130xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x65x65xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x65x65xf32>\n    memref.copy %2, %alloc : memref<256x512x65x65xf32> to memref<256x512x65x65xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 65 {\n          affine.for %arg6 = 0 to 65 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x130x130xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x65x65xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x65x65xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x65x65xf32>\n    return %3 : tensor<256x512x65x65xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x65x65xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x130x130xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x130x130xf32>) -> tensor<256x32x130x130xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x32x1x1xf32>) -> tensor<512x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x65x65xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x130x130xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x65x65xf32>) -> tensor<256x512x65x65xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x65x65xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x65x65xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          65,
          1
        ],
        [
          "%arg6",
          0,
          65,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 43908811178
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x28x28xf32>, tensor<48x128x7x7xf32>) outs (%init: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x28x28xf32>, tensor<48x128x7x7xf32>) outs (%init: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x128x28x28xf32>, %filter: tensor<48x128x7x7xf32>, %init: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x28x28xf32>, tensor<48x128x7x7xf32>) outs (%init: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32>\n  return %ret : tensor<256x48x22x22xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x28x28xf32>, %arg1: tensor<48x128x7x7xf32>, %arg2: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x128x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x48x22x22xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x48x22x22xf32>\n    memref.copy %2, %alloc : memref<256x48x22x22xf32> to memref<256x48x22x22xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 22 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x128x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x128x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x48x22x22xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x48x22x22xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x48x22x22xf32>\n    return %3 : tensor<256x48x22x22xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x48x22x22xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x28x28xf32>) -> tensor<256x128x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x128x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x128x7x7xf32>) -> tensor<48x128x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x48x22x22xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x28x28xf32>, tensor<48x128x7x7xf32>) outs (%init: tensor<256x48x22x22xf32>) -> tensor<256x48x22x22xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x48x22x22xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x48x22x22xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          22,
          1
        ],
        [
          "%arg6",
          0,
          22,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 141094755027
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x14x14xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x14x14xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x192x14x14xf32>, %filter: tensor<96x192x1x1xf32>, %init: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x14x14xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>\n  return %ret : tensor<128x96x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x192x14x14xf32>, %arg1: tensor<96x192x1x1xf32>, %arg2: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x192x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x192x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x96x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x14x14xf32>\n    memref.copy %2, %alloc : memref<128x96x14x14xf32> to memref<128x96x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x192x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x192x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x96x14x14xf32>\n    return %3 : tensor<128x96x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x192x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x192x14x14xf32>) -> tensor<128x192x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x192x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x192x1x1xf32>) -> tensor<96x192x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x96x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x14x14xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1608337474
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x7x7xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x7x7xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x96x7x7xf32>, %filter: tensor<512x96x1x1xf32>, %init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x7x7xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n  return %ret : tensor<256x512x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x96x7x7xf32>, %arg1: tensor<512x96x1x1xf32>, %arg2: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x96x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x4x4xf32>\n    memref.copy %2, %alloc : memref<256x512x4x4xf32> to memref<256x512x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x96x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x4x4xf32>\n    return %3 : tensor<256x512x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x96x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x96x7x7xf32>) -> tensor<256x96x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x96x1x1xf32>) -> tensor<512x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x7x7xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<256x512x4x4xf32>) -> tensor<256x512x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 638226144
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x120x120xf32>, tensor<64x64x1x1xf32>) outs (%init: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x120x120xf32>, tensor<64x64x1x1xf32>) outs (%init: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x64x120x120xf32>, %filter: tensor<64x64x1x1xf32>, %init: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x120x120xf32>, tensor<64x64x1x1xf32>) outs (%init: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32>\n  return %ret : tensor<256x64x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x120x120xf32>, %arg1: tensor<64x64x1x1xf32>, %arg2: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x60x60xf32>\n    memref.copy %2, %alloc : memref<256x64x60x60xf32> to memref<256x64x60x60xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x64x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x60x60xf32>\n    return %3 : tensor<256x64x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x120x120xf32>) -> tensor<256x64x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x64x1x1xf32>) -> tensor<64x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x120x120xf32>, tensor<64x64x1x1xf32>) outs (%init: tensor<256x64x60x60xf32>) -> tensor<256x64x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 11052430177
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x130x130xf32>, tensor<48x64x1x1xf32>) outs (%init: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x130x130xf32>, tensor<48x64x1x1xf32>) outs (%init: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x130x130xf32>, %filter: tensor<48x64x1x1xf32>, %init: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x130x130xf32>, tensor<48x64x1x1xf32>) outs (%init: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32>\n  return %ret : tensor<128x48x65x65xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x130x130xf32>, %arg1: tensor<48x64x1x1xf32>, %arg2: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x64x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x130x130xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x48x65x65xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x65x65xf32>\n    memref.copy %2, %alloc : memref<128x48x65x65xf32> to memref<128x48x65x65xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 65 {\n          affine.for %arg6 = 0 to 65 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x130x130xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x64x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x65x65xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x65x65xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x48x65x65xf32>\n    return %3 : tensor<128x48x65x65xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x65x65xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x130x130xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x130x130xf32>) -> tensor<128x64x130x130xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x64x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x64x1x1xf32>) -> tensor<48x64x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x48x65x65xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x130x130xf32>, tensor<48x64x1x1xf32>) outs (%init: tensor<128x48x65x65xf32>) -> tensor<128x48x65x65xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x65x65xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x65x65xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          65,
          1
        ],
        [
          "%arg6",
          0,
          65,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5012038064
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x7x7xf32>) outs (%init: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x7x7xf32>) outs (%init: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x15x15xf32>, %filter: tensor<32x96x7x7xf32>, %init: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x7x7xf32>) outs (%init: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32>\n  return %ret : tensor<128x32x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x15x15xf32>, %arg1: tensor<32x96x7x7xf32>, %arg2: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x96x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x5x5xf32>\n    memref.copy %2, %alloc : memref<128x32x5x5xf32> to memref<128x32x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x96x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x5x5xf32>\n    return %3 : tensor<128x32x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x96x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x96x7x7xf32>) -> tensor<32x96x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x7x7xf32>) outs (%init: tensor<128x32x5x5xf32>) -> tensor<128x32x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1819432327
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<288x128x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<288x128x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x128x7x7xf32>, %filter: tensor<288x128x1x1xf32>, %init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<288x128x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n  return %ret : tensor<128x288x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x7x7xf32>, %arg1: tensor<288x128x1x1xf32>, %arg2: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x288x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x7x7xf32>\n    memref.copy %2, %alloc : memref<128x288x7x7xf32> to memref<128x288x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 288 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x128x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<288x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x288x7x7xf32>\n    return %3 : tensor<128x288x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x128x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<288x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<288x128x1x1xf32>) -> tensor<288x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x288x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x128x7x7xf32>, tensor<288x128x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          288,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 766465536
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x56x56xf32>, tensor<240x32x3x3xf32>) outs (%init: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x56x56xf32>, tensor<240x32x3x3xf32>) outs (%init: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x56x56xf32>, %filter: tensor<240x32x3x3xf32>, %init: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x56x56xf32>, tensor<240x32x3x3xf32>) outs (%init: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32>\n  return %ret : tensor<128x240x54x54xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x56x56xf32>, %arg1: tensor<240x32x3x3xf32>, %arg2: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x32x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x240x54x54xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x54x54xf32>\n    memref.copy %2, %alloc : memref<128x240x54x54xf32> to memref<128x240x54x54xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 54 {\n          affine.for %arg6 = 0 to 54 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x32x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x54x54xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x54x54xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x240x54x54xf32>\n    return %3 : tensor<128x240x54x54xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x54x54xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x56x56xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x56x56xf32>) -> tensor<128x32x56x56xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x32x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x32x3x3xf32>) -> tensor<240x32x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x240x54x54xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x56x56xf32>, tensor<240x32x3x3xf32>) outs (%init: tensor<128x240x54x54xf32>) -> tensor<128x240x54x54xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x54x54xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x54x54xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          54,
          1
        ],
        [
          "%arg6",
          0,
          54,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 95809114353
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x3x3xf32>) outs (%init: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x3x3xf32>) outs (%init: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x15x15xf32>, %filter: tensor<32x96x3x3xf32>, %init: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x3x3xf32>) outs (%init: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32>\n  return %ret : tensor<128x32x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x15x15xf32>, %arg1: tensor<32x96x3x3xf32>, %arg2: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x96x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x13x13xf32>\n    memref.copy %2, %alloc : memref<128x32x13x13xf32> to memref<128x32x13x13xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x96x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x13x13xf32>\n    return %3 : tensor<128x32x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x96x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x96x3x3xf32>) -> tensor<32x96x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x15x15xf32>, tensor<32x96x3x3xf32>) outs (%init: tensor<128x32x13x13xf32>) -> tensor<128x32x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2251299068
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<384x240x3x3xf32>) outs (%init: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<384x240x3x3xf32>) outs (%init: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x240x14x14xf32>, %filter: tensor<384x240x3x3xf32>, %init: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<384x240x3x3xf32>) outs (%init: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32>\n  return %ret : tensor<256x384x6x6xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x240x14x14xf32>, %arg1: tensor<384x240x3x3xf32>, %arg2: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x240x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x240x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x384x6x6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x6x6xf32>\n    memref.copy %2, %alloc : memref<256x384x6x6xf32> to memref<256x384x6x6xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 6 {\n          affine.for %arg6 = 0 to 6 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x240x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x240x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x6x6xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x384x6x6xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x384x6x6xf32>\n    return %3 : tensor<256x384x6x6xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x6x6xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x240x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x240x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x240x3x3xf32>) -> tensor<384x240x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x384x6x6xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x240x14x14xf32>, tensor<384x240x3x3xf32>) outs (%init: tensor<256x384x6x6xf32>) -> tensor<256x384x6x6xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x6x6xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x6x6xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          6,
          1
        ],
        [
          "%arg6",
          0,
          6,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 28919288926
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<288x96x1x1xf32>) outs (%init: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<288x96x1x1xf32>) outs (%init: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x28x28xf32>, %filter: tensor<288x96x1x1xf32>, %init: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<288x96x1x1xf32>) outs (%init: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32>\n  return %ret : tensor<128x288x28x28xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x28x28xf32>, %arg1: tensor<288x96x1x1xf32>, %arg2: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x288x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x28x28xf32>\n    memref.copy %2, %alloc : memref<128x288x28x28xf32> to memref<128x288x28x28xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 288 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<288x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x28x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x28x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x288x28x28xf32>\n    return %3 : tensor<128x288x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<288x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<288x96x1x1xf32>) -> tensor<288x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x288x28x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<288x96x1x1xf32>) outs (%init: tensor<128x288x28x28xf32>) -> tensor<128x288x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          288,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ],
        [
          "%arg6",
          0,
          28,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 9002802320
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<48x96x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<48x96x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x28x28xf32>, %filter: tensor<48x96x3x3xf32>, %init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<48x96x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n  return %ret : tensor<128x48x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x28x28xf32>, %arg1: tensor<48x96x3x3xf32>, %arg2: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x96x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x48x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x13x13xf32>\n    memref.copy %2, %alloc : memref<128x48x13x13xf32> to memref<128x48x13x13xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x96x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x48x13x13xf32>\n    return %3 : tensor<128x48x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x96x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x96x3x3xf32>) -> tensor<48x96x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x48x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x28x28xf32>, tensor<48x96x3x3xf32>) outs (%init: tensor<128x48x13x13xf32>) -> tensor<128x48x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3382129684
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x7x7xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x7x7xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x192x7x7xf32>, %filter: tensor<96x192x1x1xf32>, %init: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x7x7xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n  return %ret : tensor<128x96x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x192x7x7xf32>, %arg1: tensor<96x192x1x1xf32>, %arg2: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x192x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x192x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x96x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x7x7xf32>\n    memref.copy %2, %alloc : memref<128x96x7x7xf32> to memref<128x96x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x192x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x192x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x96x7x7xf32>\n    return %3 : tensor<128x96x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x192x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x192x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x192x1x1xf32>) -> tensor<96x192x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x96x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x7x7xf32>, tensor<96x192x1x1xf32>) outs (%init: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 401377572
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<288x48x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<288x48x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x14x14xf32>, %filter: tensor<288x48x1x1xf32>, %init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<288x48x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n  return %ret : tensor<128x288x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x14x14xf32>, %arg1: tensor<288x48x1x1xf32>, %arg2: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x288x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x7x7xf32>\n    memref.copy %2, %alloc : memref<128x288x7x7xf32> to memref<128x288x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 288 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<288x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x288x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x288x7x7xf32>\n    return %3 : tensor<128x288x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x14x14xf32>) -> tensor<128x48x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<288x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<288x48x1x1xf32>) -> tensor<288x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x288x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<288x48x1x1xf32>) outs (%init: tensor<128x288x7x7xf32>) -> tensor<128x288x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          288,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 221564071
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<128x288x3x3xf32>) outs (%init: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<128x288x3x3xf32>) outs (%init: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x7x7xf32>, %filter: tensor<128x288x3x3xf32>, %init: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<128x288x3x3xf32>) outs (%init: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32>\n  return %ret : tensor<256x128x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x7x7xf32>, %arg1: tensor<128x288x3x3xf32>, %arg2: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x288x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x5x5xf32>\n    memref.copy %2, %alloc : memref<256x128x5x5xf32> to memref<256x128x5x5xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x288x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x5x5xf32>\n    return %3 : tensor<256x128x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x7x7xf32>) -> tensor<256x288x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x288x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x288x3x3xf32>) -> tensor<128x288x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x7x7xf32>, tensor<128x288x3x3xf32>) outs (%init: tensor<256x128x5x5xf32>) -> tensor<256x128x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 8026251843
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x28x28xf32>, tensor<256x288x3x3xf32>) outs (%init: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x28x28xf32>, tensor<256x288x3x3xf32>) outs (%init: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x288x28x28xf32>, %filter: tensor<256x288x3x3xf32>, %init: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x28x28xf32>, tensor<256x288x3x3xf32>) outs (%init: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32>\n  return %ret : tensor<256x256x13x13xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x288x28x28xf32>, %arg1: tensor<256x288x3x3xf32>, %arg2: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x288x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x288x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x13x13xf32>\n    memref.copy %2, %alloc : memref<256x256x13x13xf32> to memref<256x256x13x13xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 288 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x288x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x288x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x256x13x13xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x256x13x13xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x13x13xf32>\n    return %3 : tensor<256x256x13x13xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x288x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x288x28x28xf32>) -> tensor<256x288x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x288x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x288x3x3xf32>) -> tensor<256x288x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x256x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x288x28x28xf32>, tensor<256x288x3x3xf32>) outs (%init: tensor<256x256x13x13xf32>) -> tensor<256x256x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x13x13xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          13,
          1
        ],
        [
          "%arg6",
          0,
          13,
          1
        ],
        [
          "%arg7",
          0,
          288,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 108642895324
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<64x384x3x3xf32>) outs (%init: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<64x384x3x3xf32>) outs (%init: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x14x14xf32>, %filter: tensor<64x384x3x3xf32>, %init: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<64x384x3x3xf32>) outs (%init: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32>\n  return %ret : tensor<128x64x12x12xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x14x14xf32>, %arg1: tensor<64x384x3x3xf32>, %arg2: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x384x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x12x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x12x12xf32>\n    memref.copy %2, %alloc : memref<128x64x12x12xf32> to memref<128x64x12x12xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 12 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x384x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x12x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x12x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x12x12xf32>\n    return %3 : tensor<128x64x12x12xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x12x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x384x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x384x3x3xf32>) -> tensor<64x384x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x64x12x12xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x14x14xf32>, tensor<64x384x3x3xf32>) outs (%init: tensor<128x64x12x12xf32>) -> tensor<128x64x12x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x12x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x12x12xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          12,
          1
        ],
        [
          "%arg6",
          0,
          12,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 15425154946
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<96x32x1x1xf32>) outs (%init: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<96x32x1x1xf32>) outs (%init: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x120x120xf32>, %filter: tensor<96x32x1x1xf32>, %init: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<96x32x1x1xf32>) outs (%init: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32>\n  return %ret : tensor<128x96x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x120x120xf32>, %arg1: tensor<96x32x1x1xf32>, %arg2: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x96x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x60x60xf32>\n    memref.copy %2, %alloc : memref<128x96x60x60xf32> to memref<128x96x60x60xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x96x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x96x60x60xf32>\n    return %3 : tensor<128x96x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x32x1x1xf32>) -> tensor<96x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x96x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x120x120xf32>, tensor<96x32x1x1xf32>) outs (%init: tensor<128x96x60x60xf32>) -> tensor<128x96x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3418431174
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x150x150xf32>, tensor<240x192x1x1xf32>) outs (%init: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x150x150xf32>, tensor<240x192x1x1xf32>) outs (%init: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x192x150x150xf32>, %filter: tensor<240x192x1x1xf32>, %init: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x150x150xf32>, tensor<240x192x1x1xf32>) outs (%init: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32>\n  return %ret : tensor<128x240x75x75xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x192x150x150xf32>, %arg1: tensor<240x192x1x1xf32>, %arg2: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x192x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x192x150x150xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x240x75x75xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x75x75xf32>\n    memref.copy %2, %alloc : memref<128x240x75x75xf32> to memref<128x240x75x75xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 75 {\n          affine.for %arg6 = 0 to 75 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x192x150x150xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x192x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x75x75xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x75x75xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x240x75x75xf32>\n    return %3 : tensor<128x240x75x75xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x75x75xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x192x150x150xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x192x150x150xf32>) -> tensor<128x192x150x150xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x192x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x192x1x1xf32>) -> tensor<240x192x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x240x75x75xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x192x150x150xf32>, tensor<240x192x1x1xf32>) outs (%init: tensor<128x240x75x75xf32>) -> tensor<128x240x75x75xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x75x75xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x75x75xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          75,
          1
        ],
        [
          "%arg6",
          0,
          75,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 117201674975
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x28x28xf32>, %filter: tensor<128x48x1x1xf32>, %init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n  return %ret : tensor<128x128x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x28x28xf32>, %arg1: tensor<128x48x1x1xf32>, %arg2: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x14x14xf32>\n    memref.copy %2, %alloc : memref<128x128x14x14xf32> to memref<128x128x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x14x14xf32>\n    return %3 : tensor<128x128x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x28x28xf32>) -> tensor<128x48x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x48x1x1xf32>) -> tensor<128x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 394376872
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x7x7xf32>, tensor<128x128x1x1xf32>) outs (%init: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x7x7xf32>, tensor<128x128x1x1xf32>) outs (%init: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x128x7x7xf32>, %filter: tensor<128x128x1x1xf32>, %init: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x7x7xf32>, tensor<128x128x1x1xf32>) outs (%init: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32>\n  return %ret : tensor<256x128x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x7x7xf32>, %arg1: tensor<128x128x1x1xf32>, %arg2: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x128x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x4x4xf32>\n    memref.copy %2, %alloc : memref<256x128x4x4xf32> to memref<256x128x4x4xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 128 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x128x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x128x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x4x4xf32>\n    return %3 : tensor<256x128x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x7x7xf32>) -> tensor<256x128x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x128x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x128x1x1xf32>) -> tensor<128x128x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x7x7xf32>, tensor<128x128x1x1xf32>) outs (%init: tensor<256x128x4x4xf32>) -> tensor<256x128x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          128,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 223402937
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x15x15xf32>, tensor<64x192x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x15x15xf32>, tensor<64x192x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x192x15x15xf32>, %filter: tensor<64x192x7x7xf32>, %init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x15x15xf32>, tensor<64x192x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n  return %ret : tensor<256x64x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x192x15x15xf32>, %arg1: tensor<64x192x7x7xf32>, %arg2: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x192x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x192x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x5x5xf32>\n    memref.copy %2, %alloc : memref<256x64x5x5xf32> to memref<256x64x5x5xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 192 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x192x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x192x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x5x5xf32>\n    return %3 : tensor<256x64x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x192x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x192x15x15xf32>) -> tensor<256x192x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x192x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x192x7x7xf32>) -> tensor<64x192x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x192x15x15xf32>, tensor<64x192x7x7xf32>) outs (%init: tensor<256x64x5x5xf32>) -> tensor<256x64x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          192,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 14571322115
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x14x14xf32>, %filter: tensor<32x48x1x1xf32>, %init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n  return %ret : tensor<128x32x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x14x14xf32>, %arg1: tensor<32x48x1x1xf32>, %arg2: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x14x14xf32>\n    memref.copy %2, %alloc : memref<128x32x14x14xf32> to memref<128x32x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x14x14xf32>\n    return %3 : tensor<128x32x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x14x14xf32>) -> tensor<128x48x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x48x1x1xf32>) -> tensor<32x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x14x14xf32>, tensor<32x48x1x1xf32>) outs (%init: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 98476092
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<384x96x1x1xf32>) outs (%init: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<384x96x1x1xf32>) outs (%init: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x14x14xf32>, %filter: tensor<384x96x1x1xf32>, %init: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<384x96x1x1xf32>) outs (%init: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n  return %ret : tensor<128x384x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x14x14xf32>, %arg1: tensor<384x96x1x1xf32>, %arg2: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x384x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x14x14xf32>\n    memref.copy %2, %alloc : memref<128x384x14x14xf32> to memref<128x384x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 384 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<384x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x384x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x384x14x14xf32>\n    return %3 : tensor<128x384x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<384x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<384x96x1x1xf32>) -> tensor<384x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x384x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<384x96x1x1xf32>) outs (%init: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          384,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2928462522
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x96x14x14xf32>, %filter: tensor<512x96x1x1xf32>, %init: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n  return %ret : tensor<128x512x14x14xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x96x14x14xf32>, %arg1: tensor<512x96x1x1xf32>, %arg2: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x96x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x14x14xf32>\n    memref.copy %2, %alloc : memref<128x512x14x14xf32> to memref<128x512x14x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x96x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x14x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x512x14x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x14x14xf32>\n    return %3 : tensor<128x512x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x96x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x96x14x14xf32>) -> tensor<128x96x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x96x1x1xf32>) -> tensor<512x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x512x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x96x14x14xf32>, tensor<512x96x1x1xf32>) outs (%init: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ],
        [
          "%arg6",
          0,
          14,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3903612524
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x28x28xf32>, %filter: tensor<240x48x3x3xf32>, %init: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32>\n  return %ret : tensor<128x240x26x26xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x28x28xf32>, %arg1: tensor<240x48x3x3xf32>, %arg2: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x48x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x240x26x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x26x26xf32>\n    memref.copy %2, %alloc : memref<128x240x26x26xf32> to memref<128x240x26x26xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x48x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x26x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x240x26x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x240x26x26xf32>\n    return %3 : tensor<128x240x26x26xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x26x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x28x28xf32>) -> tensor<128x48x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x48x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x48x3x3xf32>) -> tensor<240x48x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x240x26x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x28x28xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<128x240x26x26xf32>) -> tensor<128x240x26x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x26x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x26x26xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          26,
          1
        ],
        [
          "%arg6",
          0,
          26,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 33615844386
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x3x3xf32>) outs (%init: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x3x3xf32>) outs (%init: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x64x7x7xf32>, %filter: tensor<128x64x3x3xf32>, %init: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x3x3xf32>) outs (%init: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32>\n  return %ret : tensor<128x128x3x3xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x7x7xf32>, %arg1: tensor<128x64x3x3xf32>, %arg2: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x64x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x3x3xf32>\n    memref.copy %2, %alloc : memref<128x128x3x3xf32> to memref<128x128x3x3xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 64 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x64x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x64x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x3x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x3x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x3x3xf32>\n    return %3 : tensor<128x128x3x3xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x64x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x64x3x3xf32>) -> tensor<128x64x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x3x3xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x64x7x7xf32>, tensor<128x64x3x3xf32>) outs (%init: tensor<128x128x3x3xf32>) -> tensor<128x128x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x3x3xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          3,
          1
        ],
        [
          "%arg6",
          0,
          3,
          1
        ],
        [
          "%arg7",
          0,
          64,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 319060812
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<512x48x3x3xf32>) outs (%init: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<512x48x3x3xf32>) outs (%init: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x48x28x28xf32>, %filter: tensor<512x48x3x3xf32>, %init: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<512x48x3x3xf32>) outs (%init: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32>\n  return %ret : tensor<256x512x26x26xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x28x28xf32>, %arg1: tensor<512x48x3x3xf32>, %arg2: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x48x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x26x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x26x26xf32>\n    memref.copy %2, %alloc : memref<256x512x26x26xf32> to memref<256x512x26x26xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x48x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x48x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x26x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x26x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x26x26xf32>\n    return %3 : tensor<256x512x26x26xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x26x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x48x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x48x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x48x3x3xf32>) -> tensor<512x48x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x26x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x28x28xf32>, tensor<512x48x3x3xf32>) outs (%init: tensor<256x512x26x26xf32>) -> tensor<256x512x26x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x26x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x26x26xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          26,
          1
        ],
        [
          "%arg6",
          0,
          26,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 143430540572
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x7x7xf32>, tensor<192x256x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x7x7xf32>, tensor<192x256x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x256x7x7xf32>, %filter: tensor<192x256x1x1xf32>, %init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x7x7xf32>, tensor<192x256x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n  return %ret : tensor<128x192x7x7xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x7x7xf32>, %arg1: tensor<192x256x1x1xf32>, %arg2: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x256x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x192x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x7x7xf32>\n    memref.copy %2, %alloc : memref<128x192x7x7xf32> to memref<128x192x7x7xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 192 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x256x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<192x256x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x7x7xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x192x7x7xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x192x7x7xf32>\n    return %3 : tensor<128x192x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x256x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x256x7x7xf32>) -> tensor<128x256x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<192x256x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<192x256x1x1xf32>) -> tensor<192x256x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x192x7x7xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x256x7x7xf32>, tensor<192x256x1x1xf32>) outs (%init: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          192,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ],
        [
          "%arg6",
          0,
          7,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1094410866
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<48x256x3x3xf32>) outs (%init: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<48x256x3x3xf32>) outs (%init: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x256x7x7xf32>, %filter: tensor<48x256x3x3xf32>, %init: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<48x256x3x3xf32>) outs (%init: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32>\n  return %ret : tensor<256x48x3x3xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x7x7xf32>, %arg1: tensor<48x256x3x3xf32>, %arg2: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x256x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x48x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x48x3x3xf32>\n    memref.copy %2, %alloc : memref<256x48x3x3xf32> to memref<256x48x3x3xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 256 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x256x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x256x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x48x3x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x48x3x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x48x3x3xf32>\n    return %3 : tensor<256x48x3x3xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x48x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x256x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x256x7x7xf32>) -> tensor<256x256x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x256x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x256x3x3xf32>) -> tensor<48x256x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x48x3x3xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x256x7x7xf32>, tensor<48x256x3x3xf32>) outs (%init: tensor<256x48x3x3xf32>) -> tensor<256x48x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x48x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x48x3x3xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          3,
          1
        ],
        [
          "%arg6",
          0,
          3,
          1
        ],
        [
          "%arg7",
          0,
          256,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 962928267
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x56x56xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x56x56xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x48x56x56xf32>, %filter: tensor<240x48x3x3xf32>, %init: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x56x56xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32>\n  return %ret : tensor<256x240x27x27xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x56x56xf32>, %arg1: tensor<240x48x3x3xf32>, %arg2: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x48x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x56x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x240x27x27xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x27x27xf32>\n    memref.copy %2, %alloc : memref<256x240x27x27xf32> to memref<256x240x27x27xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 240 {\n        affine.for %arg5 = 0 to 27 {\n          affine.for %arg6 = 0 to 27 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x48x56x56xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<240x48x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x27x27xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x240x27x27xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x240x27x27xf32>\n    return %3 : tensor<256x240x27x27xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x27x27xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x48x56x56xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x48x56x56xf32>) -> tensor<256x48x56x56xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<240x48x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<240x48x3x3xf32>) -> tensor<240x48x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x240x27x27xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x48x56x56xf32>, tensor<240x48x3x3xf32>) outs (%init: tensor<256x240x27x27xf32>) -> tensor<256x240x27x27xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x27x27xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x27x27xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          27,
          1
        ],
        [
          "%arg6",
          0,
          27,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 72634701408
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x28x28xf32>, tensor<48x240x7x7xf32>) outs (%init: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x28x28xf32>, tensor<48x240x7x7xf32>) outs (%init: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x240x28x28xf32>, %filter: tensor<48x240x7x7xf32>, %init: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x28x28xf32>, tensor<48x240x7x7xf32>) outs (%init: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32>\n  return %ret : tensor<128x48x11x11xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x240x28x28xf32>, %arg1: tensor<48x240x7x7xf32>, %arg2: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x240x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x240x28x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x48x11x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x11x11xf32>\n    memref.copy %2, %alloc : memref<128x48x11x11xf32> to memref<128x48x11x11xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 48 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x240x28x28xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<48x240x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x11x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x48x11x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x48x11x11xf32>\n    return %3 : tensor<128x48x11x11xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x11x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x240x28x28xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x240x28x28xf32>) -> tensor<128x240x28x28xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<48x240x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<48x240x7x7xf32>) -> tensor<48x240x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x48x11x11xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x28x28xf32>, tensor<48x240x7x7xf32>) outs (%init: tensor<128x48x11x11xf32>) -> tensor<128x48x11x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x11x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x11x11xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          48,
          1
        ],
        [
          "%arg5",
          0,
          11,
          1
        ],
        [
          "%arg6",
          0,
          11,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 33096974100
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x112x112xf32>, tensor<64x32x3x3xf32>) outs (%init: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x112x112xf32>, tensor<64x32x3x3xf32>) outs (%init: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x112x112xf32>, %filter: tensor<64x32x3x3xf32>, %init: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x112x112xf32>, tensor<64x32x3x3xf32>) outs (%init: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32>\n  return %ret : tensor<256x64x55x55xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x112x112xf32>, %arg1: tensor<64x32x3x3xf32>, %arg2: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x32x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x112x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x55x55xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x55x55xf32>\n    memref.copy %2, %alloc : memref<256x64x55x55xf32> to memref<256x64x55x55xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 55 {\n          affine.for %arg6 = 0 to 55 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x112x112xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x32x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x55x55xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x55x55xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x55x55xf32>\n    return %3 : tensor<256x64x55x55xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x55x55xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x112x112xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x112x112xf32>) -> tensor<256x32x112x112xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x32x3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x32x3x3xf32>) -> tensor<64x32x3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x55x55xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x112x112xf32>, tensor<64x32x3x3xf32>) outs (%init: tensor<256x64x55x55xf32>) -> tensor<256x64x55x55xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x55x55xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x55x55xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          55,
          1
        ],
        [
          "%arg6",
          0,
          55,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          3,
          1
        ],
        [
          "%arg9",
          0,
          3,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 53101169024
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x240x14x14xf32>, %filter: tensor<128x240x7x7xf32>, %init: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32>\n  return %ret : tensor<128x128x8x8xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x240x14x14xf32>, %arg1: tensor<128x240x7x7xf32>, %arg2: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x240x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x240x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x8x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x8x8xf32>\n    memref.copy %2, %alloc : memref<128x128x8x8xf32> to memref<128x128x8x8xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x240x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x240x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x8x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x8x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x8x8xf32>\n    return %3 : tensor<128x128x8x8xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x8x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x240x14x14xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x240x14x14xf32>) -> tensor<128x240x14x14xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x240x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x240x7x7xf32>) -> tensor<128x240x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x8x8xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x14x14xf32>, tensor<128x240x7x7xf32>) outs (%init: tensor<128x128x8x8xf32>) -> tensor<128x128x8x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x8x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x8x8xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          8,
          1
        ],
        [
          "%arg6",
          0,
          8,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 46645597166
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x120x120xf32>, tensor<32x384x1x1xf32>) outs (%init: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x120x120xf32>, tensor<32x384x1x1xf32>) outs (%init: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x384x120x120xf32>, %filter: tensor<32x384x1x1xf32>, %init: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x120x120xf32>, tensor<32x384x1x1xf32>) outs (%init: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32>\n  return %ret : tensor<128x32x60x60xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x384x120x120xf32>, %arg1: tensor<32x384x1x1xf32>, %arg2: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x384x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x384x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x60x60xf32>\n    memref.copy %2, %alloc : memref<128x32x60x60xf32> to memref<128x32x60x60xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 384 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x384x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x384x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x60x60xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x60x60xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x60x60xf32>\n    return %3 : tensor<128x32x60x60xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x384x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x384x120x120xf32>) -> tensor<128x384x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x384x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x384x1x1xf32>) -> tensor<32x384x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x384x120x120xf32>, tensor<32x384x1x1xf32>) outs (%init: tensor<128x32x60x60xf32>) -> tensor<128x32x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x60x60xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          60,
          1
        ],
        [
          "%arg6",
          0,
          60,
          1
        ],
        [
          "%arg7",
          0,
          384,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 21043162651
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<256x32x1x1xf32>) outs (%init: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<256x32x1x1xf32>) outs (%init: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x224x224xf32>, %filter: tensor<256x32x1x1xf32>, %init: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<256x32x1x1xf32>) outs (%init: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32>\n  return %ret : tensor<256x256x112x112xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x224x224xf32>, %arg1: tensor<256x32x1x1xf32>, %arg2: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x224x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x112x112xf32>\n    memref.copy %2, %alloc : memref<256x256x112x112xf32> to memref<256x256x112x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x224x224xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<256x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x256x112x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x256x112x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x112x112xf32>\n    return %3 : tensor<256x256x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x224x224xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<256x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<256x32x1x1xf32>) -> tensor<256x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x256x112x112xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x224x224xf32>, tensor<256x32x1x1xf32>) outs (%init: tensor<256x256x112x112xf32>) -> tensor<256x256x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          256,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ],
        [
          "%arg6",
          0,
          112,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 62375382216
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x120x120xf32>, tensor<96x96x1x1xf32>) outs (%init: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x120x120xf32>, tensor<96x96x1x1xf32>) outs (%init: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x96x120x120xf32>, %filter: tensor<96x96x1x1xf32>, %init: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x120x120xf32>, tensor<96x96x1x1xf32>) outs (%init: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>\n  return %ret : tensor<256x96x120x120xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x96x120x120xf32>, %arg1: tensor<96x96x1x1xf32>, %arg2: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x96x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x96x120x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x96x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x120x120xf32>\n    memref.copy %2, %alloc : memref<256x96x120x120xf32> to memref<256x96x120x120xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 96 {\n        affine.for %arg5 = 0 to 120 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 96 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x96x120x120xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<96x96x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x120x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x96x120x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x96x120x120xf32>\n    return %3 : tensor<256x96x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x96x120x120xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<96x96x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<96x96x1x1xf32>) -> tensor<96x96x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x96x120x120xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x96x120x120xf32>, tensor<96x96x1x1xf32>) outs (%init: tensor<256x96x120x120xf32>) -> tensor<256x96x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          96,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ],
        [
          "%arg6",
          0,
          120,
          1
        ],
        [
          "%arg7",
          0,
          96,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 + %arg8",
          "%arg6 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 114159829093
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x7x7xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x7x7xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x48x7x7xf32>, %filter: tensor<128x48x1x1xf32>, %init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x7x7xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n  return %ret : tensor<128x128x4x4xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x48x7x7xf32>, %arg1: tensor<128x48x1x1xf32>, %arg2: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x48x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x48x7x7xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x4x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x4x4xf32>\n    memref.copy %2, %alloc : memref<128x128x4x4xf32> to memref<128x128x4x4xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 4 {\n            affine.for %arg7 = 0 to 48 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x48x7x7xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<128x48x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x4x4xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x128x4x4xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x4x4xf32>\n    return %3 : tensor<128x128x4x4xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x4x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x48x7x7xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x48x7x7xf32>) -> tensor<128x48x7x7xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<128x48x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<128x48x1x1xf32>) -> tensor<128x48x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x128x4x4xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x48x7x7xf32>, tensor<128x48x1x1xf32>) outs (%init: tensor<128x128x4x4xf32>) -> tensor<128x128x4x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x4x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x4x4xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          128,
          1
        ],
        [
          "%arg5",
          0,
          4,
          1
        ],
        [
          "%arg6",
          0,
          4,
          1
        ],
        [
          "%arg7",
          0,
          48,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 32005244
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x150x150xf32>, tensor<32x32x7x7xf32>) outs (%init: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x150x150xf32>, tensor<32x32x7x7xf32>) outs (%init: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x32x150x150xf32>, %filter: tensor<32x32x7x7xf32>, %init: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x150x150xf32>, tensor<32x32x7x7xf32>) outs (%init: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32>\n  return %ret : tensor<128x32x72x72xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x150x150xf32>, %arg1: tensor<32x32x7x7xf32>, %arg2: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x32x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x150x150xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x72x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x72x72xf32>\n    memref.copy %2, %alloc : memref<128x32x72x72xf32> to memref<128x32x72x72xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 72 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x32x150x150xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<32x32x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x72x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x32x72x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x72x72xf32>\n    return %3 : tensor<128x32x72x72xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x72x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x32x150x150xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x32x150x150xf32>) -> tensor<128x32x150x150xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<32x32x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<32x32x7x7xf32>) -> tensor<32x32x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x32x72x72xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x32x150x150xf32>, tensor<32x32x7x7xf32>) outs (%init: tensor<128x32x72x72xf32>) -> tensor<128x32x72x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x72x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x72x72xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          32,
          1
        ],
        [
          "%arg5",
          0,
          72,
          1
        ],
        [
          "%arg6",
          0,
          72,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 125521470033
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x15x15xf32>, tensor<64x240x7x7xf32>) outs (%init: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x15x15xf32>, tensor<64x240x7x7xf32>) outs (%init: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<128x240x15x15xf32>, %filter: tensor<64x240x7x7xf32>, %init: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x15x15xf32>, tensor<64x240x7x7xf32>) outs (%init: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32>\n  return %ret : tensor<128x64x5x5xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x240x15x15xf32>, %arg1: tensor<64x240x7x7xf32>, %arg2: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x240x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x240x15x15xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x5x5xf32>\n    memref.copy %2, %alloc : memref<128x64x5x5xf32> to memref<128x64x5x5xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 240 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<128x240x15x15xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<64x240x7x7xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x5x5xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x64x5x5xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x5x5xf32>\n    return %3 : tensor<128x64x5x5xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<128x240x15x15xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<128x240x15x15xf32>) -> tensor<128x240x15x15xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<64x240x7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<64x240x7x7xf32>) -> tensor<64x240x7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<128x64x5x5xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<128x240x15x15xf32>, tensor<64x240x7x7xf32>) outs (%init: tensor<128x64x5x5xf32>) -> tensor<128x64x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x5x5xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          64,
          1
        ],
        [
          "%arg5",
          0,
          5,
          1
        ],
        [
          "%arg6",
          0,
          5,
          1
        ],
        [
          "%arg7",
          0,
          240,
          1
        ],
        [
          "%arg8",
          0,
          7,
          1
        ],
        [
          "%arg9",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 9104511295
  },
  "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x228x228xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32>": {
    "operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x228x228xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32>",
    "wrapped_operation": "func.func @func_call(%input: tensor<256x32x228x228xf32>, %filter: tensor<512x32x1x1xf32>, %init: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x228x228xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32>\n  return %ret : tensor<256x512x114x114xf32>\n}",
    "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x228x228xf32>, %arg1: tensor<512x32x1x1xf32>, %arg2: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x32x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x228x228xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x114x114xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x114x114xf32>\n    memref.copy %2, %alloc : memref<256x512x114x114xf32> to memref<256x512x114x114xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 114 {\n          affine.for %arg6 = 0 to 114 {\n            affine.for %arg7 = 0 to 32 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<256x32x228x228xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<512x32x1x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x114x114xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x114x114xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x114x114xf32>\n    return %3 : tensor<256x512x114x114xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x114x114xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x228x228xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x228x228xf32>) -> tensor<256x32x228x228xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<512x32x1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<512x32x1x1xf32>) -> tensor<512x32x1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x114x114xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x228x228xf32>, tensor<512x32x1x1xf32>) outs (%init: tensor<256x512x114x114xf32>) -> tensor<256x512x114x114xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x114x114xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x114x114xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          512,
          1
        ],
        [
          "%arg5",
          0,
          114,
          1
        ],
        [
          "%arg6",
          0,
          114,
          1
        ],
        [
          "%arg7",
          0,
          32,
          1
        ],
        [
          "%arg8",
          0,
          1,
          1
        ],
        [
          "%arg9",
          0,
          1,
          1
        ]
      ],
      "op_count": {
        "+": 1,
        "-": 0,
        "*": 1,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg3",
          "%arg7",
          "%arg5 * 2 + %arg8",
          "%arg6 * 2 + %arg9"
        ],
        [
          "%arg4",
          "%arg7",
          "%arg8",
          "%arg9"
        ],
        [
          "%arg3",
          "%arg4",
          "%arg5",
          "%arg6"
        ]
      ],
      "store_data": []
    },
    "execution_time": 131003787274
  }
}