[["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%3 : tensor<3x3x3x2xf32>) outs(%8 : tensor<2x3x3x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<2x3x3x3xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%3 : tensor<3x3x3x2xf32>) outs(%8 : tensor<2x3x3x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<2x3x3x3xf32>", "wrapped_operation": "func.func @func_call(%3: tensor<3x3x3x2xf32>, %8: tensor<2x3x3x3xf32>) -> tensor<2x3x3x3xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%3 : tensor<3x3x3x2xf32>) outs(%8 : tensor<2x3x3x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<2x3x3x3xf32>\n  return %ret : tensor<2x3x3x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<3x3x3x2xf32>, %arg1: tensor<2x3x3x3xf32>) -> tensor<2x3x3x3xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<3x3x3x2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x3x3x3xf32>\n    affine.for %arg2 = 0 to 2 {\n      affine.for %arg3 = 0 to 3 {\n        affine.for %arg4 = 0 to 3 {\n          affine.for %arg5 = 0 to 3 {\n            %2 = affine.load %0[%arg3, %arg4, %arg5, %arg2] : memref<3x3x3x2xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<2x3x3x3xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<2x3x3x3xf32>\n    return %1 : tensor<2x3x3x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<2x3x3x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x3x2xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<2x3x3x3xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<2x3x3x3xf32>) -> tensor<2x3x3x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%3 : tensor<3x3x3x2xf32>) outs(%8 : tensor<2x3x3x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<2x3x3x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<2x3x3x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<2x3x3x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 2, 1], ["%arg3", 0, 3, 1], ["%arg4", 0, 3, 1], ["%arg5", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg2"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 157.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%9 : tensor<2x3x3x3xf32>) outs(%10 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%9 : tensor<2x3x3x3xf32>) outs(%10 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>", "wrapped_operation": "func.func @func_call(%9: tensor<2x3x3x3xf32>, %10: tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%9 : tensor<2x3x3x3xf32>) outs(%10 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>\n  return %ret : tensor<3x3x3x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<2x3x3x3xf32>, %arg1: tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<2x3x3x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<3x3x3x2xf32>\n    affine.for %arg2 = 0 to 3 {\n      affine.for %arg3 = 0 to 3 {\n        affine.for %arg4 = 0 to 3 {\n          affine.for %arg5 = 0 to 2 {\n            %2 = affine.load %0[%arg5, %arg2, %arg3, %arg4] : memref<2x3x3x3xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<3x3x3x2xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<3x3x3x2xf32>\n    return %1 : tensor<3x3x3x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<3x3x3x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_9 = bufferization.alloc_tensor() : tensor<2x3x3x3xf32>\n%9 = linalg.fill ins(%val : f32) outs(%tmp_9 : tensor<2x3x3x3xf32>) -> tensor<2x3x3x3xf32>\n%tmp_10 = bufferization.alloc_tensor() : tensor<3x3x3x2xf32>\n%10 = linalg.fill ins(%val : f32) outs(%tmp_10 : tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%9 : tensor<2x3x3x3xf32>) outs(%10 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<3x3x3x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<3x3x3x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 3, 1], ["%arg3", 0, 3, 1], ["%arg4", 0, 3, 1], ["%arg5", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5", "%arg2", "%arg3", "%arg4"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 212.0}], ["linalg.fill ins(%cst_1 : f32) outs(%12 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>", {"operation": "linalg.fill ins(%cst_1 : f32) outs(%12 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>", "wrapped_operation": "func.func @func_call(%cst_1: f32, %12: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n  %ret = linalg.fill ins(%cst_1 : f32) outs(%12 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n  return %ret : tensor<16x26x26x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x26x26x2xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 26 {\n        affine.for %arg4 = 0 to 26 {\n          affine.for %arg5 = 0 to 2 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x26x26x2xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<16x26x26x2xf32>\n    return %0 : tensor<16x26x26x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x26x26x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_1 = arith.constant 2.00000e+00 : f32\n%tmp_12 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%12 = linalg.fill ins(%val : f32) outs(%tmp_12 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_1 : f32) outs(%12 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x26x26x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x26x26x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 26, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 20907.0}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %11 : tensor<16x28x28x3xf32>, tensor<3x3x3x2xf32>) outs(%13 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %11 : tensor<16x28x28x3xf32>, tensor<3x3x3x2xf32>) outs(%13 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<16x28x28x3xf32>, %11: tensor<3x3x3x2xf32>, %13: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %11 : tensor<16x28x28x3xf32>, tensor<3x3x3x2xf32>) outs(%13 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n  return %ret : tensor<16x26x26x2xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x28x28x3xf32>, %arg1: tensor<3x3x3x2xf32>, %arg2: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3x2xf32>\n    %1 = bufferization.to_memref %arg0 : memref<16x28x28x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<16x26x26x2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x26x26x2xf32>\n    memref.copy %2, %alloc : memref<16x26x26x2xf32> to memref<16x26x26x2xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 2 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<16x28x28x3xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<3x3x3x2xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x26x26x2xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x26x26x2xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<16x26x26x2xf32>\n    return %3 : tensor<16x26x26x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x26x26x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_arg0 = bufferization.alloc_tensor() : tensor<16x28x28x3xf32>\n%arg0 = linalg.fill ins(%val : f32) outs(%tmp_arg0 : tensor<16x28x28x3xf32>) -> tensor<16x28x28x3xf32>\n%tmp_11 = bufferization.alloc_tensor() : tensor<3x3x3x2xf32>\n%11 = linalg.fill ins(%val : f32) outs(%tmp_11 : tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32>\n%tmp_13 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%13 = linalg.fill ins(%val : f32) outs(%tmp_13 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %11 : tensor<16x28x28x3xf32>, tensor<3x3x3x2xf32>) outs(%13 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x26x26x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x26x26x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 2, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1659699.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2, %15 : tensor<2xf32>, tensor<16x26x26x2xf32>) outs(%14 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x26x26x2xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2, %15 : tensor<2xf32>, tensor<16x26x26x2xf32>) outs(%14 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x26x26x2xf32>", "wrapped_operation": "func.func @func_call(%2: tensor<2xf32>, %15: tensor<16x26x26x2xf32>, %14: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2, %15 : tensor<2xf32>, tensor<16x26x26x2xf32>) outs(%14 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x26x26x2xf32>\n  return %ret : tensor<16x26x26x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<2xf32>, %arg1: tensor<16x26x26x2xf32>, %arg2: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x26x26x2xf32>\n    %1 = bufferization.to_memref %arg0 : memref<2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x26x26x2xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 2 {\n            %3 = affine.load %1[%arg6] : memref<2xf32>\n            %4 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<16x26x26x2xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x26x26x2xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x26x26x2xf32>\n    return %2 : tensor<16x26x26x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x26x26x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_2 = bufferization.alloc_tensor() : tensor<2xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<2xf32>) -> tensor<2xf32>\n%tmp_15 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%15 = linalg.fill ins(%val : f32) outs(%tmp_15 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n%tmp_14 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%14 = linalg.fill ins(%val : f32) outs(%tmp_14 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2, %15 : tensor<2xf32>, tensor<16x26x26x2xf32>) outs(%14 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x26x26x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x26x26x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x26x26x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 2, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27615.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%16 : tensor<16x26x26x2xf32>) outs(%17 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x26x26x2xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%16 : tensor<16x26x26x2xf32>) outs(%17 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x26x26x2xf32>", "wrapped_operation": "func.func @func_call(%16: tensor<16x26x26x2xf32>, %17: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%16 : tensor<16x26x26x2xf32>) outs(%17 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x26x26x2xf32>\n  return %ret : tensor<16x26x26x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x26x26x2xf32>, %arg1: tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %cst_0 = arith.constant 3.40282347E+38 : f32\n    %0 = bufferization.to_memref %arg0 : memref<16x26x26x2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x26x26x2xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 26 {\n        affine.for %arg4 = 0 to 26 {\n          affine.for %arg5 = 0 to 2 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<16x26x26x2xf32>\n            %3 = arith.minf %2, %cst_0 : f32\n            %4 = arith.maxf %3, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x26x26x2xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16x26x26x2xf32>\n    return %1 : tensor<16x26x26x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x26x26x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_16 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%16 = linalg.fill ins(%val : f32) outs(%tmp_16 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n%tmp_17 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%16 : tensor<16x26x26x2xf32>) outs(%17 : tensor<16x26x26x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x26x26x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x26x26x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x26x26x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 26, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 27374.0}], ["linalg.fill ins(%cst_2 : f32) outs(%19 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>", {"operation": "linalg.fill ins(%cst_2 : f32) outs(%19 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>", "wrapped_operation": "func.func @func_call(%cst_2: f32, %19: tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32> {\n  %ret = linalg.fill ins(%cst_2 : f32) outs(%19 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n  return %ret : tensor<16x13x13x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x13x13x2xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 13 {\n        affine.for %arg4 = 0 to 13 {\n          affine.for %arg5 = 0 to 2 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x13x13x2xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<16x13x13x2xf32>\n    return %0 : tensor<16x13x13x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x13x13x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_2 = arith.constant 2.00000e+00 : f32\n%tmp_19 = bufferization.alloc_tensor() : tensor<16x13x13x2xf32>\n%19 = linalg.fill ins(%val : f32) outs(%tmp_19 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_2 : f32) outs(%19 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x13x13x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x13x13x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 13, 1], ["%arg4", 0, 13, 1], ["%arg5", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 5221.0}], ["linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%18, %21 : tensor<16x26x26x2xf32>, tensor<2x2xf32>) outs(%20 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>", {"operation": "linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%18, %21 : tensor<16x26x26x2xf32>, tensor<2x2xf32>) outs(%20 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>", "wrapped_operation": "func.func @func_call(%18: tensor<16x26x26x2xf32>, %21: tensor<2x2xf32>, %20: tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32> {\n  %ret = linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%18, %21 : tensor<16x26x26x2xf32>, tensor<2x2xf32>) outs(%20 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n  return %ret : tensor<16x13x13x2xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x26x26x2xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x26x26x2xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x13x13x2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x13x13x2xf32>\n    memref.copy %1, %alloc : memref<16x13x13x2xf32> to memref<16x13x13x2xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 13 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 2 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg4, %arg7)\n                %4 = affine.apply #map(%arg5, %arg8)\n                %5 = affine.load %0[%arg3, %3, %4, %arg6] : memref<16x26x26x2xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x13x13x2xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x13x13x2xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x13x13x2xf32>\n    return %2 : tensor<16x13x13x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x13x13x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_18 = bufferization.alloc_tensor() : tensor<16x26x26x2xf32>\n%18 = linalg.fill ins(%val : f32) outs(%tmp_18 : tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>\n%tmp_21 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%21 = linalg.fill ins(%val : f32) outs(%tmp_21 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_20 = bufferization.alloc_tensor() : tensor<16x13x13x2xf32>\n%20 = linalg.fill ins(%val : f32) outs(%tmp_20 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%18, %21 : tensor<16x26x26x2xf32>, tensor<2x2xf32>) outs(%20 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x13x13x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x13x13x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 13, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 2, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 68522.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<3x3x2x3xf32>) outs(%23 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<3x3x2x3xf32>) outs(%23 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<3x3x2x3xf32>, %23: tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<3x3x2x3xf32>) outs(%23 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>\n  return %ret : tensor<3x3x3x2xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<3x3x2x3xf32>, %arg1: tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<3x3x2x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<3x3x3x2xf32>\n    affine.for %arg2 = 0 to 3 {\n      affine.for %arg3 = 0 to 3 {\n        affine.for %arg4 = 0 to 3 {\n          affine.for %arg5 = 0 to 2 {\n            %2 = affine.load %0[%arg3, %arg4, %arg5, %arg2] : memref<3x3x2x3xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<3x3x3x2xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<3x3x3x2xf32>\n    return %1 : tensor<3x3x3x2xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<3x3x3x2xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<3x3x2x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<3x3x2x3xf32>) -> tensor<3x3x2x3xf32>\n%tmp_23 = bufferization.alloc_tensor() : tensor<3x3x3x2xf32>\n%23 = linalg.fill ins(%val : f32) outs(%tmp_23 : tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1, d2, d3, d0)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%1 : tensor<3x3x2x3xf32>) outs(%23 : tensor<3x3x3x2xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x3x2xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<3x3x3x2xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<3x3x3x2xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 3, 1], ["%arg3", 0, 3, 1], ["%arg4", 0, 3, 1], ["%arg5", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5", "%arg2"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 216.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%24 : tensor<3x3x3x2xf32>) outs(%25 : tensor<3x3x2x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x2x3xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%24 : tensor<3x3x3x2xf32>) outs(%25 : tensor<3x3x2x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x2x3xf32>", "wrapped_operation": "func.func @func_call(%24: tensor<3x3x3x2xf32>, %25: tensor<3x3x2x3xf32>) -> tensor<3x3x2x3xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%24 : tensor<3x3x3x2xf32>) outs(%25 : tensor<3x3x2x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x2x3xf32>\n  return %ret : tensor<3x3x2x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<3x3x3x2xf32>, %arg1: tensor<3x3x2x3xf32>) -> tensor<3x3x2x3xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<3x3x3x2xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<3x3x2x3xf32>\n    affine.for %arg2 = 0 to 3 {\n      affine.for %arg3 = 0 to 3 {\n        affine.for %arg4 = 0 to 2 {\n          affine.for %arg5 = 0 to 3 {\n            %2 = affine.load %0[%arg5, %arg2, %arg3, %arg4] : memref<3x3x3x2xf32>\n            affine.store %2, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<3x3x2x3xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<3x3x2x3xf32>\n    return %1 : tensor<3x3x2x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<3x3x2x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_24 = bufferization.alloc_tensor() : tensor<3x3x3x2xf32>\n%24 = linalg.fill ins(%val : f32) outs(%tmp_24 : tensor<3x3x3x2xf32>) -> tensor<3x3x3x2xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<3x3x2x3xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<3x3x2x3xf32>) -> tensor<3x3x2x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3, d0, d1, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%24 : tensor<3x3x3x2xf32>) outs(%25 : tensor<3x3x2x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<3x3x2x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<3x3x2x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<3x3x2x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 3, 1], ["%arg3", 0, 3, 1], ["%arg4", 0, 2, 1], ["%arg5", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg5", "%arg2", "%arg3", "%arg4"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 186.0}], ["linalg.fill ins(%cst_4 : f32) outs(%27 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>", {"operation": "linalg.fill ins(%cst_4 : f32) outs(%27 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>", "wrapped_operation": "func.func @func_call(%cst_4: f32, %27: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n  %ret = linalg.fill ins(%cst_4 : f32) outs(%27 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n  return %ret : tensor<16x11x11x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x11x11x3xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 11 {\n        affine.for %arg4 = 0 to 11 {\n          affine.for %arg5 = 0 to 3 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x11x11x3xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<16x11x11x3xf32>\n    return %0 : tensor<16x11x11x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x11x11x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_4 = arith.constant 2.00000e+00 : f32\n%tmp_27 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%27 = linalg.fill ins(%val : f32) outs(%tmp_27 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_4 : f32) outs(%27 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x11x11x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x11x11x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 11, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 6875.0}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%22, %26 : tensor<16x13x13x2xf32>, tensor<3x3x2x3xf32>) outs(%28 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%22, %26 : tensor<16x13x13x2xf32>, tensor<3x3x2x3xf32>) outs(%28 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>", "wrapped_operation": "func.func @func_call(%22: tensor<16x13x13x2xf32>, %26: tensor<3x3x2x3xf32>, %28: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%22, %26 : tensor<16x13x13x2xf32>, tensor<3x3x2x3xf32>) outs(%28 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n  return %ret : tensor<16x11x11x3xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x13x13x2xf32>, %arg1: tensor<3x3x2x3xf32>, %arg2: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x2x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<16x13x13x2xf32>\n    %2 = bufferization.to_memref %arg2 : memref<16x11x11x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x11x11x3xf32>\n    memref.copy %2, %alloc : memref<16x11x11x3xf32> to memref<16x11x11x3xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 2 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<16x13x13x2xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<3x3x2x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x11x11x3xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x11x11x3xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<16x11x11x3xf32>\n    return %3 : tensor<16x11x11x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x11x11x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_22 = bufferization.alloc_tensor() : tensor<16x13x13x2xf32>\n%22 = linalg.fill ins(%val : f32) outs(%tmp_22 : tensor<16x13x13x2xf32>) -> tensor<16x13x13x2xf32>\n%tmp_26 = bufferization.alloc_tensor() : tensor<3x3x2x3xf32>\n%26 = linalg.fill ins(%val : f32) outs(%tmp_26 : tensor<3x3x2x3xf32>) -> tensor<3x3x2x3xf32>\n%tmp_28 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%22, %26 : tensor<16x13x13x2xf32>, tensor<3x3x2x3xf32>) outs(%28 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x11x11x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x11x11x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 2, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 304376.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%0, %30 : tensor<3xf32>, tensor<16x11x11x3xf32>) outs(%29 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x11x11x3xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%0, %30 : tensor<3xf32>, tensor<16x11x11x3xf32>) outs(%29 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x11x11x3xf32>", "wrapped_operation": "func.func @func_call(%0: tensor<3xf32>, %30: tensor<16x11x11x3xf32>, %29: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%0, %30 : tensor<3xf32>, tensor<16x11x11x3xf32>) outs(%29 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x11x11x3xf32>\n  return %ret : tensor<16x11x11x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<3xf32>, %arg1: tensor<16x11x11x3xf32>, %arg2: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x11x11x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x11x11x3xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 3 {\n            %3 = affine.load %1[%arg6] : memref<3xf32>\n            %4 = affine.load %0[%arg3, %arg4, %arg5, %arg6] : memref<16x11x11x3xf32>\n            %5 = arith.addf %3, %4 : f32\n            affine.store %5, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x11x11x3xf32>\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x11x11x3xf32>\n    return %2 : tensor<16x11x11x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x11x11x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_0 = bufferization.alloc_tensor() : tensor<3xf32>\n%0 = linalg.fill ins(%val : f32) outs(%tmp_0 : tensor<3xf32>) -> tensor<3xf32>\n%tmp_30 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%30 = linalg.fill ins(%val : f32) outs(%tmp_30 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n%tmp_29 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%29 = linalg.fill ins(%val : f32) outs(%tmp_29 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%0, %30 : tensor<3xf32>, tensor<16x11x11x3xf32>) outs(%29 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x11x11x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x11x11x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x11x11x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7114.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%31 : tensor<16x11x11x3xf32>) outs(%32 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x11x11x3xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%31 : tensor<16x11x11x3xf32>) outs(%32 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x11x11x3xf32>", "wrapped_operation": "func.func @func_call(%31: tensor<16x11x11x3xf32>, %32: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%31 : tensor<16x11x11x3xf32>) outs(%32 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x11x11x3xf32>\n  return %ret : tensor<16x11x11x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x11x11x3xf32>, %arg1: tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %cst_0 = arith.constant 3.40282347E+38 : f32\n    %0 = bufferization.to_memref %arg0 : memref<16x11x11x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x11x11x3xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 11 {\n        affine.for %arg4 = 0 to 11 {\n          affine.for %arg5 = 0 to 3 {\n            %2 = affine.load %0[%arg2, %arg3, %arg4, %arg5] : memref<16x11x11x3xf32>\n            %3 = arith.minf %2, %cst_0 : f32\n            %4 = arith.maxf %3, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x11x11x3xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16x11x11x3xf32>\n    return %1 : tensor<16x11x11x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x11x11x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_31 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%31 = linalg.fill ins(%val : f32) outs(%tmp_31 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n%tmp_32 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%32 = linalg.fill ins(%val : f32) outs(%tmp_32 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%31 : tensor<16x11x11x3xf32>) outs(%32 : tensor<16x11x11x3xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x11x11x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x11x11x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x11x11x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 11, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4", "%arg5"]], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 7165.0}], ["linalg.fill ins(%cst_5 : f32) outs(%34 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>", {"operation": "linalg.fill ins(%cst_5 : f32) outs(%34 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>", "wrapped_operation": "func.func @func_call(%cst_5: f32, %34: tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32> {\n  %ret = linalg.fill ins(%cst_5 : f32) outs(%34 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n  return %ret : tensor<16x5x5x3xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x5x5x3xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 5 {\n        affine.for %arg4 = 0 to 5 {\n          affine.for %arg5 = 0 to 3 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<16x5x5x3xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<16x5x5x3xf32>\n    return %0 : tensor<16x5x5x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x5x5x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_5 = arith.constant 2.00000e+00 : f32\n%tmp_34 = bufferization.alloc_tensor() : tensor<16x5x5x3xf32>\n%34 = linalg.fill ins(%val : f32) outs(%tmp_34 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_5 : f32) outs(%34 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x5x5x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x5x5x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 5, 1], ["%arg4", 0, 5, 1], ["%arg5", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": 1495.0}], ["linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%33, %36 : tensor<16x11x11x3xf32>, tensor<2x2xf32>) outs(%35 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>", {"operation": "linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%33, %36 : tensor<16x11x11x3xf32>, tensor<2x2xf32>) outs(%35 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>", "wrapped_operation": "func.func @func_call(%33: tensor<16x11x11x3xf32>, %36: tensor<2x2xf32>, %35: tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32> {\n  %ret = linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%33, %36 : tensor<16x11x11x3xf32>, tensor<2x2xf32>) outs(%35 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n  return %ret : tensor<16x5x5x3xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x11x11x3xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x11x11x3xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x5x5x3xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x5x5x3xf32>\n    memref.copy %1, %alloc : memref<16x5x5x3xf32> to memref<16x5x5x3xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 5 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg4, %arg7)\n                %4 = affine.apply #map(%arg5, %arg8)\n                %5 = affine.load %0[%arg3, %3, %4, %arg6] : memref<16x11x11x3xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x5x5x3xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x5x5x3xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x5x5x3xf32>\n    return %2 : tensor<16x5x5x3xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x5x5x3xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_33 = bufferization.alloc_tensor() : tensor<16x11x11x3xf32>\n%33 = linalg.fill ins(%val : f32) outs(%tmp_33 : tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>\n%tmp_36 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%36 = linalg.fill ins(%val : f32) outs(%tmp_36 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<16x5x5x3xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nhwc_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%33, %36 : tensor<16x11x11x3xf32>, tensor<2x2xf32>) outs(%35 : tensor<16x5x5x3xf32>) -> tensor<16x5x5x3xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x5x5x3xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x5x5x3xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 5, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14127.0}], ["linalg.fill ins(%cst_7 : f32) outs(%38 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>", {"operation": "linalg.fill ins(%cst_7 : f32) outs(%38 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>", "wrapped_operation": "func.func @func_call(%cst_7: f32, %38: tensor<1x16x4xf32>) -> tensor<1x16x4xf32> {\n  %ret = linalg.fill ins(%cst_7 : f32) outs(%38 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n  return %ret : tensor<1x16x4xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x16x4xf32>) -> tensor<1x16x4xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x4xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 4 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x16x4xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x16x4xf32>\n    return %0 : tensor<1x16x4xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_7 = arith.constant 2.00000e+00 : f32\n%tmp_38 = bufferization.alloc_tensor() : tensor<1x16x4xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_7 : f32) outs(%38 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x4xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 4, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 86.0}], ["linalg.batch_matmul ins(%expanded, %expanded_6 : tensor<1x16x75xf32>, tensor<1x75x4xf32>) outs(%39 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>", {"operation": "linalg.batch_matmul ins(%expanded, %expanded_6 : tensor<1x16x75xf32>, tensor<1x75x4xf32>) outs(%39 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>", "wrapped_operation": "func.func @func_call(%expanded: tensor<1x16x75xf32>, %expanded_6: tensor<1x75x4xf32>, %39: tensor<1x16x4xf32>) -> tensor<1x16x4xf32> {\n  %ret = linalg.batch_matmul ins(%expanded, %expanded_6 : tensor<1x16x75xf32>, tensor<1x75x4xf32>) outs(%39 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n  return %ret : tensor<1x16x4xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x16x75xf32>, %arg1: tensor<1x75x4xf32>, %arg2: tensor<1x16x4xf32>) -> tensor<1x16x4xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x75x4xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x16x75xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x16x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x4xf32>\n    memref.copy %2, %alloc : memref<1x16x4xf32> to memref<1x16x4xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 4 {\n          affine.for %arg6 = 0 to 75 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x16x75xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x75x4xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x16x4xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x16x4xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x16x4xf32>\n    return %3 : tensor<1x16x4xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_expanded = bufferization.alloc_tensor() : tensor<1x16x75xf32>\n%expanded = linalg.fill ins(%val : f32) outs(%tmp_expanded : tensor<1x16x75xf32>) -> tensor<1x16x75xf32>\n%tmp_expanded_6 = bufferization.alloc_tensor() : tensor<1x75x4xf32>\n%expanded_6 = linalg.fill ins(%val : f32) outs(%tmp_expanded_6 : tensor<1x75x4xf32>) -> tensor<1x75x4xf32>\n%tmp_39 = bufferization.alloc_tensor() : tensor<1x16x4xf32>\n%39 = linalg.fill ins(%val : f32) outs(%tmp_39 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%expanded, %expanded_6 : tensor<1x16x75xf32>, tensor<1x75x4xf32>) outs(%39 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x4xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 4, 1], ["%arg6", 0, 75, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 14459.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_8, %expanded_9 : tensor<16x4xf32>, tensor<1x4xf32>) outs(%41 : tensor<16x4xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x4xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_8, %expanded_9 : tensor<16x4xf32>, tensor<1x4xf32>) outs(%41 : tensor<16x4xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x4xf32>", "wrapped_operation": "func.func @func_call(%collapsed_8: tensor<16x4xf32>, %expanded_9: tensor<1x4xf32>, %41: tensor<16x4xf32>) -> tensor<16x4xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_8, %expanded_9 : tensor<16x4xf32>, tensor<1x4xf32>) outs(%41 : tensor<16x4xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x4xf32>\n  return %ret : tensor<16x4xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x4xf32>, %arg1: tensor<1x4xf32>, %arg2: tensor<16x4xf32>) -> tensor<16x4xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x4xf32>\n    %collapsed = tensor.collapse_shape %arg1 [[0, 1]] : tensor<1x4xf32> into tensor<4xf32>\n    %1 = bufferization.to_memref %collapsed : memref<4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x4xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 4 {\n        %3 = affine.load %0[%arg3, %arg4] : memref<16x4xf32>\n        %4 = affine.load %1[%arg4] : memref<4xf32>\n        %5 = arith.addf %3, %4 : f32\n        affine.store %5, %alloc[%arg3, %arg4] : memref<16x4xf32>\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x4xf32>\n    return %2 : tensor<16x4xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_collapsed_8 = bufferization.alloc_tensor() : tensor<16x4xf32>\n%collapsed_8 = linalg.fill ins(%val : f32) outs(%tmp_collapsed_8 : tensor<16x4xf32>) -> tensor<16x4xf32>\n%tmp_expanded_9 = bufferization.alloc_tensor() : tensor<1x4xf32>\n%expanded_9 = linalg.fill ins(%val : f32) outs(%tmp_expanded_9 : tensor<1x4xf32>) -> tensor<1x4xf32>\n%tmp_41 = bufferization.alloc_tensor() : tensor<16x4xf32>\n%41 = linalg.fill ins(%val : f32) outs(%tmp_41 : tensor<16x4xf32>) -> tensor<16x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_8, %expanded_9 : tensor<16x4xf32>, tensor<1x4xf32>) outs(%41 : tensor<16x4xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x4xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 4, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4"], ["%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 127.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%42 : tensor<16x4xf32>) outs(%43 : tensor<16x4xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x4xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%42 : tensor<16x4xf32>) outs(%43 : tensor<16x4xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x4xf32>", "wrapped_operation": "func.func @func_call(%42: tensor<16x4xf32>, %43: tensor<16x4xf32>) -> tensor<16x4xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%42 : tensor<16x4xf32>) outs(%43 : tensor<16x4xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x4xf32>\n  return %ret : tensor<16x4xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x4xf32>, %arg1: tensor<16x4xf32>) -> tensor<16x4xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %cst_0 = arith.constant 3.40282347E+38 : f32\n    %0 = bufferization.to_memref %arg0 : memref<16x4xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x4xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 4 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<16x4xf32>\n        %3 = arith.minf %2, %cst_0 : f32\n        %4 = arith.maxf %3, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<16x4xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16x4xf32>\n    return %1 : tensor<16x4xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x4xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_42 = bufferization.alloc_tensor() : tensor<16x4xf32>\n%42 = linalg.fill ins(%val : f32) outs(%tmp_42 : tensor<16x4xf32>) -> tensor<16x4xf32>\n%tmp_43 = bufferization.alloc_tensor() : tensor<16x4xf32>\n%43 = linalg.fill ins(%val : f32) outs(%tmp_43 : tensor<16x4xf32>) -> tensor<16x4xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%42 : tensor<16x4xf32>) outs(%43 : tensor<16x4xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 0.000000e+00 : f32\n  %cst_16 = arith.constant 3.40282347E+38 : f32\n  %58 = arith.minf %in, %cst_16 : f32\n  %59 = arith.maxf %58, %cst_15 : f32\n  linalg.yield %59 : f32\n} -> tensor<16x4xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x4xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x4xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 4, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg2", "%arg3"]}, "execution_time": 115.0}], ["linalg.fill ins(%cst_12 : f32) outs(%45 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>", {"operation": "linalg.fill ins(%cst_12 : f32) outs(%45 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>", "wrapped_operation": "func.func @func_call(%cst_12: f32, %45: tensor<1x16x1xf32>) -> tensor<1x16x1xf32> {\n  %ret = linalg.fill ins(%cst_12 : f32) outs(%45 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n  return %ret : tensor<1x16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x16x1xf32>) -> tensor<1x16x1xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x1xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 1 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x16x1xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x16x1xf32>\n    return %0 : tensor<1x16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_12 = arith.constant 2.00000e+00 : f32\n%tmp_45 = bufferization.alloc_tensor() : tensor<1x16x1xf32>\n%45 = linalg.fill ins(%val : f32) outs(%tmp_45 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_12 : f32) outs(%45 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 59.0}], ["linalg.batch_matmul ins(%expanded_10, %expanded_11 : tensor<1x16x4xf32>, tensor<1x4x1xf32>) outs(%46 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>", {"operation": "linalg.batch_matmul ins(%expanded_10, %expanded_11 : tensor<1x16x4xf32>, tensor<1x4x1xf32>) outs(%46 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>", "wrapped_operation": "func.func @func_call(%expanded_10: tensor<1x16x4xf32>, %expanded_11: tensor<1x4x1xf32>, %46: tensor<1x16x1xf32>) -> tensor<1x16x1xf32> {\n  %ret = linalg.batch_matmul ins(%expanded_10, %expanded_11 : tensor<1x16x4xf32>, tensor<1x4x1xf32>) outs(%46 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n  return %ret : tensor<1x16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x16x4xf32>, %arg1: tensor<1x4x1xf32>, %arg2: tensor<1x16x1xf32>) -> tensor<1x16x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x4x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x16x4xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x16x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x1xf32>\n    memref.copy %2, %alloc : memref<1x16x1xf32> to memref<1x16x1xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 4 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x16x4xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x4x1xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x16x1xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x16x1xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x16x1xf32>\n    return %3 : tensor<1x16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_expanded_10 = bufferization.alloc_tensor() : tensor<1x16x4xf32>\n%expanded_10 = linalg.fill ins(%val : f32) outs(%tmp_expanded_10 : tensor<1x16x4xf32>) -> tensor<1x16x4xf32>\n%tmp_expanded_11 = bufferization.alloc_tensor() : tensor<1x4x1xf32>\n%expanded_11 = linalg.fill ins(%val : f32) outs(%tmp_expanded_11 : tensor<1x4x1xf32>) -> tensor<1x4x1xf32>\n%tmp_46 = bufferization.alloc_tensor() : tensor<1x16x1xf32>\n%46 = linalg.fill ins(%val : f32) outs(%tmp_46 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%expanded_10, %expanded_11 : tensor<1x16x4xf32>, tensor<1x4x1xf32>) outs(%46 : tensor<1x16x1xf32>) -> tensor<1x16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 4, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 141.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_13, %expanded_14 : tensor<16x1xf32>, tensor<1x1xf32>) outs(%48 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_13, %expanded_14 : tensor<16x1xf32>, tensor<1x1xf32>) outs(%48 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", "wrapped_operation": "func.func @func_call(%collapsed_13: tensor<16x1xf32>, %expanded_14: tensor<1x1xf32>, %48: tensor<16x1xf32>) -> tensor<16x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_13, %expanded_14 : tensor<16x1xf32>, tensor<1x1xf32>) outs(%48 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n  return %ret : tensor<16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x1xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x1xf32>) -> tensor<16x1xf32> {\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16xf32>\n    %collapsed_0 = tensor.collapse_shape %arg1 [] : tensor<1x1xf32> into tensor<f32>\n    %1 = bufferization.to_memref %collapsed_0 : memref<f32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16xf32>\n    affine.for %arg3 = 0 to 16 {\n      %3 = affine.load %0[%arg3] : memref<16xf32>\n      %4 = affine.load %1[] : memref<f32>\n      %5 = arith.addf %3, %4 : f32\n      affine.store %5, %alloc[%arg3] : memref<16xf32>\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16xf32>\n    %expanded = tensor.expand_shape %2 [[0, 1]] : tensor<16xf32> into tensor<16x1xf32>\n    return %expanded : tensor<16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_collapsed_13 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%collapsed_13 = linalg.fill ins(%val : f32) outs(%tmp_collapsed_13 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_expanded_14 = bufferization.alloc_tensor() : tensor<1x1xf32>\n%expanded_14 = linalg.fill ins(%val : f32) outs(%tmp_expanded_14 : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_48 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%48 = linalg.fill ins(%val : f32) outs(%tmp_48 : tensor<16x1xf32>) -> tensor<16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%collapsed_13, %expanded_14 : tensor<16x1xf32>, tensor<1x1xf32>) outs(%48 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.addf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"], []], "store_data": ["%arg3"]}, "execution_time": 69.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%49, %49 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%50 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.subf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%49, %49 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%50 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.subf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", "wrapped_operation": "func.func @func_call(%49: tensor<16x1xf32>, %50: tensor<16x1xf32>) -> tensor<16x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%49, %49 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%50 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.subf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n  return %ret : tensor<16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x1xf32>, %arg1: tensor<16x1xf32>) -> tensor<16x1xf32> {\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16xf32>\n    affine.for %arg2 = 0 to 16 {\n      %2 = affine.load %0[%arg2] : memref<16xf32>\n      %3 = arith.subf %2, %2 : f32\n      affine.store %3, %alloc[%arg2] : memref<16xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<16xf32> into tensor<16x1xf32>\n    return %expanded : tensor<16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_49 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%49 = linalg.fill ins(%val : f32) outs(%tmp_49 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_50 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%50 = linalg.fill ins(%val : f32) outs(%tmp_50 : tensor<16x1xf32>) -> tensor<16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%49, %49 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%50 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.subf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1]], "op_count": {"+": 0, "-": 1, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": 47.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%51 : tensor<16x1xf32>) outs(%52 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %58 = math.exp %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%51 : tensor<16x1xf32>) outs(%52 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %58 = math.exp %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", "wrapped_operation": "func.func @func_call(%51: tensor<16x1xf32>, %52: tensor<16x1xf32>) -> tensor<16x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%51 : tensor<16x1xf32>) outs(%52 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %58 = math.exp %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n  return %ret : tensor<16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x1xf32>, %arg1: tensor<16x1xf32>) -> tensor<16x1xf32> {\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16xf32>\n    affine.for %arg2 = 0 to 16 {\n      %2 = affine.load %0[%arg2] : memref<16xf32>\n      %3 = math.exp %2 : f32\n      affine.store %3, %alloc[%arg2] : memref<16xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<16xf32> into tensor<16x1xf32>\n    return %expanded : tensor<16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_51 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%51 = linalg.fill ins(%val : f32) outs(%tmp_51 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_52 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%52 = linalg.fill ins(%val : f32) outs(%tmp_52 : tensor<16x1xf32>) -> tensor<16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%51 : tensor<16x1xf32>) outs(%52 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %58 = math.exp %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 1}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": 127.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53 : tensor<16x1xf32>) outs(%54 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 1.000000e+00 : f32\n  %58 = arith.divf %cst_15, %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53 : tensor<16x1xf32>) outs(%54 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 1.000000e+00 : f32\n  %58 = arith.divf %cst_15, %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", "wrapped_operation": "func.func @func_call(%53: tensor<16x1xf32>, %54: tensor<16x1xf32>) -> tensor<16x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53 : tensor<16x1xf32>) outs(%54 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 1.000000e+00 : f32\n  %58 = arith.divf %cst_15, %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n  return %ret : tensor<16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x1xf32>, %arg1: tensor<16x1xf32>) -> tensor<16x1xf32> {\n    %cst = arith.constant 1.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16xf32>\n    affine.for %arg2 = 0 to 16 {\n      %2 = affine.load %0[%arg2] : memref<16xf32>\n      %3 = arith.divf %cst, %2 : f32\n      affine.store %3, %alloc[%arg2] : memref<16xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<16xf32> into tensor<16x1xf32>\n    return %expanded : tensor<16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_53 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%53 = linalg.fill ins(%val : f32) outs(%tmp_53 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_54 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%54 = linalg.fill ins(%val : f32) outs(%tmp_54 : tensor<16x1xf32>) -> tensor<16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53 : tensor<16x1xf32>) outs(%54 : tensor<16x1xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_15 = arith.constant 1.000000e+00 : f32\n  %58 = arith.divf %cst_15, %in : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 1, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": 73.0}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53, %55 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%56 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.mulf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53, %55 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%56 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.mulf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>", "wrapped_operation": "func.func @func_call(%53: tensor<16x1xf32>, %55: tensor<16x1xf32>, %56: tensor<16x1xf32>) -> tensor<16x1xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53, %55 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%56 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.mulf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n  return %ret : tensor<16x1xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16x1xf32>, %arg1: tensor<16x1xf32>, %arg2: tensor<16x1xf32>) -> tensor<16x1xf32> {\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16xf32>\n    %collapsed_0 = tensor.collapse_shape %arg1 [[0, 1]] : tensor<16x1xf32> into tensor<16xf32>\n    %1 = bufferization.to_memref %collapsed_0 : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16xf32>\n    affine.for %arg3 = 0 to 16 {\n      %3 = affine.load %0[%arg3] : memref<16xf32>\n      %4 = affine.load %1[%arg3] : memref<16xf32>\n      %5 = arith.mulf %3, %4 : f32\n      affine.store %5, %alloc[%arg3] : memref<16xf32>\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16xf32>\n    %expanded = tensor.expand_shape %2 [[0, 1]] : tensor<16xf32> into tensor<16x1xf32>\n    return %expanded : tensor<16x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_53 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%53 = linalg.fill ins(%val : f32) outs(%tmp_53 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_55 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%55 = linalg.fill ins(%val : f32) outs(%tmp_55 : tensor<16x1xf32>) -> tensor<16x1xf32>\n%tmp_56 = bufferization.alloc_tensor() : tensor<16x1xf32>\n%56 = linalg.fill ins(%val : f32) outs(%tmp_56 : tensor<16x1xf32>) -> tensor<16x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%53, %55 : tensor<16x1xf32>, tensor<16x1xf32>) outs(%56 : tensor<16x1xf32>) {\n^bb0(%in: f32, %in_15: f32, %out: f32):\n  %58 = arith.mulf %in, %in_15 : f32\n  linalg.yield %58 : f32\n} -> tensor<16x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1]], "op_count": {"+": 0, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3"], ["%arg3"]], "store_data": ["%arg3"]}, "execution_time": 48.0}]]