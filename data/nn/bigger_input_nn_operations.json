[["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_3 : tensor<6xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x6x28x28xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_3 : tensor<6xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x6x28x28xf32>", "wrapped_operation": "func.func @func_call(%cst_3: tensor<6xf32>, %0: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_3 : tensor<6xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x6x28x28xf32>\n  return %ret : tensor<1x6x28x28xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<6xf32>, %arg1: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<6x28x28xf32>\n    affine.for %arg2 = 0 to 6 {\n      affine.for %arg3 = 0 to 28 {\n        affine.for %arg4 = 0 to 28 {\n          %2 = affine.load %0[%arg2] : memref<6xf32>\n          affine.store %2, %alloc[%arg2, %arg3, %arg4] : memref<6x28x28xf32>\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<6x28x28xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1], [2], [3]] : tensor<6x28x28xf32> into tensor<1x6x28x28xf32>\n    return %expanded : tensor<1x6x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x6x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_3 = bufferization.alloc_tensor() : tensor<6xf32>\n%cst_3 = linalg.fill ins(%val : f32) outs(%tmp_cst_3 : tensor<6xf32>) -> tensor<6xf32>\n%tmp_0 = bufferization.alloc_tensor() : tensor<1x6x28x28xf32>\n%0 = linalg.fill ins(%val : f32) outs(%tmp_0 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_3 : tensor<6xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x6x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x6x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x6x28x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 6, 1], ["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": null}], ["linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%cst_1, %cst_2 : tensor<1x1x32x32xf32>, tensor<6x1x5x5xf32>) outs(%1 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>", {"operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%cst_1, %cst_2 : tensor<1x1x32x32xf32>, tensor<6x1x5x5xf32>) outs(%1 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>", "wrapped_operation": "func.func @func_call(%cst_1: tensor<1x1x32x32xf32>, %cst_2: tensor<6x1x5x5xf32>, %1: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%cst_1, %cst_2 : tensor<1x1x32x32xf32>, tensor<6x1x5x5xf32>) outs(%1 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n  return %ret : tensor<1x6x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1x1x32x32xf32>, %arg1: tensor<6x1x5x5xf32>, %arg2: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<6x1x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x1x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x6x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x6x28x28xf32>\n    memref.copy %2, %alloc : memref<1x6x28x28xf32> to memref<1x6x28x28xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 6 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<1x1x32x32xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<6x1x5x5xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x6x28x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x6x28x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x6x28x28xf32>\n    return %3 : tensor<1x6x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x6x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_1 = bufferization.alloc_tensor() : tensor<1x1x32x32xf32>\n%cst_1 = linalg.fill ins(%val : f32) outs(%tmp_cst_1 : tensor<1x1x32x32xf32>) -> tensor<1x1x32x32xf32>\n%tmp_cst_2 = bufferization.alloc_tensor() : tensor<6x1x5x5xf32>\n%cst_2 = linalg.fill ins(%val : f32) outs(%tmp_cst_2 : tensor<6x1x5x5xf32>) -> tensor<6x1x5x5xf32>\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x6x28x28xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%cst_1, %cst_2 : tensor<1x1x32x32xf32>, tensor<6x1x5x5xf32>) outs(%1 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x6x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x6x28x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 6, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg7", "%arg5 + %arg8", "%arg6 + %arg9"], ["%arg4", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<1x6x28x28xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x6x28x28xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<1x6x28x28xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x6x28x28xf32>", "wrapped_operation": "func.func @func_call(%2: tensor<1x6x28x28xf32>, %0: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<1x6x28x28xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x6x28x28xf32>\n  return %ret : tensor<1x6x28x28xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x6x28x28xf32>, %arg1: tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1], [2], [3]] : tensor<1x6x28x28xf32> into tensor<6x28x28xf32>\n    %0 = bufferization.to_memref %collapsed : memref<6x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<6x28x28xf32>\n    affine.for %arg2 = 0 to 6 {\n      affine.for %arg3 = 0 to 28 {\n        affine.for %arg4 = 0 to 28 {\n          %2 = affine.load %0[%arg2, %arg3, %arg4] : memref<6x28x28xf32>\n          %3 = arith.cmpf ugt, %2, %cst : f32\n          %4 = arith.select %3, %2, %cst : f32\n          affine.store %4, %alloc[%arg2, %arg3, %arg4] : memref<6x28x28xf32>\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<6x28x28xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1], [2], [3]] : tensor<6x28x28xf32> into tensor<1x6x28x28xf32>\n    return %expanded : tensor<1x6x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x6x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_2 = bufferization.alloc_tensor() : tensor<1x6x28x28xf32>\n%2 = linalg.fill ins(%val : f32) outs(%tmp_2 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n%tmp_0 = bufferization.alloc_tensor() : tensor<1x6x28x28xf32>\n%0 = linalg.fill ins(%val : f32) outs(%tmp_0 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<1x6x28x28xf32>) outs(%0 : tensor<1x6x28x28xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x6x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x6x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x6x28x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 6, 1], ["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4"]], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": null}], ["linalg.fill ins(%cst : f32) outs(%4 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>", {"operation": "linalg.fill ins(%cst : f32) outs(%4 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>", "wrapped_operation": "func.func @func_call(%cst: f32, %4: tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32> {\n  %ret = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n  return %ret : tensor<1x6x14x14xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x6x14x14xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 6 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<1x6x14x14xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x6x14x14xf32>\n    return %0 : tensor<1x6x14x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x6x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x6x14x14xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst : f32) outs(%4 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x6x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x6x14x14xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 6, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%3, %6 : tensor<1x6x28x28xf32>, tensor<2x2xf32>) outs(%5 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>", {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%3, %6 : tensor<1x6x28x28xf32>, tensor<2x2xf32>) outs(%5 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>", "wrapped_operation": "func.func @func_call(%3: tensor<1x6x28x28xf32>, %6: tensor<2x2xf32>, %5: tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%3, %6 : tensor<1x6x28x28xf32>, tensor<2x2xf32>) outs(%5 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n  return %ret : tensor<1x6x14x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1x6x28x28xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<1x6x28x28xf32>\n    %1 = bufferization.to_memref %arg2 : memref<1x6x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x6x14x14xf32>\n    memref.copy %1, %alloc : memref<1x6x14x14xf32> to memref<1x6x14x14xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 6 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<1x6x28x28xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x6x14x14xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x6x14x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<1x6x14x14xf32>\n    return %2 : tensor<1x6x14x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x6x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x6x28x28xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x6x28x28xf32>) -> tensor<1x6x28x28xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x6x14x14xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%3, %6 : tensor<1x6x28x28xf32>, tensor<2x2xf32>) outs(%5 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x6x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x6x14x14xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 6, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_5 : tensor<16xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x16x10x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_5 : tensor<16xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x16x10x10xf32>", "wrapped_operation": "func.func @func_call(%cst_5: tensor<16xf32>, %8: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_5 : tensor<16xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x16x10x10xf32>\n  return %ret : tensor<1x16x10x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<16xf32>, %arg1: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x10x10xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 10 {\n        affine.for %arg4 = 0 to 10 {\n          %2 = affine.load %0[%arg2] : memref<16xf32>\n          affine.store %2, %alloc[%arg2, %arg3, %arg4] : memref<16x10x10xf32>\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16x10x10xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1], [2], [3]] : tensor<16x10x10xf32> into tensor<1x16x10x10xf32>\n    return %expanded : tensor<1x16x10x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x10x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_5 = bufferization.alloc_tensor() : tensor<16xf32>\n%cst_5 = linalg.fill ins(%val : f32) outs(%tmp_cst_5 : tensor<16xf32>) -> tensor<16xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<1x16x10x10xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%cst_5 : tensor<16xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<1x16x10x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x10x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x10x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 10, 1], ["%arg4", 0, 10, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": null}], ["linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%7, %cst_4 : tensor<1x6x14x14xf32>, tensor<16x6x5x5xf32>) outs(%9 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>", {"operation": "linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%7, %cst_4 : tensor<1x6x14x14xf32>, tensor<16x6x5x5xf32>) outs(%9 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>", "wrapped_operation": "func.func @func_call(%7: tensor<1x6x14x14xf32>, %cst_4: tensor<16x6x5x5xf32>, %9: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n  %ret = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%7, %cst_4 : tensor<1x6x14x14xf32>, tensor<16x6x5x5xf32>) outs(%9 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n  return %ret : tensor<1x16x10x10xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1x6x14x14xf32>, %arg1: tensor<16x6x5x5xf32>, %arg2: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x6x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x6x14x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x16x10x10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x10x10xf32>\n    memref.copy %2, %alloc : memref<1x16x10x10xf32> to memref<1x16x10x10xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 10 {\n          affine.for %arg6 = 0 to 10 {\n            affine.for %arg7 = 0 to 6 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg5, %arg8)\n                  %5 = affine.apply #map(%arg6, %arg9)\n                  %6 = affine.load %1[%arg3, %arg7, %4, %5] : memref<1x6x14x14xf32>\n                  %7 = affine.load %0[%arg4, %arg7, %arg8, %arg9] : memref<16x6x5x5xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x16x10x10xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x16x10x10xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x16x10x10xf32>\n    return %3 : tensor<1x16x10x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x10x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_7 = bufferization.alloc_tensor() : tensor<1x6x14x14xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<1x6x14x14xf32>) -> tensor<1x6x14x14xf32>\n%tmp_cst_4 = bufferization.alloc_tensor() : tensor<16x6x5x5xf32>\n%cst_4 = linalg.fill ins(%val : f32) outs(%tmp_cst_4 : tensor<16x6x5x5xf32>) -> tensor<16x6x5x5xf32>\n%tmp_9 = bufferization.alloc_tensor() : tensor<1x16x10x10xf32>\n%9 = linalg.fill ins(%val : f32) outs(%tmp_9 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%7, %cst_4 : tensor<1x6x14x14xf32>, tensor<16x6x5x5xf32>) outs(%9 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x10x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x10x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 10, 1], ["%arg6", 0, 10, 1], ["%arg7", 0, 6, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg7", "%arg5 + %arg8", "%arg6 + %arg9"], ["%arg4", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%10 : tensor<1x16x10x10xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x16x10x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%10 : tensor<1x16x10x10xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x16x10x10xf32>", "wrapped_operation": "func.func @func_call(%10: tensor<1x16x10x10xf32>, %8: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%10 : tensor<1x16x10x10xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x16x10x10xf32>\n  return %ret : tensor<1x16x10x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x16x10x10xf32>, %arg1: tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1], [2], [3]] : tensor<1x16x10x10xf32> into tensor<16x10x10xf32>\n    %0 = bufferization.to_memref %collapsed : memref<16x10x10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x10x10xf32>\n    affine.for %arg2 = 0 to 16 {\n      affine.for %arg3 = 0 to 10 {\n        affine.for %arg4 = 0 to 10 {\n          %2 = affine.load %0[%arg2, %arg3, %arg4] : memref<16x10x10xf32>\n          %3 = arith.cmpf ugt, %2, %cst : f32\n          %4 = arith.select %3, %2, %cst : f32\n          affine.store %4, %alloc[%arg2, %arg3, %arg4] : memref<16x10x10xf32>\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<16x10x10xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1], [2], [3]] : tensor<16x10x10xf32> into tensor<1x16x10x10xf32>\n    return %expanded : tensor<1x16x10x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x10x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_10 = bufferization.alloc_tensor() : tensor<1x16x10x10xf32>\n%10 = linalg.fill ins(%val : f32) outs(%tmp_10 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n%tmp_8 = bufferization.alloc_tensor() : tensor<1x16x10x10xf32>\n%8 = linalg.fill ins(%val : f32) outs(%tmp_8 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%10 : tensor<1x16x10x10xf32>) outs(%8 : tensor<1x16x10x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x16x10x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x10x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x10x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 16, 1], ["%arg3", 0, 10, 1], ["%arg4", 0, 10, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3", "%arg4"]], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": null}], ["linalg.fill ins(%cst : f32) outs(%12 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>", {"operation": "linalg.fill ins(%cst : f32) outs(%12 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>", "wrapped_operation": "func.func @func_call(%cst: f32, %12: tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32> {\n  %ret = linalg.fill ins(%cst : f32) outs(%12 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n  return %ret : tensor<1x16x5x5xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x5x5xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 16 {\n        affine.for %arg4 = 0 to 5 {\n          affine.for %arg5 = 0 to 5 {\n            affine.store %arg0, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<1x16x5x5xf32>\n          }\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x16x5x5xf32>\n    return %0 : tensor<1x16x5x5xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst = arith.constant 2.00000e+00 : f32\n%tmp_12 = bufferization.alloc_tensor() : tensor<1x16x5x5xf32>\n%12 = linalg.fill ins(%val : f32) outs(%tmp_12 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst : f32) outs(%12 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x5x5xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 16, 1], ["%arg4", 0, 5, 1], ["%arg5", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4", "%arg5"]}, "execution_time": null}], ["linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%11, %6 : tensor<1x16x10x10xf32>, tensor<2x2xf32>) outs(%13 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>", {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%11, %6 : tensor<1x16x10x10xf32>, tensor<2x2xf32>) outs(%13 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>", "wrapped_operation": "func.func @func_call(%11: tensor<1x16x10x10xf32>, %6: tensor<2x2xf32>, %13: tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%11, %6 : tensor<1x16x10x10xf32>, tensor<2x2xf32>) outs(%13 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n  return %ret : tensor<1x16x5x5xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1x16x10x10xf32>, %arg1: tensor<2x2xf32>, %arg2: tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<1x16x10x10xf32>\n    %1 = bufferization.to_memref %arg2 : memref<1x16x5x5xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x16x5x5xf32>\n    memref.copy %1, %alloc : memref<1x16x5x5xf32> to memref<1x16x5x5xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<1x16x10x10xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x16x5x5xf32>\n                %7 = arith.maxf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<1x16x5x5xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<1x16x5x5xf32>\n    return %2 : tensor<1x16x5x5xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x16x5x5xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_11 = bufferization.alloc_tensor() : tensor<1x16x10x10xf32>\n%11 = linalg.fill ins(%val : f32) outs(%tmp_11 : tensor<1x16x10x10xf32>) -> tensor<1x16x10x10xf32>\n%tmp_6 = bufferization.alloc_tensor() : tensor<2x2xf32>\n%6 = linalg.fill ins(%val : f32) outs(%tmp_6 : tensor<2x2xf32>) -> tensor<2x2xf32>\n%tmp_13 = bufferization.alloc_tensor() : tensor<1x16x5x5xf32>\n%13 = linalg.fill ins(%val : f32) outs(%tmp_13 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%11, %6 : tensor<1x16x10x10xf32>, tensor<2x2xf32>) outs(%13 : tensor<1x16x5x5xf32>) -> tensor<1x16x5x5xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x16x5x5xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x16x5x5xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_6 : tensor<120x400xf32>) outs(%15 : tensor<400x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<400x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_6 : tensor<120x400xf32>) outs(%15 : tensor<400x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<400x120xf32>", "wrapped_operation": "func.func @func_call(%cst_6: tensor<120x400xf32>, %15: tensor<400x120xf32>) -> tensor<400x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_6 : tensor<120x400xf32>) outs(%15 : tensor<400x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<400x120xf32>\n  return %ret : tensor<400x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<120x400xf32>, %arg1: tensor<400x120xf32>) -> tensor<400x120xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<120x400xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<400x120xf32>\n    affine.for %arg2 = 0 to 120 {\n      affine.for %arg3 = 0 to 400 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<120x400xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<400x120xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<400x120xf32>\n    return %1 : tensor<400x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<400x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_6 = bufferization.alloc_tensor() : tensor<120x400xf32>\n%cst_6 = linalg.fill ins(%val : f32) outs(%tmp_cst_6 : tensor<120x400xf32>) -> tensor<120x400xf32>\n%tmp_15 = bufferization.alloc_tensor() : tensor<400x120xf32>\n%15 = linalg.fill ins(%val : f32) outs(%tmp_15 : tensor<400x120xf32>) -> tensor<400x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_6 : tensor<120x400xf32>) outs(%15 : tensor<400x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<400x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<400x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<400x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 120, 1], ["%arg3", 0, 400, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": null}], ["linalg.fill ins(%cst_0 : f32) outs(%17 : tensor<1x120xf32>) -> tensor<1x120xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%17 : tensor<1x120xf32>) -> tensor<1x120xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %17: tensor<1x120xf32>) -> tensor<1x120xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%17 : tensor<1x120xf32>) -> tensor<1x120xf32>\n  return %ret : tensor<1x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x120xf32>) -> tensor<1x120xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x120xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 120 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<1x120xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x120xf32>\n    return %0 : tensor<1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_17 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<1x120xf32>) -> tensor<1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%17 : tensor<1x120xf32>) -> tensor<1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.matmul ins(%collapsed, %16 : tensor<1x400xf32>, tensor<400x120xf32>) outs(%18 : tensor<1x120xf32>) -> tensor<1x120xf32>", {"operation": "linalg.matmul ins(%collapsed, %16 : tensor<1x400xf32>, tensor<400x120xf32>) outs(%18 : tensor<1x120xf32>) -> tensor<1x120xf32>", "wrapped_operation": "func.func @func_call(%collapsed: tensor<1x400xf32>, %16: tensor<400x120xf32>, %18: tensor<1x120xf32>) -> tensor<1x120xf32> {\n  %ret = linalg.matmul ins(%collapsed, %16 : tensor<1x400xf32>, tensor<400x120xf32>) outs(%18 : tensor<1x120xf32>) -> tensor<1x120xf32>\n  return %ret : tensor<1x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x400xf32>, %arg1: tensor<400x120xf32>, %arg2: tensor<1x120xf32>) -> tensor<1x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<400x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x400xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x120xf32>\n    memref.copy %2, %alloc : memref<1x120xf32> to memref<1x120xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 120 {\n        affine.for %arg5 = 0 to 400 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1x400xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<400x120xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1x120xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1x120xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x120xf32>\n    return %3 : tensor<1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_collapsed = bufferization.alloc_tensor() : tensor<1x400xf32>\n%collapsed = linalg.fill ins(%val : f32) outs(%tmp_collapsed : tensor<1x400xf32>) -> tensor<1x400xf32>\n%tmp_16 = bufferization.alloc_tensor() : tensor<400x120xf32>\n%16 = linalg.fill ins(%val : f32) outs(%tmp_16 : tensor<400x120xf32>) -> tensor<400x120xf32>\n%tmp_18 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%18 = linalg.fill ins(%val : f32) outs(%tmp_18 : tensor<1x120xf32>) -> tensor<1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%collapsed, %16 : tensor<1x400xf32>, tensor<400x120xf32>) outs(%18 : tensor<1x120xf32>) -> tensor<1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 120, 1], ["%arg5", 0, 400, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%19, %cst_7 : tensor<1x120xf32>, tensor<120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%19, %cst_7 : tensor<1x120xf32>, tensor<120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x120xf32>", "wrapped_operation": "func.func @func_call(%19: tensor<1x120xf32>, %cst_7: tensor<120xf32>, %17: tensor<1x120xf32>) -> tensor<1x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%19, %cst_7 : tensor<1x120xf32>, tensor<120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x120xf32>\n  return %ret : tensor<1x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x120xf32>, %arg1: tensor<120xf32>, %arg2: tensor<1x120xf32>) -> tensor<1x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120xf32>\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<1x120xf32> into tensor<120xf32>\n    %1 = bufferization.to_memref %collapsed : memref<120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<120xf32>\n    affine.for %arg3 = 0 to 120 {\n      %3 = affine.load %1[%arg3] : memref<120xf32>\n      %4 = affine.load %0[%arg3] : memref<120xf32>\n      %5 = arith.addf %3, %4 : f32\n      affine.store %5, %alloc[%arg3] : memref<120xf32>\n    }\n    %2 = bufferization.to_tensor %alloc : memref<120xf32>\n    %expanded = tensor.expand_shape %2 [[0, 1]] : tensor<120xf32> into tensor<1x120xf32>\n    return %expanded : tensor<1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_19 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%19 = linalg.fill ins(%val : f32) outs(%tmp_19 : tensor<1x120xf32>) -> tensor<1x120xf32>\n%tmp_cst_7 = bufferization.alloc_tensor() : tensor<120xf32>\n%cst_7 = linalg.fill ins(%val : f32) outs(%tmp_cst_7 : tensor<120xf32>) -> tensor<120xf32>\n%tmp_17 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<1x120xf32>) -> tensor<1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%19, %cst_7 : tensor<1x120xf32>, tensor<120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"], ["%arg3"]], "store_data": ["%arg3"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%20 : tensor<1x120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x120xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%20 : tensor<1x120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x120xf32>", "wrapped_operation": "func.func @func_call(%20: tensor<1x120xf32>, %17: tensor<1x120xf32>) -> tensor<1x120xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%20 : tensor<1x120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x120xf32>\n  return %ret : tensor<1x120xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x120xf32>, %arg1: tensor<1x120xf32>) -> tensor<1x120xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<1x120xf32> into tensor<120xf32>\n    %0 = bufferization.to_memref %collapsed : memref<120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<120xf32>\n    affine.for %arg2 = 0 to 120 {\n      %2 = affine.load %0[%arg2] : memref<120xf32>\n      %3 = arith.cmpf ugt, %2, %cst : f32\n      %4 = arith.select %3, %2, %cst : f32\n      affine.store %4, %alloc[%arg2] : memref<120xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<120xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<120xf32> into tensor<1x120xf32>\n    return %expanded : tensor<1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_20 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%20 = linalg.fill ins(%val : f32) outs(%tmp_20 : tensor<1x120xf32>) -> tensor<1x120xf32>\n%tmp_17 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%17 = linalg.fill ins(%val : f32) outs(%tmp_17 : tensor<1x120xf32>) -> tensor<1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%20 : tensor<1x120xf32>) outs(%17 : tensor<1x120xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_8 : tensor<84x120xf32>) outs(%22 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_8 : tensor<84x120xf32>) outs(%22 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>", "wrapped_operation": "func.func @func_call(%cst_8: tensor<84x120xf32>, %22: tensor<120x84xf32>) -> tensor<120x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_8 : tensor<84x120xf32>) outs(%22 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>\n  return %ret : tensor<120x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<84x120xf32>, %arg1: tensor<120x84xf32>) -> tensor<120x84xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<84x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<120x84xf32>\n    affine.for %arg2 = 0 to 84 {\n      affine.for %arg3 = 0 to 120 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<84x120xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<120x84xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<120x84xf32>\n    return %1 : tensor<120x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<120x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_8 = bufferization.alloc_tensor() : tensor<84x120xf32>\n%cst_8 = linalg.fill ins(%val : f32) outs(%tmp_cst_8 : tensor<84x120xf32>) -> tensor<84x120xf32>\n%tmp_22 = bufferization.alloc_tensor() : tensor<120x84xf32>\n%22 = linalg.fill ins(%val : f32) outs(%tmp_22 : tensor<120x84xf32>) -> tensor<120x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_8 : tensor<84x120xf32>) outs(%22 : tensor<120x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<120x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<120x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<120x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 84, 1], ["%arg3", 0, 120, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": null}], ["linalg.fill ins(%cst_0 : f32) outs(%24 : tensor<1x84xf32>) -> tensor<1x84xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%24 : tensor<1x84xf32>) -> tensor<1x84xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %24: tensor<1x84xf32>) -> tensor<1x84xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%24 : tensor<1x84xf32>) -> tensor<1x84xf32>\n  return %ret : tensor<1x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x84xf32>) -> tensor<1x84xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x84xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 84 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<1x84xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x84xf32>\n    return %0 : tensor<1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_24 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%24 = linalg.fill ins(%val : f32) outs(%tmp_24 : tensor<1x84xf32>) -> tensor<1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%24 : tensor<1x84xf32>) -> tensor<1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.matmul ins(%21, %23 : tensor<1x120xf32>, tensor<120x84xf32>) outs(%25 : tensor<1x84xf32>) -> tensor<1x84xf32>", {"operation": "linalg.matmul ins(%21, %23 : tensor<1x120xf32>, tensor<120x84xf32>) outs(%25 : tensor<1x84xf32>) -> tensor<1x84xf32>", "wrapped_operation": "func.func @func_call(%21: tensor<1x120xf32>, %23: tensor<120x84xf32>, %25: tensor<1x84xf32>) -> tensor<1x84xf32> {\n  %ret = linalg.matmul ins(%21, %23 : tensor<1x120xf32>, tensor<120x84xf32>) outs(%25 : tensor<1x84xf32>) -> tensor<1x84xf32>\n  return %ret : tensor<1x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x120xf32>, %arg1: tensor<120x84xf32>, %arg2: tensor<1x84xf32>) -> tensor<1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x84xf32>\n    memref.copy %2, %alloc : memref<1x84xf32> to memref<1x84xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 84 {\n        affine.for %arg5 = 0 to 120 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1x120xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<120x84xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1x84xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1x84xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x84xf32>\n    return %3 : tensor<1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_21 = bufferization.alloc_tensor() : tensor<1x120xf32>\n%21 = linalg.fill ins(%val : f32) outs(%tmp_21 : tensor<1x120xf32>) -> tensor<1x120xf32>\n%tmp_23 = bufferization.alloc_tensor() : tensor<120x84xf32>\n%23 = linalg.fill ins(%val : f32) outs(%tmp_23 : tensor<120x84xf32>) -> tensor<120x84xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<1x84xf32>) -> tensor<1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%21, %23 : tensor<1x120xf32>, tensor<120x84xf32>) outs(%25 : tensor<1x84xf32>) -> tensor<1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 84, 1], ["%arg5", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%26, %cst_9 : tensor<1x84xf32>, tensor<84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%26, %cst_9 : tensor<1x84xf32>, tensor<84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x84xf32>", "wrapped_operation": "func.func @func_call(%26: tensor<1x84xf32>, %cst_9: tensor<84xf32>, %24: tensor<1x84xf32>) -> tensor<1x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%26, %cst_9 : tensor<1x84xf32>, tensor<84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x84xf32>\n  return %ret : tensor<1x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x84xf32>, %arg1: tensor<84xf32>, %arg2: tensor<1x84xf32>) -> tensor<1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84xf32>\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<1x84xf32> into tensor<84xf32>\n    %1 = bufferization.to_memref %collapsed : memref<84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<84xf32>\n    affine.for %arg3 = 0 to 84 {\n      %3 = affine.load %1[%arg3] : memref<84xf32>\n      %4 = affine.load %0[%arg3] : memref<84xf32>\n      %5 = arith.addf %3, %4 : f32\n      affine.store %5, %alloc[%arg3] : memref<84xf32>\n    }\n    %2 = bufferization.to_tensor %alloc : memref<84xf32>\n    %expanded = tensor.expand_shape %2 [[0, 1]] : tensor<84xf32> into tensor<1x84xf32>\n    return %expanded : tensor<1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_26 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%26 = linalg.fill ins(%val : f32) outs(%tmp_26 : tensor<1x84xf32>) -> tensor<1x84xf32>\n%tmp_cst_9 = bufferization.alloc_tensor() : tensor<84xf32>\n%cst_9 = linalg.fill ins(%val : f32) outs(%tmp_cst_9 : tensor<84xf32>) -> tensor<84xf32>\n%tmp_24 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%24 = linalg.fill ins(%val : f32) outs(%tmp_24 : tensor<1x84xf32>) -> tensor<1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%26, %cst_9 : tensor<1x84xf32>, tensor<84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"], ["%arg3"]], "store_data": ["%arg3"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%27 : tensor<1x84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x84xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%27 : tensor<1x84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x84xf32>", "wrapped_operation": "func.func @func_call(%27: tensor<1x84xf32>, %24: tensor<1x84xf32>) -> tensor<1x84xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%27 : tensor<1x84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x84xf32>\n  return %ret : tensor<1x84xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x84xf32>, %arg1: tensor<1x84xf32>) -> tensor<1x84xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<1x84xf32> into tensor<84xf32>\n    %0 = bufferization.to_memref %collapsed : memref<84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<84xf32>\n    affine.for %arg2 = 0 to 84 {\n      %2 = affine.load %0[%arg2] : memref<84xf32>\n      %3 = arith.cmpf ugt, %2, %cst : f32\n      %4 = arith.select %3, %2, %cst : f32\n      affine.store %4, %alloc[%arg2] : memref<84xf32>\n    }\n    %1 = bufferization.to_tensor %alloc : memref<84xf32>\n    %expanded = tensor.expand_shape %1 [[0, 1]] : tensor<84xf32> into tensor<1x84xf32>\n    return %expanded : tensor<1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_27 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%27 = linalg.fill ins(%val : f32) outs(%tmp_27 : tensor<1x84xf32>) -> tensor<1x84xf32>\n%tmp_24 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%24 = linalg.fill ins(%val : f32) outs(%tmp_24 : tensor<1x84xf32>) -> tensor<1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%27 : tensor<1x84xf32>) outs(%24 : tensor<1x84xf32>) {\n^bb0(%in: f32, %out: f32):\n  %cst_12 = arith.constant 0.000000e+00 : f32\n  %35 = arith.cmpf ugt, %in, %cst_12 : f32\n  %36 = arith.select %35, %in, %cst_12 : f32\n  linalg.yield %36 : f32\n} -> tensor<1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2"]], "store_data": ["%arg2"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_10 : tensor<10x84xf32>) outs(%29 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_10 : tensor<10x84xf32>) outs(%29 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>", "wrapped_operation": "func.func @func_call(%cst_10: tensor<10x84xf32>, %29: tensor<84x10xf32>) -> tensor<84x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_10 : tensor<10x84xf32>) outs(%29 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>\n  return %ret : tensor<84x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<10x84xf32>, %arg1: tensor<84x10xf32>) -> tensor<84x10xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<10x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<84x10xf32>\n    affine.for %arg2 = 0 to 10 {\n      affine.for %arg3 = 0 to 84 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<10x84xf32>\n        affine.store %2, %alloc[%arg3, %arg2] : memref<84x10xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<84x10xf32>\n    return %1 : tensor<84x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<84x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_cst_10 = bufferization.alloc_tensor() : tensor<10x84xf32>\n%cst_10 = linalg.fill ins(%val : f32) outs(%tmp_cst_10 : tensor<10x84xf32>) -> tensor<10x84xf32>\n%tmp_29 = bufferization.alloc_tensor() : tensor<84x10xf32>\n%29 = linalg.fill ins(%val : f32) outs(%tmp_29 : tensor<84x10xf32>) -> tensor<84x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_10 : tensor<10x84xf32>) outs(%29 : tensor<84x10xf32>) {\n^bb0(%in: f32, %out: f32):\n  linalg.yield %in : f32\n} -> tensor<84x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<84x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<84x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 10, 1], ["%arg3", 0, 84, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg2", "%arg3"]], "store_data": ["%arg3", "%arg2"]}, "execution_time": null}], ["linalg.fill ins(%cst_0 : f32) outs(%31 : tensor<1x10xf32>) -> tensor<1x10xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%31 : tensor<1x10xf32>) -> tensor<1x10xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %31: tensor<1x10xf32>) -> tensor<1x10xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%31 : tensor<1x10xf32>) -> tensor<1x10xf32>\n  return %ret : tensor<1x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x10xf32>) -> tensor<1x10xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x10xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 10 {\n        affine.store %arg0, %alloc[%arg2, %arg3] : memref<1x10xf32>\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x10xf32>\n    return %0 : tensor<1x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_31 = bufferization.alloc_tensor() : tensor<1x10xf32>\n%31 = linalg.fill ins(%val : f32) outs(%tmp_31 : tensor<1x10xf32>) -> tensor<1x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%31 : tensor<1x10xf32>) -> tensor<1x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 10, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3"]}, "execution_time": null}], ["linalg.matmul ins(%28, %30 : tensor<1x84xf32>, tensor<84x10xf32>) outs(%32 : tensor<1x10xf32>) -> tensor<1x10xf32>", {"operation": "linalg.matmul ins(%28, %30 : tensor<1x84xf32>, tensor<84x10xf32>) outs(%32 : tensor<1x10xf32>) -> tensor<1x10xf32>", "wrapped_operation": "func.func @func_call(%28: tensor<1x84xf32>, %30: tensor<84x10xf32>, %32: tensor<1x10xf32>) -> tensor<1x10xf32> {\n  %ret = linalg.matmul ins(%28, %30 : tensor<1x84xf32>, tensor<84x10xf32>) outs(%32 : tensor<1x10xf32>) -> tensor<1x10xf32>\n  return %ret : tensor<1x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x84xf32>, %arg1: tensor<84x10xf32>, %arg2: tensor<1x10xf32>) -> tensor<1x10xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x10xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x10xf32>\n    memref.copy %2, %alloc : memref<1x10xf32> to memref<1x10xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 10 {\n        affine.for %arg5 = 0 to 84 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1x84xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<84x10xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1x10xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1x10xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x10xf32>\n    return %3 : tensor<1x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<1x84xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<1x84xf32>) -> tensor<1x84xf32>\n%tmp_30 = bufferization.alloc_tensor() : tensor<84x10xf32>\n%30 = linalg.fill ins(%val : f32) outs(%tmp_30 : tensor<84x10xf32>) -> tensor<84x10xf32>\n%tmp_32 = bufferization.alloc_tensor() : tensor<1x10xf32>\n%32 = linalg.fill ins(%val : f32) outs(%tmp_32 : tensor<1x10xf32>) -> tensor<1x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.matmul ins(%28, %30 : tensor<1x84xf32>, tensor<84x10xf32>) outs(%32 : tensor<1x10xf32>) -> tensor<1x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 10, 1], ["%arg5", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": null}], ["linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%33, %cst_11 : tensor<1x10xf32>, tensor<10xf32>) outs(%31 : tensor<1x10xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x10xf32>", {"operation": "linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%33, %cst_11 : tensor<1x10xf32>, tensor<10xf32>) outs(%31 : tensor<1x10xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x10xf32>", "wrapped_operation": "func.func @func_call(%33: tensor<1x10xf32>, %cst_11: tensor<10xf32>, %31: tensor<1x10xf32>) -> tensor<1x10xf32> {\n  %ret = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%33, %cst_11 : tensor<1x10xf32>, tensor<10xf32>) outs(%31 : tensor<1x10xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x10xf32>\n  return %ret : tensor<1x10xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<1x10xf32>) -> tensor<1x10xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<10xf32>\n    %collapsed = tensor.collapse_shape %arg0 [[0, 1]] : tensor<1x10xf32> into tensor<10xf32>\n    %1 = bufferization.to_memref %collapsed : memref<10xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<10xf32>\n    affine.for %arg3 = 0 to 10 {\n      %3 = affine.load %1[%arg3] : memref<10xf32>\n      %4 = affine.load %0[%arg3] : memref<10xf32>\n      %5 = arith.addf %3, %4 : f32\n      affine.store %5, %alloc[%arg3] : memref<10xf32>\n    }\n    %2 = bufferization.to_tensor %alloc : memref<10xf32>\n    %expanded = tensor.expand_shape %2 [[0, 1]] : tensor<10xf32> into tensor<1x10xf32>\n    return %expanded : tensor<1x10xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x10xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_33 = bufferization.alloc_tensor() : tensor<1x10xf32>\n%33 = linalg.fill ins(%val : f32) outs(%tmp_33 : tensor<1x10xf32>) -> tensor<1x10xf32>\n%tmp_cst_11 = bufferization.alloc_tensor() : tensor<10xf32>\n%cst_11 = linalg.fill ins(%val : f32) outs(%tmp_cst_11 : tensor<10xf32>) -> tensor<10xf32>\n%tmp_31 = bufferization.alloc_tensor() : tensor<1x10xf32>\n%31 = linalg.fill ins(%val : f32) outs(%tmp_31 : tensor<1x10xf32>) -> tensor<1x10xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%33, %cst_11 : tensor<1x10xf32>, tensor<10xf32>) outs(%31 : tensor<1x10xf32>) {\n^bb0(%in: f32, %in_12: f32, %out: f32):\n  %35 = arith.addf %in, %in_12 : f32\n  linalg.yield %35 : f32\n} -> tensor<1x10xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x10xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x10xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 10, 1]], "op_count": {"+": 1, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3"], ["%arg3"]], "store_data": ["%arg3"]}, "execution_time": null}]]