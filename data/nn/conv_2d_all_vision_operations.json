[["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<2240x1x1x224xf32>, %7: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n  return %ret : tensor<512x1x1x2240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<2240x1x1x224xf32>, %arg2: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<2240x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x2240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x2240xf32>\n    memref.copy %2, %alloc : memref<512x1x1x2240xf32> to memref<512x1x1x2240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 2240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<2240x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x2240xf32>\n    return %3 : tensor<512x1x1x2240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x2240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<2240x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<2240x1x1x224xf32>) -> tensor<2240x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x2240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x2240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x2240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 2240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 912989491}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<12x1x1x48xf32>, %7: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n  return %ret : tensor<512x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<12x1x1x48xf32>, %arg2: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x12xf32>\n    memref.copy %2, %alloc : memref<512x1x1x12xf32> to memref<512x1x1x12xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x12xf32>\n    return %3 : tensor<512x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x48xf32>) -> tensor<12x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 796770}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x83x83x42xf32>, %3: tensor<42x1x1x42xf32>, %7: tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n  return %ret : tensor<256x83x83x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x83x83x42xf32>, %arg1: tensor<42x1x1x42xf32>, %arg2: tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x83x83x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x83x83x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x83x83x42xf32>\n    memref.copy %2, %alloc : memref<256x83x83x42xf32> to memref<256x83x83x42xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x83x83x42xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x83x83x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x83x83x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x83x83x42xf32>\n    return %3 : tensor<256x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x83x83x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x42xf32>) -> tensor<42x1x1x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x83x83x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9644966147}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x11x11x176xf32>, %3: tensor<5x5x176x1xf32>, %7: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n  return %ret : tensor<256x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x11x11x176xf32>, %arg1: tensor<5x5x176x1xf32>, %arg2: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x11x11x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<256x7x7x176x1xf32> to memref<256x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x11x11x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x176x1xf32>\n    return %3 : tensor<256x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x11x11x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x11x11x176xf32>) -> tensor<256x11x11x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x176x1xf32>) -> tensor<5x5x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 158767669}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x113x113x96xf32>, %3: tensor<3x3x96x1xf32>, %7: tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32>\n  return %ret : tensor<128x56x56x96x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x113x113x96xf32>, %arg1: tensor<3x3x96x1xf32>, %arg2: tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x96x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x113x113x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x96x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x96x1xf32>\n    memref.copy %2, %alloc : memref<128x56x56x96x1xf32> to memref<128x56x56x96x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x113x113x96xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x96x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x96x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x96x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x96x1xf32>\n    return %3 : tensor<128x56x56x96x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x96x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x113x113x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x113x113x96xf32>) -> tensor<128x113x113x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x96x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x96x1xf32>) -> tensor<3x3x96x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x96x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<128x56x56x96x1xf32>) -> tensor<128x56x56x96x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x96x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x96x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 890580459}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x480xf32>, %3: tensor<128x1x1x480xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x480xf32>, %arg1: tensor<128x1x1x480xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x480xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x480xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 480 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x480xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x480xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x480xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x480xf32>) -> tensor<256x14x14x480xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x480xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x480xf32>) -> tensor<128x1x1x480xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 480, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11460368176}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x324xf32>, %3: tensor<1296x1x1x324xf32>, %7: tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32>\n  return %ret : tensor<256x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x324xf32>, %arg1: tensor<1296x1x1x324xf32>, %arg2: tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1296x1x1x324xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x324xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<256x1x1x1296xf32> to memref<256x1x1x1296xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 324 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x324xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1296x1x1x324xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x1296xf32>\n    return %3 : tensor<256x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x324xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x324xf32>) -> tensor<256x1x1x324xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1296x1x1x324xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1296x1x1x324xf32>) -> tensor<1296x1x1x324xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<256x1x1x1296xf32>) -> tensor<256x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 324, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 389348857}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x48xf32>, %3: tensor<104x1x1x48xf32>, %7: tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>\n  return %ret : tensor<256x28x28x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x48xf32>, %arg1: tensor<104x1x1x48xf32>, %arg2: tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x104xf32>\n    memref.copy %2, %alloc : memref<256x28x28x104xf32> to memref<256x28x28x104xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x104xf32>\n    return %3 : tensor<256x28x28x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x48xf32>) -> tensor<256x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x48xf32>) -> tensor<104x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3133526682}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x34x34x22xf32>, %3: tensor<7x7x22x1xf32>, %7: tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n  return %ret : tensor<256x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x34x34x22xf32>, %arg1: tensor<7x7x22x1xf32>, %arg2: tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x34x34x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<256x28x28x22x1xf32> to memref<256x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x34x34x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x22x1xf32>\n    return %3 : tensor<256x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x34x34x22xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x34x34x22xf32>) -> tensor<256x34x34x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x22x1xf32>) -> tensor<7x7x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 718300982}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1216xf32>, %3: tensor<128x1x1x1216xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1216xf32>, %arg1: tensor<128x1x1x1216xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1216xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1216xf32>) -> tensor<128x7x7x1216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1216xf32>) -> tensor<128x1x1x1216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3665327890}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x448xf32>, %3: tensor<42x1x1x448xf32>, %7: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n  return %ret : tensor<512x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x448xf32>, %arg1: tensor<42x1x1x448xf32>, %arg2: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x42xf32>\n    memref.copy %2, %alloc : memref<512x1x1x42xf32> to memref<512x1x1x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x42xf32>\n    return %3 : tensor<512x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x448xf32>) -> tensor<512x1x1x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x448xf32>) -> tensor<42x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 35266442}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1824xf32>, %3: tensor<128x1x1x1824xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1824xf32>, %arg1: tensor<128x1x1x1824xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1824xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1824xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1824 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1824xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1824xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1824xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1824xf32>) -> tensor<256x7x7x1824xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1824xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1824xf32>) -> tensor<128x1x1x1824xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1824, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11020632429}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x32xf32>, %3: tensor<128x1x1x32xf32>, %7: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n  return %ret : tensor<128x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x32xf32>, %arg1: tensor<128x1x1x32xf32>, %arg2: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x128xf32>\n    memref.copy %2, %alloc : memref<128x1x1x128xf32> to memref<128x1x1x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x128xf32>\n    return %3 : tensor<128x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1297359}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x864xf32>, %3: tensor<128x1x1x864xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x864xf32>, %arg1: tensor<128x1x1x864xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x864xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x864xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 864 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x864xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x864xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x864xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x864xf32>) -> tensor<128x7x7x864xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x864xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x864xf32>) -> tensor<128x1x1x864xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 864, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2597705999}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x224xf32>, %3: tensor<56x1x1x224xf32>, %7: tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>\n  return %ret : tensor<128x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x224xf32>, %arg1: tensor<56x1x1x224xf32>, %arg2: tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x56xf32>\n    memref.copy %2, %alloc : memref<128x1x1x56xf32> to memref<128x1x1x56xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x56xf32>\n    return %3 : tensor<128x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x224xf32>) -> tensor<56x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5696206}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x120xf32>, %3: tensor<336x1x1x120xf32>, %7: tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32>\n  return %ret : tensor<128x28x28x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x120xf32>, %arg1: tensor<336x1x1x120xf32>, %arg2: tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x336xf32>\n    memref.copy %2, %alloc : memref<128x28x28x336xf32> to memref<128x28x28x336xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x336xf32>\n    return %3 : tensor<128x28x28x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x120xf32>) -> tensor<336x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<128x28x28x336xf32>) -> tensor<128x28x28x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14291697653}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x72xf32>, %3: tensor<216x1x1x72xf32>, %7: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n  return %ret : tensor<512x28x28x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x72xf32>, %arg1: tensor<216x1x1x72xf32>, %arg2: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<216x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x216xf32>\n    memref.copy %2, %alloc : memref<512x28x28x216xf32> to memref<512x28x28x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<216x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x216xf32>\n    return %3 : tensor<512x28x28x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<216x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<216x1x1x72xf32>) -> tensor<216x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20965130380}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x576xf32>, %3: tensor<160x1x1x576xf32>, %7: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n  return %ret : tensor<512x7x7x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x576xf32>, %arg1: tensor<160x1x1x576xf32>, %arg2: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x160xf32>\n    memref.copy %2, %alloc : memref<512x7x7x160xf32> to memref<512x7x7x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x160xf32>\n    return %3 : tensor<512x7x7x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x576xf32>) -> tensor<512x7x7x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x576xf32>) -> tensor<160x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8626333275}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<96x1x1x48xf32>, %7: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n  return %ret : tensor<512x28x28x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<96x1x1x48xf32>, %arg2: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x96xf32>\n    memref.copy %2, %alloc : memref<512x28x28x96xf32> to memref<512x28x28x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x96xf32>\n    return %3 : tensor<512x28x28x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x48xf32>) -> tensor<96x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5950640842}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<336x1x1x32xf32>) outs(%7 : tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<336x1x1x32xf32>) outs(%7 : tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<336x1x1x32xf32>, %7: tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<336x1x1x32xf32>) outs(%7 : tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32>\n  return %ret : tensor<256x56x56x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<336x1x1x32xf32>, %arg2: tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x336xf32>\n    memref.copy %2, %alloc : memref<256x56x56x336xf32> to memref<256x56x56x336xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x336xf32>\n    return %3 : tensor<256x56x56x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x32xf32>) -> tensor<336x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<336x1x1x32xf32>) outs(%7 : tensor<256x56x56x336xf32>) -> tensor<256x56x56x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23976174107}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1632xf32>, %3: tensor<128x1x1x1632xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1632xf32>, %arg1: tensor<128x1x1x1632xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1632xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1632xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1632 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1632xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1632xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1632xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1632xf32>) -> tensor<128x14x14x1632xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1632xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1632xf32>) -> tensor<128x1x1x1632xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1632, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19709357495}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x26xf32>, %3: tensor<104x1x1x26xf32>, %7: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n  return %ret : tensor<512x1x1x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x26xf32>, %arg1: tensor<104x1x1x26xf32>, %arg2: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x104xf32>\n    memref.copy %2, %alloc : memref<512x1x1x104xf32> to memref<512x1x1x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x26xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x104xf32>\n    return %3 : tensor<512x1x1x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x26xf32>) -> tensor<104x1x1x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3176556}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<80x1x1x32xf32>, %7: tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>\n  return %ret : tensor<128x56x56x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<80x1x1x32xf32>, %arg2: tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x80xf32>\n    memref.copy %2, %alloc : memref<128x56x56x80xf32> to memref<128x56x56x80xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x80xf32>\n    return %3 : tensor<128x56x56x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x32xf32>) -> tensor<80x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2897517686}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1088xf32>, %3: tensor<128x1x1x1088xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1088xf32>, %arg1: tensor<128x1x1x1088xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1088xf32>) -> tensor<128x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 266237052}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x800xf32>, %3: tensor<128x1x1x800xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x800xf32>, %arg1: tensor<128x1x1x800xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x800xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x800xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 800 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x800xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x800xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x800xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x800xf32>) -> tensor<512x7x7x800xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x800xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x800xf32>) -> tensor<128x1x1x800xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 800, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9613292617}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x152xf32>, %3: tensor<152x1x1x152xf32>, %7: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n  return %ret : tensor<512x14x14x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x152xf32>, %arg1: tensor<152x1x1x152xf32>, %arg2: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x152xf32>\n    memref.copy %2, %alloc : memref<512x14x14x152xf32> to memref<512x14x14x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x152xf32>\n    return %3 : tensor<512x14x14x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x152xf32>) -> tensor<152x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8380352653}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x87x87x84xf32>, %3: tensor<5x5x84x1xf32>, %7: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n  return %ret : tensor<128x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x87x87x84xf32>, %arg1: tensor<5x5x84x1xf32>, %arg2: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x87x87x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<128x42x42x84x1xf32> to memref<128x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x87x87x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x42x42x84x1xf32>\n    return %3 : tensor<128x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x87x87x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x87x87x84xf32>) -> tensor<128x87x87x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x84x1xf32>) -> tensor<5x5x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1354874569}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x114x114x32xf32>, %3: tensor<3x3x32x1xf32>, %7: tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32>\n  return %ret : tensor<128x112x112x32x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x114x114x32xf32>, %arg1: tensor<3x3x32x1xf32>, %arg2: tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x32x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x114x114x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x112x112x32x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x112x112x32x1xf32>\n    memref.copy %2, %alloc : memref<128x112x112x32x1xf32> to memref<128x112x112x32x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x114x114x32xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x32x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x112x112x32x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x112x112x32x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x112x112x32x1xf32>\n    return %3 : tensor<128x112x112x32x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x112x112x32x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x114x114x32xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x114x114x32xf32>) -> tensor<128x114x114x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x32x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x32x1xf32>) -> tensor<3x3x32x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x112x112x32x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<128x112x112x32x1xf32>) -> tensor<128x112x112x32x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x112x112x32x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x112x112x32x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1176171255}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<64x1x1x32xf32>, %7: tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n  return %ret : tensor<256x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<64x1x1x32xf32>, %arg2: tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x64xf32>\n    memref.copy %2, %alloc : memref<256x56x56x64xf32> to memref<256x56x56x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x64xf32>\n    return %3 : tensor<256x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x32xf32>) -> tensor<64x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4588228860}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x216xf32>, %3: tensor<18x1x1x216xf32>, %7: tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>\n  return %ret : tensor<256x1x1x18xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x216xf32>, %arg1: tensor<18x1x1x216xf32>, %arg2: tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<18x1x1x216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x18xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x18xf32>\n    memref.copy %2, %alloc : memref<256x1x1x18xf32> to memref<256x1x1x18xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 18 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x216xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<18x1x1x216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x18xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x18xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x18xf32>\n    return %3 : tensor<256x1x1x18xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x18xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x216xf32>) -> tensor<256x1x1x216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<18x1x1x216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<18x1x1x216xf32>) -> tensor<18x1x1x216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x18xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x18xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x18xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 18, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3514559}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1280xf32>, %3: tensor<128x1x1x1280xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1280xf32>, %arg1: tensor<128x1x1x1280xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1280xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1280 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1280xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1280xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1280xf32>) -> tensor<512x7x7x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1280xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1280xf32>) -> tensor<128x1x1x1280xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15437223956}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1376xf32>, %3: tensor<128x1x1x1376xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1376xf32>, %arg1: tensor<128x1x1x1376xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1376xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1376xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1376 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1376xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1376xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1376xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1376xf32>) -> tensor<128x7x7x1376xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1376xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1376xf32>) -> tensor<128x1x1x1376xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1376, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4150551527}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x147x147x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x147x147x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x147x147x64xf32>, %3: tensor<128x1x1x64xf32>, %7: tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x147x147x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32>\n  return %ret : tensor<128x74x74x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x147x147x64xf32>, %arg1: tensor<128x1x1x64xf32>, %arg2: tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x147x147x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x74x74x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x74x74x128xf32>\n    memref.copy %2, %alloc : memref<128x74x74x128xf32> to memref<128x74x74x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 74 {\n        affine.for %arg5 = 0 to 74 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x147x147x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x74x74x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x74x74x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x74x74x128xf32>\n    return %3 : tensor<128x74x74x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x74x74x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x147x147x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x147x147x64xf32>) -> tensor<128x147x147x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x64xf32>) -> tensor<128x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x74x74x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x147x147x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<128x74x74x128xf32>) -> tensor<128x74x74x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x74x74x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x74x74x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 74, 1], ["%arg5", 0, 74, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19034414468}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x64xf32>, %3: tensor<8x1x1x64xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x64xf32>, %arg1: tensor<8x1x1x64xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x64xf32>) -> tensor<8x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 376946}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x528xf32>, tensor<176x1x1x528xf32>) outs(%7 : tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x528xf32>, tensor<176x1x1x528xf32>) outs(%7 : tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x528xf32>, %3: tensor<176x1x1x528xf32>, %7: tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x528xf32>, tensor<176x1x1x528xf32>) outs(%7 : tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32>\n  return %ret : tensor<128x14x14x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x528xf32>, %arg1: tensor<176x1x1x528xf32>, %arg2: tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<176x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x176xf32>\n    memref.copy %2, %alloc : memref<128x14x14x176xf32> to memref<128x14x14x176xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<176x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x176xf32>\n    return %3 : tensor<128x14x14x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x528xf32>) -> tensor<128x14x14x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<176x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<176x1x1x528xf32>) -> tensor<176x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x528xf32>, tensor<176x1x1x528xf32>) outs(%7 : tensor<128x14x14x176xf32>) -> tensor<128x14x14x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8679381463}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x64xf32>, %3: tensor<160x1x1x64xf32>, %7: tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>\n  return %ret : tensor<256x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x64xf32>, %arg1: tensor<160x1x1x64xf32>, %arg2: tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x160xf32>\n    memref.copy %2, %alloc : memref<256x14x14x160xf32> to memref<256x14x14x160xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x160xf32>\n    return %3 : tensor<256x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x64xf32>) -> tensor<256x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x64xf32>) -> tensor<160x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1718333711}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x864xf32>, %3: tensor<128x1x1x864xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x864xf32>, %arg1: tensor<128x1x1x864xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x864xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x864xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 864 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x864xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x864xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x864xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x864xf32>) -> tensor<256x14x14x864xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x864xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x864xf32>) -> tensor<128x1x1x864xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 864, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20781153942}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x528xf32>, %3: tensor<88x1x1x528xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x528xf32>, %arg1: tensor<88x1x1x528xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x528xf32>) -> tensor<512x14x14x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x528xf32>) -> tensor<88x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17346483100}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x256xf32>, %3: tensor<64x1x1x256xf32>, %7: tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32>\n  return %ret : tensor<128x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x256xf32>, %arg1: tensor<64x1x1x256xf32>, %arg2: tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x64xf32>\n    memref.copy %2, %alloc : memref<128x56x56x64xf32> to memref<128x56x56x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x64xf32>\n    return %3 : tensor<128x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x256xf32>) -> tensor<64x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x56x56x64xf32>) -> tensor<128x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24086277344}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1856xf32>, %3: tensor<128x1x1x1856xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1856xf32>, %arg1: tensor<128x1x1x1856xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1856xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1856xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1856 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1856xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1856xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1856xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1856xf32>) -> tensor<512x7x7x1856xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1856xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1856xf32>) -> tensor<128x1x1x1856xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1856, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22428798123}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<48x3x3x32xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<48x3x3x32xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x37x37x32xf32>, %3: tensor<48x3x3x32xf32>, %7: tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<48x3x3x32xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n  return %ret : tensor<128x35x35x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x37x37x32xf32>, %arg1: tensor<48x3x3x32xf32>, %arg2: tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x3x3x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x37x37x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x48xf32>\n    memref.copy %2, %alloc : memref<128x35x35x48xf32> to memref<128x35x35x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x37x37x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x3x3x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x48xf32>\n    return %3 : tensor<128x35x35x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x37x37x32xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x37x37x32xf32>) -> tensor<128x37x37x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x3x3x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x3x3x32xf32>) -> tensor<48x3x3x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<48x3x3x32xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8026392105}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x30xf32>, %3: tensor<336x1x1x30xf32>, %7: tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n  return %ret : tensor<128x1x1x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x30xf32>, %arg1: tensor<336x1x1x30xf32>, %arg2: tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x30xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x30xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x336xf32>\n    memref.copy %2, %alloc : memref<128x1x1x336xf32> to memref<128x1x1x336xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 30 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x30xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x30xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x336xf32>\n    return %3 : tensor<128x1x1x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x30xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x30xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x30xf32>) -> tensor<336x1x1x30xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 30, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3033680}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x87x87x42xf32>, %3: tensor<5x5x42x1xf32>, %7: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n  return %ret : tensor<128x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x87x87x42xf32>, %arg1: tensor<5x5x42x1xf32>, %arg2: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x87x87x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<128x83x83x42x1xf32> to memref<128x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x87x87x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x83x83x42x1xf32>\n    return %3 : tensor<128x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x87x87x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x87x87x42xf32>) -> tensor<128x87x87x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x42x1xf32>) -> tensor<5x5x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2599765365}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x56xf32>, %3: tensor<56x1x1x56xf32>, %7: tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n  return %ret : tensor<128x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x56xf32>, %arg1: tensor<56x1x1x56xf32>, %arg2: tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x56xf32>\n    memref.copy %2, %alloc : memref<128x28x28x56xf32> to memref<128x28x28x56xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x56xf32>\n    return %3 : tensor<128x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x56xf32>) -> tensor<56x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1078687564}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x208xf32>, %3: tensor<440x1x1x208xf32>, %7: tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32>\n  return %ret : tensor<128x14x14x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x208xf32>, %arg1: tensor<440x1x1x208xf32>, %arg2: tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x440xf32>\n    memref.copy %2, %alloc : memref<128x14x14x440xf32> to memref<128x14x14x440xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x440xf32>\n    return %3 : tensor<128x14x14x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x208xf32>) -> tensor<128x14x14x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x208xf32>) -> tensor<440x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<128x14x14x440xf32>) -> tensor<128x14x14x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8339261150}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<128x4x4x3xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<128x4x4x3xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x224x224x3xf32>, %3: tensor<128x4x4x3xf32>, %7: tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<128x4x4x3xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n  return %ret : tensor<128x56x56x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 4 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x224x224x3xf32>, %arg1: tensor<128x4x4x3xf32>, %arg2: tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x4x4x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x128xf32>\n    memref.copy %2, %alloc : memref<128x56x56x128xf32> to memref<128x56x56x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 4 {\n              affine.for %arg8 = 0 to 4 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x4x4x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x128xf32>\n    return %3 : tensor<128x56x56x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x224x224x3xf32>) -> tensor<128x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x4x4x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x4x4x3xf32>) -> tensor<128x4x4x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<128x4x4x3xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 4, 1], ["%arg8", 0, 4, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 4 + %arg7", "%arg5 * 4 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8120175019}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x8x8x2048xf32>, tensor<192x1x1x2048xf32>) outs(%7 : tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x8x8x2048xf32>, tensor<192x1x1x2048xf32>) outs(%7 : tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x8x8x2048xf32>, %3: tensor<192x1x1x2048xf32>, %7: tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x8x8x2048xf32>, tensor<192x1x1x2048xf32>) outs(%7 : tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32>\n  return %ret : tensor<256x8x8x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x8x8x2048xf32>, %arg1: tensor<192x1x1x2048xf32>, %arg2: tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x2048xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x8x8x2048xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x8x8x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x8x8x192xf32>\n    memref.copy %2, %alloc : memref<256x8x8x192xf32> to memref<256x8x8x192xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 2048 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x8x8x2048xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x2048xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x8x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x8x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x8x8x192xf32>\n    return %3 : tensor<256x8x8x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x8x8x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x8x8x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x8x8x2048xf32>) -> tensor<256x8x8x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x2048xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x2048xf32>) -> tensor<192x1x1x2048xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x8x8x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x8x8x2048xf32>, tensor<192x1x1x2048xf32>) outs(%7 : tensor<256x8x8x192xf32>) -> tensor<256x8x8x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x8x8x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x8x8x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 2048, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24277365901}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x208xf32>, %3: tensor<26x1x1x208xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x208xf32>, %arg1: tensor<26x1x1x208xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<26x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<26x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<26x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<26x1x1x208xf32>) -> tensor<26x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9923788}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x42x42x84xf32>, %3: tensor<84x1x1x84xf32>, %7: tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>\n  return %ret : tensor<256x42x42x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x42x42x84xf32>, %arg1: tensor<84x1x1x84xf32>, %arg2: tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x42x42x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x42x42x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x42x42x84xf32>\n    memref.copy %2, %alloc : memref<256x42x42x84xf32> to memref<256x42x42x84xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 84 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x42x42x84xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x42x42x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x42x42x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x42x42x84xf32>\n    return %3 : tensor<256x42x42x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x42x42x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x42x42x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x84xf32>) -> tensor<84x1x1x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x42x42x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<256x42x42x84xf32>) -> tensor<256x42x42x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x42x42x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x42x42x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10897260378}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x192xf32>, tensor<192x3x3x192xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x192xf32>, tensor<192x3x3x192xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x17x17x192xf32>, %3: tensor<192x3x3x192xf32>, %7: tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x192xf32>, tensor<192x3x3x192xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n  return %ret : tensor<128x8x8x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x17x17x192xf32>, %arg1: tensor<192x3x3x192xf32>, %arg2: tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x3x3x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x17x17x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x8x8x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x8x8x192xf32>\n    memref.copy %2, %alloc : memref<128x8x8x192xf32> to memref<128x8x8x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x17x17x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x3x3x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x8x8x192xf32>\n    return %3 : tensor<128x8x8x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x8x8x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x17x17x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x3x3x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x3x3x192xf32>) -> tensor<192x3x3x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x8x8x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x192xf32>, tensor<192x3x3x192xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x8x8x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x8x8x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10255117068}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x224xf32>, %3: tensor<896x1x1x224xf32>, %7: tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>\n  return %ret : tensor<256x1x1x896xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x224xf32>, %arg1: tensor<896x1x1x224xf32>, %arg2: tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<896x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x896xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x896xf32>\n    memref.copy %2, %alloc : memref<256x1x1x896xf32> to memref<256x1x1x896xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 896 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<896x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x896xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x896xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x896xf32>\n    return %3 : tensor<256x1x1x896xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x896xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<896x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<896x1x1x224xf32>) -> tensor<896x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x896xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x896xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x896xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 896, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 182627869}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x440xf32>, %3: tensor<440x1x1x440xf32>, %7: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n  return %ret : tensor<512x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x440xf32>, %arg1: tensor<440x1x1x440xf32>, %arg2: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x440xf32>\n    memref.copy %2, %alloc : memref<512x7x7x440xf32> to memref<512x7x7x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x440xf32>\n    return %3 : tensor<512x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x440xf32>) -> tensor<440x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18014438874}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x57x57x144xf32>, %3: tensor<3x3x144x1xf32>, %7: tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32>\n  return %ret : tensor<256x28x28x144x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x57x57x144xf32>, %arg1: tensor<3x3x144x1xf32>, %arg2: tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x144x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x57x57x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x144x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x144x1xf32>\n    memref.copy %2, %alloc : memref<256x28x28x144x1xf32> to memref<256x28x28x144x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x57x57x144xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x144x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x144x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x144x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x144x1xf32>\n    return %3 : tensor<256x28x28x144x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x144x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x57x57x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x57x57x144xf32>) -> tensor<256x57x57x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x144x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x144x1xf32>) -> tensor<3x3x144x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x144x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<256x28x28x144x1xf32>) -> tensor<256x28x28x144x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x144x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x144x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 671870470}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x112xf32>, %3: tensor<256x1x1x112xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x112xf32>, %arg1: tensor<256x1x1x112xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x112xf32>) -> tensor<512x28x28x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10112721060}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<1088x1x1x1088xf32>) outs(%7 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<1088x1x1x1088xf32>) outs(%7 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1088xf32>, %3: tensor<1088x1x1x1088xf32>, %7: tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<1088x1x1x1088xf32>) outs(%7 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>\n  return %ret : tensor<128x7x7x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1088xf32>, %arg1: tensor<1088x1x1x1088xf32>, %arg2: tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1088x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x1088xf32>\n    memref.copy %2, %alloc : memref<128x7x7x1088xf32> to memref<128x7x7x1088xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1088x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x1088xf32>\n    return %3 : tensor<128x7x7x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1088x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1088x1x1x1088xf32>) -> tensor<1088x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<1088x1x1x1088xf32>) outs(%7 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27853350558}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x85x85x42xf32>, %3: tensor<3x3x42x1xf32>, %7: tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32>\n  return %ret : tensor<256x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x85x85x42xf32>, %arg1: tensor<3x3x42x1xf32>, %arg2: tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x85x85x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<256x83x83x42x1xf32> to memref<256x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x85x85x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x83x83x42x1xf32>\n    return %3 : tensor<256x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x85x85x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x85x85x42xf32>) -> tensor<256x85x85x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x42x1xf32>) -> tensor<3x3x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<256x83x83x42x1xf32>) -> tensor<256x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1657596359}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x8xf32>, %3: tensor<64x1x1x8xf32>, %7: tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n  return %ret : tensor<256x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x8xf32>, %arg1: tensor<64x1x1x8xf32>, %arg2: tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x64xf32>\n    memref.copy %2, %alloc : memref<256x1x1x64xf32> to memref<256x1x1x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x64xf32>\n    return %3 : tensor<256x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x8xf32>) -> tensor<64x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 212296}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<80x1x1x32xf32>, %7: tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32>\n  return %ret : tensor<128x112x112x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<80x1x1x32xf32>, %arg2: tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x112x112x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x112x112x80xf32>\n    memref.copy %2, %alloc : memref<128x112x112x80xf32> to memref<128x112x112x80xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x112x112x80xf32>\n    return %3 : tensor<128x112x112x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x112x112x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x32xf32>) -> tensor<80x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x112x112x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<128x112x112x80xf32>) -> tensor<128x112x112x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x112x112x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x112x112x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11579615302}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x58xf32>, %3: tensor<232x1x1x58xf32>, %7: tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n  return %ret : tensor<256x1x1x232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x58xf32>, %arg1: tensor<232x1x1x58xf32>, %arg2: tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<232x1x1x58xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x232xf32>\n    memref.copy %2, %alloc : memref<256x1x1x232xf32> to memref<256x1x1x232xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x58xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<232x1x1x58xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x232xf32>\n    return %3 : tensor<256x1x1x232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x58xf32>) -> tensor<256x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<232x1x1x58xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<232x1x1x58xf32>) -> tensor<232x1x1x58xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10081690}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x1512xf32>, %3: tensor<144x1x1x1512xf32>, %7: tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n  return %ret : tensor<256x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x1512xf32>, %arg1: tensor<144x1x1x1512xf32>, %arg2: tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x1512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x1512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x144xf32>\n    memref.copy %2, %alloc : memref<256x1x1x144xf32> to memref<256x1x1x144xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x1512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x1512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x144xf32>\n    return %3 : tensor<256x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x1512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x1512xf32>) -> tensor<256x1x1x1512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x1512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x1512xf32>) -> tensor<144x1x1x1512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 208958458}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x26xf32>, %3: tensor<208x1x1x26xf32>, %7: tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n  return %ret : tensor<128x1x1x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x26xf32>, %arg1: tensor<208x1x1x26xf32>, %arg2: tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x208xf32>\n    memref.copy %2, %alloc : memref<128x1x1x208xf32> to memref<128x1x1x208xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x26xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x208xf32>\n    return %3 : tensor<128x1x1x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x26xf32>) -> tensor<128x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x26xf32>) -> tensor<208x1x1x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1587356}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<120x1x1x48xf32>, %7: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n  return %ret : tensor<512x28x28x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<120x1x1x48xf32>, %arg2: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x120xf32>\n    memref.copy %2, %alloc : memref<512x28x28x120xf32> to memref<512x28x28x120xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x120xf32>\n    return %3 : tensor<512x28x28x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x48xf32>) -> tensor<120x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7229237166}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x89x89x42xf32>, %3: tensor<7x7x42x1xf32>, %7: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n  return %ret : tensor<128x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x89x89x42xf32>, %arg1: tensor<7x7x42x1xf32>, %arg2: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x89x89x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<128x83x83x42x1xf32> to memref<128x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x89x89x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x83x83x42x1xf32>\n    return %3 : tensor<128x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x89x89x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x89x89x42xf32>) -> tensor<128x89x89x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x42x1xf32>) -> tensor<7x7x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5848467427}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x64xf32>, %3: tensor<256x1x1x64xf32>, %7: tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>\n  return %ret : tensor<256x1x1x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x64xf32>, %arg1: tensor<256x1x1x64xf32>, %arg2: tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x256xf32>\n    memref.copy %2, %alloc : memref<256x1x1x256xf32> to memref<256x1x1x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x256xf32>\n    return %3 : tensor<256x1x1x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12609364}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x2x2x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x2x2x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x96xf32>, %3: tensor<192x2x2x96xf32>, %7: tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x2x2x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n  return %ret : tensor<128x28x28x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x96xf32>, %arg1: tensor<192x2x2x96xf32>, %arg2: tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x2x2x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x192xf32>\n    memref.copy %2, %alloc : memref<128x28x28x192xf32> to memref<128x28x28x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x2x2x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x192xf32>\n    return %3 : tensor<128x28x28x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x96xf32>) -> tensor<128x56x56x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x2x2x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x2x2x96xf32>) -> tensor<192x2x2x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x2x2x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27508945699}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x2048xf32>, tensor<320x1x1x2048xf32>) outs(%7 : tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x2048xf32>, tensor<320x1x1x2048xf32>) outs(%7 : tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x8x8x2048xf32>, %3: tensor<320x1x1x2048xf32>, %7: tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x2048xf32>, tensor<320x1x1x2048xf32>) outs(%7 : tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32>\n  return %ret : tensor<128x8x8x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x8x8x2048xf32>, %arg1: tensor<320x1x1x2048xf32>, %arg2: tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x2048xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x8x8x2048xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x8x8x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x8x8x320xf32>\n    memref.copy %2, %alloc : memref<128x8x8x320xf32> to memref<128x8x8x320xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 2048 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x8x8x2048xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x2048xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x8x8x320xf32>\n    return %3 : tensor<128x8x8x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x8x8x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x8x8x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x8x8x2048xf32>) -> tensor<128x8x8x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x2048xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x2048xf32>) -> tensor<320x1x1x2048xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x8x8x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x2048xf32>, tensor<320x1x1x2048xf32>) outs(%7 : tensor<128x8x8x320xf32>) -> tensor<128x8x8x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x8x8x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x8x8x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 2048, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20230392669}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x104xf32>, %3: tensor<26x1x1x104xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x104xf32>, %arg1: tensor<26x1x1x104xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<26x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<26x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<26x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<26x1x1x104xf32>) -> tensor<26x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4696030}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x18xf32>, %3: tensor<216x1x1x18xf32>, %7: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n  return %ret : tensor<512x1x1x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x18xf32>, %arg1: tensor<216x1x1x18xf32>, %arg2: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<216x1x1x18xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x216xf32>\n    memref.copy %2, %alloc : memref<512x1x1x216xf32> to memref<512x1x1x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x18xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<216x1x1x18xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x216xf32>\n    return %3 : tensor<512x1x1x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<216x1x1x18xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<216x1x1x18xf32>) -> tensor<216x1x1x18xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4300814}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1184xf32>, %3: tensor<128x1x1x1184xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1184xf32>, %arg1: tensor<128x1x1x1184xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1184xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1184xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1184 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1184xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1184xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1184xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1184xf32>) -> tensor<128x7x7x1184xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1184xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1184xf32>) -> tensor<128x1x1x1184xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1184, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3568862376}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x128xf32>, %3: tensor<128x1x1x128xf32>, %7: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n  return %ret : tensor<512x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x128xf32>, %arg1: tensor<128x1x1x128xf32>, %arg2: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x128xf32>\n    memref.copy %2, %alloc : memref<512x28x28x128xf32> to memref<512x28x28x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x128xf32>\n    return %3 : tensor<512x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23334273981}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x896xf32>, %3: tensor<128x1x1x896xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x896xf32>, %arg1: tensor<128x1x1x896xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x896xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x896xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 896 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x896xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x896xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x896xf32>) -> tensor<512x7x7x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x896xf32>) -> tensor<128x1x1x896xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10778337017}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<24x1x1x32xf32>, %7: tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32>\n  return %ret : tensor<256x112x112x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<24x1x1x32xf32>, %arg2: tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x112x112x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x112x112x24xf32>\n    memref.copy %2, %alloc : memref<256x112x112x24xf32> to memref<256x112x112x24xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x112x112x24xf32>\n    return %3 : tensor<256x112x112x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x112x112x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x32xf32>) -> tensor<24x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x112x112x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x112x112x24xf32>) -> tensor<256x112x112x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x112x112x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x112x112x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6824908707}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x61x61x22xf32>, %3: tensor<7x7x22x1xf32>, %7: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n  return %ret : tensor<512x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x61x61x22xf32>, %arg1: tensor<7x7x22x1xf32>, %arg2: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x61x61x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x22x1xf32> to memref<512x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x61x61x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x22x1xf32>\n    return %3 : tensor<512x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x61x61x22xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x61x61x22xf32>) -> tensor<512x61x61x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x22x1xf32>) -> tensor<7x7x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1439994397}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x88xf32>, %3: tensor<88x1x1x88xf32>, %7: tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>\n  return %ret : tensor<128x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x88xf32>, %arg1: tensor<88x1x1x88xf32>, %arg2: tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x88xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x88xf32>\n    memref.copy %2, %alloc : memref<128x14x14x88xf32> to memref<128x14x14x88xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 88 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x88xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x88xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x88xf32>\n    return %3 : tensor<128x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x88xf32>) -> tensor<88x1x1x88xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<128x14x14x88xf32>) -> tensor<128x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 88, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 665140948}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x256xf32>, %3: tensor<256x1x1x256xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x256xf32>, %arg1: tensor<256x1x1x256xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24091036802}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x1088xf32>, %3: tensor<128x1x1x1088xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x1088xf32>, %arg1: tensor<128x1x1x1088xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x1088xf32>) -> tensor<256x14x14x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1088xf32>) -> tensor<128x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26219243879}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x72xf32>, %3: tensor<72x1x1x72xf32>, %7: tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>\n  return %ret : tensor<128x56x56x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x72xf32>, %arg1: tensor<72x1x1x72xf32>, %arg2: tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x72xf32>\n    memref.copy %2, %alloc : memref<128x56x56x72xf32> to memref<128x56x56x72xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x72xf32>\n    return %3 : tensor<128x56x56x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x72xf32>) -> tensor<72x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<128x56x56x72xf32>) -> tensor<128x56x56x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7050958190}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x30x30x22xf32>, %3: tensor<3x3x22x1xf32>, %7: tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n  return %ret : tensor<256x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x30x30x22xf32>, %arg1: tensor<3x3x22x1xf32>, %arg2: tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x30x30x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<256x28x28x22x1xf32> to memref<256x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x30x30x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x22x1xf32>\n    return %3 : tensor<256x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x30x30x22xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x30x30x22xf32>) -> tensor<256x30x30x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x22x1xf32>) -> tensor<3x3x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<256x28x28x22x1xf32>) -> tensor<256x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 105282224}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1056xf32>, %3: tensor<176x1x1x1056xf32>, %7: tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32>\n  return %ret : tensor<128x7x7x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1056xf32>, %arg1: tensor<176x1x1x1056xf32>, %arg2: tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<176x1x1x1056xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x176xf32>\n    memref.copy %2, %alloc : memref<128x7x7x176xf32> to memref<128x7x7x176xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1056 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1056xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<176x1x1x1056xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x176xf32>\n    return %3 : tensor<128x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1056xf32>) -> tensor<128x7x7x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<176x1x1x1056xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<176x1x1x1056xf32>) -> tensor<176x1x1x1056xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<128x7x7x176xf32>) -> tensor<128x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4377836864}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1696xf32>, %3: tensor<128x1x1x1696xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1696xf32>, %arg1: tensor<128x1x1x1696xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1696xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1696xf32>) -> tensor<128x14x14x1696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1696xf32>) -> tensor<128x1x1x1696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20488036299}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x1392xf32>, %3: tensor<174x1x1x1392xf32>, %7: tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>\n  return %ret : tensor<256x1x1x174xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x1392xf32>, %arg1: tensor<174x1x1x1392xf32>, %arg2: tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<174x1x1x1392xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x1392xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x174xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x174xf32>\n    memref.copy %2, %alloc : memref<256x1x1x174xf32> to memref<256x1x1x174xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 174 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1392 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x1392xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<174x1x1x1392xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x174xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x174xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x174xf32>\n    return %3 : tensor<256x1x1x174xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x174xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x1392xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<174x1x1x1392xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<174x1x1x1392xf32>) -> tensor<174x1x1x1392xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x174xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x174xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x174xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 174, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1392, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 232073082}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x768xf32>, tensor<160x1x1x768xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x768xf32>, tensor<160x1x1x768xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x17x17x768xf32>, %3: tensor<160x1x1x768xf32>, %7: tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x768xf32>, tensor<160x1x1x768xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n  return %ret : tensor<128x17x17x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x17x17x768xf32>, %arg1: tensor<160x1x1x768xf32>, %arg2: tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x17x17x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x17x17x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x17x17x160xf32>\n    memref.copy %2, %alloc : memref<128x17x17x160xf32> to memref<128x17x17x160xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x17x17x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x17x17x160xf32>\n    return %3 : tensor<128x17x17x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x17x17x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x17x17x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x17x17x768xf32>) -> tensor<128x17x17x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x768xf32>) -> tensor<160x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x17x17x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x17x17x768xf32>, tensor<160x1x1x768xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x17x17x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x17x17x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17014917832}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<64x1x1x32xf32>, %7: tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32>\n  return %ret : tensor<256x112x112x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<64x1x1x32xf32>, %arg2: tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x112x112x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x112x112x64xf32>\n    memref.copy %2, %alloc : memref<256x112x112x64xf32> to memref<256x112x112x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x112x112x64xf32>\n    return %3 : tensor<256x112x112x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x112x112x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x32xf32>) -> tensor<64x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x112x112x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x112x112x64xf32>) -> tensor<256x112x112x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x112x112x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x112x112x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18128455315}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1472xf32>, %3: tensor<128x1x1x1472xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1472xf32>, %arg1: tensor<128x1x1x1472xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1472xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1472xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1472 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1472xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1472xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1472xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1472xf32>) -> tensor<256x7x7x1472xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1472xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1472xf32>) -> tensor<128x1x1x1472xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1472, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8883931572}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x264xf32>, %3: tensor<44x1x1x264xf32>, %7: tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32>\n  return %ret : tensor<256x14x14x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x264xf32>, %arg1: tensor<44x1x1x264xf32>, %arg2: tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x264xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x264xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x44xf32>\n    memref.copy %2, %alloc : memref<256x14x14x44xf32> to memref<256x14x14x44xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 264 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x264xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x264xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x44xf32>\n    return %3 : tensor<256x14x14x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x264xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x264xf32>) -> tensor<256x14x14x264xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x264xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x264xf32>) -> tensor<44x1x1x264xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<256x14x14x44xf32>) -> tensor<256x14x14x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 264, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2134108148}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1408xf32>, %3: tensor<128x1x1x1408xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1408xf32>, %arg1: tensor<128x1x1x1408xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1408xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1408xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1408 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1408xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1408xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1408xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1408xf32>) -> tensor<128x7x7x1408xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1408xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1408xf32>) -> tensor<128x1x1x1408xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1408, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4247595111}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x48x48x84xf32>, %3: tensor<7x7x84x1xf32>, %7: tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n  return %ret : tensor<256x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x48x48x84xf32>, %arg1: tensor<7x7x84x1xf32>, %arg2: tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x48x48x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<256x42x42x84x1xf32> to memref<256x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x48x48x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x42x42x84x1xf32>\n    return %3 : tensor<256x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x48x48x84xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x48x48x84xf32>) -> tensor<256x48x48x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x84x1xf32>) -> tensor<7x7x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 6018455708}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1536xf32>, %3: tensor<128x1x1x1536xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1536xf32>, %arg1: tensor<128x1x1x1536xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1536xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1536xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1536 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1536xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1536xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1536xf32>) -> tensor<256x7x7x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1536xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1536xf32>) -> tensor<128x1x1x1536xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9272753453}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x16x16x88xf32>, %3: tensor<3x3x88x1xf32>, %7: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n  return %ret : tensor<128x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x16x16x88xf32>, %arg1: tensor<3x3x88x1xf32>, %arg2: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x16x16x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<128x14x14x88x1xf32> to memref<128x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x16x16x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x88x1xf32>\n    return %3 : tensor<128x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x16x16x88xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x16x16x88xf32>) -> tensor<128x16x16x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x88x1xf32>) -> tensor<3x3x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 48824670}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x832xf32>, %3: tensor<128x1x1x832xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x832xf32>, %arg1: tensor<128x1x1x832xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x832xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x832xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 832 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x832xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x832xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x832xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x832xf32>) -> tensor<256x7x7x832xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x832xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x832xf32>) -> tensor<128x1x1x832xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 832, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5000255242}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x18xf32>, %3: tensor<72x1x1x18xf32>, %7: tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>\n  return %ret : tensor<256x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x18xf32>, %arg1: tensor<72x1x1x18xf32>, %arg2: tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x18xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x72xf32>\n    memref.copy %2, %alloc : memref<256x1x1x72xf32> to memref<256x1x1x72xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x18xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x18xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x72xf32>\n    return %3 : tensor<256x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x18xf32>) -> tensor<256x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x18xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x18xf32>) -> tensor<72x1x1x18xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 709926}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x56xf32>, %3: tensor<6x1x1x56xf32>, %7: tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32>\n  return %ret : tensor<128x1x1x6xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x56xf32>, %arg1: tensor<6x1x1x56xf32>, %arg2: tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<6x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x6xf32>\n    memref.copy %2, %alloc : memref<128x1x1x6xf32> to memref<128x1x1x6xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 6 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<6x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x6xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x6xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x6xf32>\n    return %3 : tensor<128x1x1x6xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x6xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x56xf32>) -> tensor<128x1x1x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<6x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<6x1x1x56xf32>) -> tensor<6x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x6xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<128x1x1x6xf32>) -> tensor<128x1x1x6xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x6xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x6xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 6, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 129470}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1664xf32>, %3: tensor<128x1x1x1664xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1664xf32>, %arg1: tensor<128x1x1x1664xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1664xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1664xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1664 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1664xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1664xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1664xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1664xf32>) -> tensor<512x7x7x1664xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1664xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1664xf32>) -> tensor<128x1x1x1664xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1664, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20102621840}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x8x10x192xf32>, %3: tensor<224x1x3x192xf32>, %7: tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32>\n  return %ret : tensor<128x8x8x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x8x10x192xf32>, %arg1: tensor<224x1x3x192xf32>, %arg2: tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x3x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x8x10x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x8x8x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x8x8x224xf32>\n    memref.copy %2, %alloc : memref<128x8x8x224xf32> to memref<128x8x8x224xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x8x10x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x3x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x8x8x224xf32>\n    return %3 : tensor<128x8x8x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x8x8x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x8x10x192xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x8x10x192xf32>) -> tensor<128x8x10x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x3x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x3x192xf32>) -> tensor<224x1x3x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x8x8x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<128x8x8x224xf32>) -> tensor<128x8x8x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x8x8x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x8x8x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3945591869}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x15x15x672xf32>, %3: tensor<5x5x672x1xf32>, %7: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n  return %ret : tensor<256x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x15x15x672xf32>, %arg1: tensor<5x5x672x1xf32>, %arg2: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x15x15x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<256x11x11x672x1xf32> to memref<256x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x15x15x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x11x11x672x1xf32>\n    return %3 : tensor<256x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x15x15x672xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x15x15x672xf32>) -> tensor<256x15x15x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x672x1xf32>) -> tensor<5x5x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1544030939}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x408xf32>, tensor<912x1x1x408xf32>) outs(%7 : tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x408xf32>, tensor<912x1x1x408xf32>) outs(%7 : tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x408xf32>, %3: tensor<912x1x1x408xf32>, %7: tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x408xf32>, tensor<912x1x1x408xf32>) outs(%7 : tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32>\n  return %ret : tensor<256x7x7x912xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x408xf32>, %arg1: tensor<912x1x1x408xf32>, %arg2: tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<912x1x1x408xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x408xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x912xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x912xf32>\n    memref.copy %2, %alloc : memref<256x7x7x912xf32> to memref<256x7x7x912xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 912 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 408 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x408xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<912x1x1x408xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x912xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x912xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x912xf32>\n    return %3 : tensor<256x7x7x912xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x912xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x408xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x408xf32>) -> tensor<256x14x14x408xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<912x1x1x408xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<912x1x1x408xf32>) -> tensor<912x1x1x408xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x912xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x408xf32>, tensor<912x1x1x408xf32>) outs(%7 : tensor<256x7x7x912xf32>) -> tensor<256x7x7x912xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x912xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x912xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 912, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 408, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17304458110}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x192xf32>, %3: tensor<48x1x1x192xf32>, %7: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n  return %ret : tensor<128x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x192xf32>, %arg1: tensor<48x1x1x192xf32>, %arg2: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x48xf32>\n    memref.copy %2, %alloc : memref<128x1x1x48xf32> to memref<128x1x1x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x48xf32>\n    return %3 : tensor<128x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x192xf32>) -> tensor<48x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4162751}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<92x1x1x368xf32>, %7: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n  return %ret : tensor<512x1x1x92xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<92x1x1x368xf32>, %arg2: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<92x1x1x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x92xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x92xf32>\n    memref.copy %2, %alloc : memref<512x1x1x92xf32> to memref<512x1x1x92xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 92 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<92x1x1x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x92xf32>\n    return %3 : tensor<512x1x1x92xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x92xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<92x1x1x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<92x1x1x368xf32>) -> tensor<92x1x1x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x92xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x92xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x92xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 92, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 63189034}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x36xf32>, %3: tensor<144x1x1x36xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x36xf32>, %arg1: tensor<144x1x1x36xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x36xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x36xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 36 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x36xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x36xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x36xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x36xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x36xf32>) -> tensor<144x1x1x36xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 36, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6727714}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x96xf32>, %3: tensor<240x1x1x96xf32>, %7: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n  return %ret : tensor<512x14x14x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x96xf32>, %arg1: tensor<240x1x1x96xf32>, %arg2: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x240xf32>\n    memref.copy %2, %alloc : memref<512x14x14x240xf32> to memref<512x14x14x240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<240x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x240xf32>\n    return %3 : tensor<512x14x14x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<240x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<240x1x1x96xf32>) -> tensor<240x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8008476620}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x352xf32>, %3: tensor<128x1x1x352xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x352xf32>, %arg1: tensor<128x1x1x352xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x352xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x352xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 352 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x352xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x352xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x352xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x352xf32>) -> tensor<128x14x14x352xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x352xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x352xf32>) -> tensor<128x1x1x352xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 352, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4177072951}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x128xf32>, %3: tensor<32x1x1x128xf32>, %7: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n  return %ret : tensor<256x1x1x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x128xf32>, %arg1: tensor<32x1x1x128xf32>, %arg2: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x32xf32>\n    memref.copy %2, %alloc : memref<256x1x1x32xf32> to memref<256x1x1x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x32xf32>\n    return %3 : tensor<256x1x1x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x128xf32>) -> tensor<256x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x128xf32>) -> tensor<32x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3749440}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x120xf32>, %3: tensor<336x1x1x120xf32>, %7: tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>\n  return %ret : tensor<256x14x14x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x120xf32>, %arg1: tensor<336x1x1x120xf32>, %arg2: tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x336xf32>\n    memref.copy %2, %alloc : memref<256x14x14x336xf32> to memref<256x14x14x336xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x336xf32>\n    return %3 : tensor<256x14x14x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x120xf32>) -> tensor<256x28x28x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x120xf32>) -> tensor<336x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7142969345}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x80xf32>, tensor<80x1x1x80xf32>) outs(%7 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x80xf32>, tensor<80x1x1x80xf32>) outs(%7 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x80xf32>, %3: tensor<80x1x1x80xf32>, %7: tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x80xf32>, tensor<80x1x1x80xf32>) outs(%7 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>\n  return %ret : tensor<256x56x56x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x80xf32>, %arg1: tensor<80x1x1x80xf32>, %arg2: tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x80xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x80xf32>\n    memref.copy %2, %alloc : memref<256x56x56x80xf32> to memref<256x56x56x80xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 80 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x80xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x80xf32>\n    return %3 : tensor<256x56x56x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x80xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x80xf32>) -> tensor<80x1x1x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x80xf32>, tensor<80x1x1x80xf32>) outs(%7 : tensor<256x56x56x80xf32>) -> tensor<256x56x56x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 80, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17701974929}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x168xf32>, tensor<392x1x1x168xf32>) outs(%7 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x168xf32>, tensor<392x1x1x168xf32>) outs(%7 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x168xf32>, %3: tensor<392x1x1x168xf32>, %7: tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x168xf32>, tensor<392x1x1x168xf32>) outs(%7 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>\n  return %ret : tensor<128x28x28x392xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x168xf32>, %arg1: tensor<392x1x1x168xf32>, %arg2: tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<392x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x392xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x392xf32>\n    memref.copy %2, %alloc : memref<128x28x28x392xf32> to memref<128x28x28x392xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 392 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<392x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x392xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x392xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x392xf32>\n    return %3 : tensor<128x28x28x392xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x392xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x168xf32>) -> tensor<128x56x56x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<392x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<392x1x1x168xf32>) -> tensor<392x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x392xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x168xf32>, tensor<392x1x1x168xf32>) outs(%7 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x392xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x392xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 392, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23798714079}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1008xf32>, tensor<1008x1x1x1008xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1008xf32>, tensor<1008x1x1x1008xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1008xf32>, %3: tensor<1008x1x1x1008xf32>, %7: tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1008xf32>, tensor<1008x1x1x1008xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n  return %ret : tensor<128x7x7x1008xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1008xf32>, %arg1: tensor<1008x1x1x1008xf32>, %arg2: tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1008x1x1x1008xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1008xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x1008xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x1008xf32>\n    memref.copy %2, %alloc : memref<128x7x7x1008xf32> to memref<128x7x7x1008xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1008 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1008 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1008xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1008x1x1x1008xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1008xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1008xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x1008xf32>\n    return %3 : tensor<128x7x7x1008xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x1008xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1008xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1008x1x1x1008xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1008x1x1x1008xf32>) -> tensor<1008x1x1x1008xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x1008xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1008xf32>, tensor<1008x1x1x1008xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x1008xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x1008xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1008, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1008, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23892294936}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x256xf32>, %3: tensor<608x1x1x256xf32>, %7: tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32>\n  return %ret : tensor<128x14x14x608xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x256xf32>, %arg1: tensor<608x1x1x256xf32>, %arg2: tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<608x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x608xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x608xf32>\n    memref.copy %2, %alloc : memref<128x14x14x608xf32> to memref<128x14x14x608xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 608 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<608x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x608xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x608xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x608xf32>\n    return %3 : tensor<128x14x14x608xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x608xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<608x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<608x1x1x256xf32>) -> tensor<608x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x608xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x14x14x608xf32>) -> tensor<128x14x14x608xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x608xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x608xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 608, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14289990563}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x9x9x960xf32>, %3: tensor<3x3x960x1xf32>, %7: tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n  return %ret : tensor<512x7x7x960x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x9x9x960xf32>, %arg1: tensor<3x3x960x1xf32>, %arg2: tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x960x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x9x9x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x960x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x960x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x960x1xf32> to memref<512x7x7x960x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 960 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x9x9x960xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x960x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x960x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x960x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x960x1xf32>\n    return %3 : tensor<512x7x7x960x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x960x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x9x9x960xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x9x9x960xf32>) -> tensor<512x9x9x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x960x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x960x1xf32>) -> tensor<3x3x960x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x960x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x960x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x960x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 960, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 549900585}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x168xf32>, %3: tensor<408x1x1x168xf32>, %7: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n  return %ret : tensor<512x14x14x408xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x168xf32>, %arg1: tensor<408x1x1x168xf32>, %arg2: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<408x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x408xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x408xf32>\n    memref.copy %2, %alloc : memref<512x14x14x408xf32> to memref<512x14x14x408xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 408 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<408x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x408xf32>\n    return %3 : tensor<512x14x14x408xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x408xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x168xf32>) -> tensor<512x28x28x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<408x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<408x1x1x168xf32>) -> tensor<408x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x408xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x408xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x408xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 408, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24768974665}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x149x149x64xf32>, %3: tensor<3x3x64x1xf32>, %7: tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32>\n  return %ret : tensor<128x147x147x64x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x149x149x64xf32>, %arg1: tensor<3x3x64x1xf32>, %arg2: tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x64x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x149x149x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x147x147x64x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x147x147x64x1xf32>\n    memref.copy %2, %alloc : memref<128x147x147x64x1xf32> to memref<128x147x147x64x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 147 {\n        affine.for %arg5 = 0 to 147 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x149x149x64xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x64x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x147x147x64x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x147x147x64x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x147x147x64x1xf32>\n    return %3 : tensor<128x147x147x64x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x147x147x64x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x149x149x64xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x149x149x64xf32>) -> tensor<128x149x149x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x64x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x64x1xf32>) -> tensor<3x3x64x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x147x147x64x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<128x147x147x64x1xf32>) -> tensor<128x147x147x64x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x147x147x64x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x147x147x64x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 147, 1], ["%arg5", 0, 147, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 4041951915}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x144xf32>, %3: tensor<1296x1x1x144xf32>, %7: tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32>\n  return %ret : tensor<128x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x144xf32>, %arg1: tensor<1296x1x1x144xf32>, %arg2: tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1296x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<128x1x1x1296xf32> to memref<128x1x1x1296xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1296x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x1296xf32>\n    return %3 : tensor<128x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x144xf32>) -> tensor<128x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1296x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1296x1x1x144xf32>) -> tensor<1296x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<128x1x1x1296xf32>) -> tensor<128x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 81965001}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x128xf32>, %3: tensor<288x1x1x128xf32>, %7: tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32>\n  return %ret : tensor<256x28x28x288xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x128xf32>, %arg1: tensor<288x1x1x128xf32>, %arg2: tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x288xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x288xf32>\n    memref.copy %2, %alloc : memref<256x28x28x288xf32> to memref<256x28x28x288xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 288 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<288x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x288xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x288xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x288xf32>\n    return %3 : tensor<256x28x28x288xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x288xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<288x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<288x1x1x128xf32>) -> tensor<288x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x288xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<256x28x28x288xf32>) -> tensor<256x28x28x288xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x288xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x288xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 288, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26351901757}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x128xf32>, %3: tensor<1088x1x1x128xf32>, %7: tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n  return %ret : tensor<256x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x128xf32>, %arg1: tensor<1088x1x1x128xf32>, %arg2: tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1088x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<256x1x1x1088xf32> to memref<256x1x1x1088xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1088x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x1088xf32>\n    return %3 : tensor<256x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x128xf32>) -> tensor<256x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1088x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1088x1x1x128xf32>) -> tensor<1088x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 120790402}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1120xf32>, %3: tensor<128x1x1x1120xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1120xf32>, %arg1: tensor<128x1x1x1120xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1120xf32>) -> tensor<512x7x7x1120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1120xf32>) -> tensor<128x1x1x1120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13500613329}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1504xf32>, %3: tensor<128x1x1x1504xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1504xf32>, %arg1: tensor<128x1x1x1504xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1504xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1504xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1504 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1504xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1504xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1504xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1504xf32>) -> tensor<128x14x14x1504xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1504xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1504xf32>) -> tensor<128x1x1x1504xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1504, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18160899316}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x320xf32>, %3: tensor<80x1x1x320xf32>, %7: tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n  return %ret : tensor<256x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x320xf32>, %arg1: tensor<80x1x1x320xf32>, %arg2: tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x80xf32>\n    memref.copy %2, %alloc : memref<256x1x1x80xf32> to memref<256x1x1x80xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x80xf32>\n    return %3 : tensor<256x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x320xf32>) -> tensor<80x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23808611}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x58xf32>, %3: tensor<696x1x1x58xf32>, %7: tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>\n  return %ret : tensor<128x1x1x696xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x58xf32>, %arg1: tensor<696x1x1x58xf32>, %arg2: tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<696x1x1x58xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x696xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x696xf32>\n    memref.copy %2, %alloc : memref<128x1x1x696xf32> to memref<128x1x1x696xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 696 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x58xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<696x1x1x58xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x696xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x696xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x696xf32>\n    return %3 : tensor<128x1x1x696xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x696xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<696x1x1x58xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<696x1x1x58xf32>) -> tensor<696x1x1x58xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x696xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x696xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x696xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 696, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15383203}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<144x1x1x8xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<144x1x1x8xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x8xf32>) -> tensor<144x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 982228}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x48xf32>, %3: tensor<192x1x1x48xf32>, %7: tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n  return %ret : tensor<256x1x1x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x48xf32>, %arg1: tensor<192x1x1x48xf32>, %arg2: tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x192xf32>\n    memref.copy %2, %alloc : memref<256x1x1x192xf32> to memref<256x1x1x192xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x192xf32>\n    return %3 : tensor<256x1x1x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x48xf32>) -> tensor<192x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6628493}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x112xf32>, %3: tensor<28x1x1x112xf32>, %7: tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>\n  return %ret : tensor<128x1x1x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x112xf32>, %arg1: tensor<28x1x1x112xf32>, %arg2: tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<28x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x28xf32>\n    memref.copy %2, %alloc : memref<128x1x1x28xf32> to memref<128x1x1x28xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<28x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x28xf32>\n    return %3 : tensor<128x1x1x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<28x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<28x1x1x112xf32>) -> tensor<28x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x28xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1560326}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x13x13x672xf32>, %3: tensor<3x3x672x1xf32>, %7: tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32>\n  return %ret : tensor<128x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x13x13x672xf32>, %arg1: tensor<3x3x672x1xf32>, %arg2: tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x13x13x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<128x11x11x672x1xf32> to memref<128x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x13x13x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x11x11x672x1xf32>\n    return %3 : tensor<128x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x13x13x672xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x13x13x672xf32>) -> tensor<128x13x13x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x672x1xf32>) -> tensor<3x3x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<128x11x11x672x1xf32>) -> tensor<128x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 234762774}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x272xf32>, %3: tensor<1088x1x1x272xf32>, %7: tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n  return %ret : tensor<256x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x272xf32>, %arg1: tensor<1088x1x1x272xf32>, %arg2: tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1088x1x1x272xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x272xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<256x1x1x1088xf32> to memref<256x1x1x1088xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 272 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x272xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1088x1x1x272xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x1088xf32>\n    return %3 : tensor<256x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x272xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x272xf32>) -> tensor<256x1x1x272xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1088x1x1x272xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1088x1x1x272xf32>) -> tensor<1088x1x1x272xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<256x1x1x1088xf32>) -> tensor<256x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 272, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 272306884}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x512xf32>, %3: tensor<128x1x1x512xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x512xf32>, %arg1: tensor<128x1x1x512xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6122494496}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x240xf32>, %3: tensor<528x1x1x240xf32>, %7: tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n  return %ret : tensor<256x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x240xf32>, %arg1: tensor<528x1x1x240xf32>, %arg2: tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<528x1x1x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x528xf32>\n    memref.copy %2, %alloc : memref<256x7x7x528xf32> to memref<256x7x7x528xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<528x1x1x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x528xf32>\n    return %3 : tensor<256x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x240xf32>) -> tensor<256x14x14x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<528x1x1x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<528x1x1x240xf32>) -> tensor<528x1x1x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5821014377}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x39x39x256xf32>, %3: tensor<3x3x256x1xf32>, %7: tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n  return %ret : tensor<512x37x37x256x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x39x39x256xf32>, %arg1: tensor<3x3x256x1xf32>, %arg2: tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x256x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x39x39x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x37x37x256x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x37x37x256x1xf32>\n    memref.copy %2, %alloc : memref<512x37x37x256x1xf32> to memref<512x37x37x256x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 37 {\n        affine.for %arg5 = 0 to 37 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x39x39x256xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x256x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x256x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x256x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x37x37x256x1xf32>\n    return %3 : tensor<512x37x37x256x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x37x37x256x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x39x39x256xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x39x39x256xf32>) -> tensor<512x39x39x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x256x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x256x1xf32>) -> tensor<3x3x256x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x37x37x256x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x37x37x256x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x37x37x256x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 37, 1], ["%arg5", 0, 37, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 4472170407}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x38xf32>, %3: tensor<368x1x1x38xf32>, %7: tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32>\n  return %ret : tensor<256x1x1x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x38xf32>, %arg1: tensor<368x1x1x38xf32>, %arg2: tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x38xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x38xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x368xf32>\n    memref.copy %2, %alloc : memref<256x1x1x368xf32> to memref<256x1x1x368xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 38 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x38xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x38xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x368xf32>\n    return %3 : tensor<256x1x1x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x38xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x38xf32>) -> tensor<256x1x1x38xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x38xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x38xf32>) -> tensor<368x1x1x38xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<256x1x1x368xf32>) -> tensor<256x1x1x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 38, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8908350}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1760xf32>, %3: tensor<128x1x1x1760xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1760xf32>, %arg1: tensor<128x1x1x1760xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1760xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1760xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1760 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1760xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1760xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1760xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1760xf32>) -> tensor<512x7x7x1760xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1760xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1760xf32>) -> tensor<128x1x1x1760xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1760, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21273202481}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<128x1x1x32xf32>, %7: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n  return %ret : tensor<512x56x56x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<128x1x1x32xf32>, %arg2: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x128xf32>\n    memref.copy %2, %alloc : memref<512x56x56x128xf32> to memref<512x56x56x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x128xf32>\n    return %3 : tensor<512x56x56x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18138420979}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x144xf32>, %3: tensor<1512x1x1x144xf32>, %7: tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32>\n  return %ret : tensor<128x1x1x1512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x144xf32>, %arg1: tensor<1512x1x1x144xf32>, %arg2: tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1512x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x1512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x1512xf32>\n    memref.copy %2, %alloc : memref<128x1x1x1512xf32> to memref<128x1x1x1512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1512x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x1512xf32>\n    return %3 : tensor<128x1x1x1512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x1512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x144xf32>) -> tensor<128x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1512x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1512x1x1x144xf32>) -> tensor<1512x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x1512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<128x1x1x1512xf32>) -> tensor<128x1x1x1512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x1512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x1512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 95483993}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x160xf32>, %3: tensor<160x1x1x160xf32>, %7: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n  return %ret : tensor<512x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x160xf32>, %arg1: tensor<160x1x1x160xf32>, %arg2: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x160xf32>\n    memref.copy %2, %alloc : memref<512x14x14x160xf32> to memref<512x14x14x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x160xf32>\n    return %3 : tensor<512x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x160xf32>) -> tensor<160x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9346943944}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x576xf32>, %3: tensor<144x1x1x576xf32>, %7: tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n  return %ret : tensor<256x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x576xf32>, %arg1: tensor<144x1x1x576xf32>, %arg2: tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x144xf32>\n    memref.copy %2, %alloc : memref<256x1x1x144xf32> to memref<256x1x1x144xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x144xf32>\n    return %3 : tensor<256x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x576xf32>) -> tensor<256x1x1x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x576xf32>) -> tensor<144x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 78628680}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x11xf32>, %3: tensor<11x1x1x11xf32>, %7: tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>\n  return %ret : tensor<256x56x56x11xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x11xf32>, %arg1: tensor<11x1x1x11xf32>, %arg2: tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<11x1x1x11xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x11xf32>\n    memref.copy %2, %alloc : memref<256x56x56x11xf32> to memref<256x56x56x11xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 11 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x11xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<11x1x1x11xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x11xf32>\n    return %3 : tensor<256x56x56x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x11xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<11x1x1x11xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<11x1x1x11xf32>) -> tensor<11x1x1x11xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x11xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<256x56x56x11xf32>) -> tensor<256x56x56x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 11, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 244768850}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x25x25x672xf32>, %3: tensor<5x5x672x1xf32>, %7: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n  return %ret : tensor<256x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x25x25x672xf32>, %arg1: tensor<5x5x672x1xf32>, %arg2: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x25x25x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<256x11x11x672x1xf32> to memref<256x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x25x25x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x11x11x672x1xf32>\n    return %3 : tensor<256x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x25x25x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x25x25x672xf32>) -> tensor<256x25x25x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x672x1xf32>) -> tensor<5x5x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1570254783}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x16x16x384xf32>, %3: tensor<3x3x384x1xf32>, %7: tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32>\n  return %ret : tensor<128x14x14x384x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x16x16x384xf32>, %arg1: tensor<3x3x384x1xf32>, %arg2: tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x384x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x16x16x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x384x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x384x1xf32>\n    memref.copy %2, %alloc : memref<128x14x14x384x1xf32> to memref<128x14x14x384x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x16x16x384xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x384x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x384x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x384x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x384x1xf32>\n    return %3 : tensor<128x14x14x384x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x384x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x16x16x384xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x16x16x384xf32>) -> tensor<128x16x16x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x384x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x384x1xf32>) -> tensor<3x3x384x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x384x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<128x14x14x384x1xf32>) -> tensor<128x14x14x384x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x384x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x384x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 229029663}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x29x29x192xf32>, %3: tensor<3x3x192x1xf32>, %7: tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n  return %ret : tensor<512x14x14x192x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x29x29x192xf32>, %arg1: tensor<3x3x192x1xf32>, %arg2: tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x192x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x29x29x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x192x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x192x1xf32>\n    memref.copy %2, %alloc : memref<512x14x14x192x1xf32> to memref<512x14x14x192x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x29x29x192xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x192x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x192x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x192x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x192x1xf32>\n    return %3 : tensor<512x14x14x192x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x192x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x29x29x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x29x29x192xf32>) -> tensor<512x29x29x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x192x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x192x1xf32>) -> tensor<3x3x192x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x192x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x192x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x192x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 470215102}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x25x25x336xf32>, %3: tensor<5x5x336x1xf32>, %7: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n  return %ret : tensor<512x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x25x25x336xf32>, %arg1: tensor<5x5x336x1xf32>, %arg2: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x25x25x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<512x21x21x336x1xf32> to memref<512x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x25x25x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x21x21x336x1xf32>\n    return %3 : tensor<512x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x25x25x336xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x25x25x336xf32>) -> tensor<512x25x25x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x336x1xf32>) -> tensor<5x5x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5487688127}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x14xf32>, %3: tensor<152x1x1x14xf32>, %7: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n  return %ret : tensor<512x1x1x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x14xf32>, %arg1: tensor<152x1x1x14xf32>, %arg2: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x14xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x152xf32>\n    memref.copy %2, %alloc : memref<512x1x1x152xf32> to memref<512x1x1x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 14 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x14xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x14xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x152xf32>\n    return %3 : tensor<512x1x1x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x14xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x14xf32>) -> tensor<512x1x1x14xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x14xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x14xf32>) -> tensor<152x1x1x14xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 14, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2063504}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x96xf32>, %3: tensor<96x1x1x96xf32>, %7: tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n  return %ret : tensor<256x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x96xf32>, %arg1: tensor<96x1x1x96xf32>, %arg2: tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x96xf32>\n    memref.copy %2, %alloc : memref<256x56x56x96xf32> to memref<256x56x56x96xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x96xf32>\n    return %3 : tensor<256x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x96xf32>) -> tensor<96x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25872042906}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x72xf32>, %3: tensor<8x1x1x72xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x72xf32>, %arg1: tensor<8x1x1x72xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x72xf32>) -> tensor<256x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x72xf32>) -> tensor<8x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 430171}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x149x149x128xf32>, %3: tensor<3x3x128x1xf32>, %7: tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32>\n  return %ret : tensor<128x147x147x128x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x149x149x128xf32>, %arg1: tensor<3x3x128x1xf32>, %arg2: tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x128x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x149x149x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x147x147x128x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x147x147x128x1xf32>\n    memref.copy %2, %alloc : memref<128x147x147x128x1xf32> to memref<128x147x147x128x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 147 {\n        affine.for %arg5 = 0 to 147 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x149x149x128xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x128x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x147x147x128x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x147x147x128x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x147x147x128x1xf32>\n    return %3 : tensor<128x147x147x128x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x147x147x128x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x149x149x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x149x149x128xf32>) -> tensor<128x149x149x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x128x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x128x1xf32>) -> tensor<3x3x128x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x147x147x128x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x149x149x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x147x147x128x1xf32>) -> tensor<128x147x147x128x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x147x147x128x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x147x147x128x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 147, 1], ["%arg5", 0, 147, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 8204598004}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x299x299x3xf32>, %3: tensor<32x3x3x3xf32>, %7: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n  return %ret : tensor<512x149x149x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x299x299x3xf32>, %arg1: tensor<32x3x3x3xf32>, %arg2: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x299x299x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x149x149x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x149x149x32xf32>\n    memref.copy %2, %alloc : memref<512x149x149x32xf32> to memref<512x149x149x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 149 {\n        affine.for %arg5 = 0 to 149 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x299x299x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x149x149x32xf32>\n    return %3 : tensor<512x149x149x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x149x149x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x299x299x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x299x299x3xf32>) -> tensor<512x299x299x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x3xf32>) -> tensor<32x3x3x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x149x149x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x149x149x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x149x149x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 149, 1], ["%arg5", 0, 149, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28977189763}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<256x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<256x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x224x224x3xf32>, %3: tensor<96x4x4x3xf32>, %7: tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<256x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n  return %ret : tensor<256x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 4 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x224x224x3xf32>, %arg1: tensor<96x4x4x3xf32>, %arg2: tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x4x4x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x96xf32>\n    memref.copy %2, %alloc : memref<256x56x56x96xf32> to memref<256x56x56x96xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 4 {\n              affine.for %arg8 = 0 to 4 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x4x4x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x96xf32>\n    return %3 : tensor<256x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x224x224x3xf32>) -> tensor<256x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x4x4x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x4x4x3xf32>) -> tensor<96x4x4x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<256x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<256x56x56x96xf32>) -> tensor<256x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 4, 1], ["%arg8", 0, 4, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 4 + %arg7", "%arg5 * 4 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12259002478}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x15x15x576xf32>, %3: tensor<3x3x576x1xf32>, %7: tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32>\n  return %ret : tensor<256x7x7x576x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x15x15x576xf32>, %arg1: tensor<3x3x576x1xf32>, %arg2: tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x576x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x15x15x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x576x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x576x1xf32>\n    memref.copy %2, %alloc : memref<256x7x7x576x1xf32> to memref<256x7x7x576x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x15x15x576xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x576x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x576x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x576x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x576x1xf32>\n    return %3 : tensor<256x7x7x576x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x576x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x15x15x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x15x15x576xf32>) -> tensor<256x15x15x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x576x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x576x1xf32>) -> tensor<3x3x576x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x576x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<256x7x7x576x1xf32>) -> tensor<256x7x7x576x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x576x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x576x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 181613765}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x704xf32>, %3: tensor<128x1x1x704xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x704xf32>, %arg1: tensor<128x1x1x704xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x704xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x704xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 704 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x704xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x704xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x704xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x704xf32>) -> tensor<256x7x7x704xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x704xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x704xf32>) -> tensor<128x1x1x704xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 704, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4226536167}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x16xf32>, %3: tensor<128x1x1x16xf32>, %7: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n  return %ret : tensor<128x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x16xf32>, %arg1: tensor<128x1x1x16xf32>, %arg2: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x16xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x16xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x128xf32>\n    memref.copy %2, %alloc : memref<128x1x1x128xf32> to memref<128x1x1x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 16 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x16xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x16xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x128xf32>\n    return %3 : tensor<128x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x16xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x16xf32>) -> tensor<128x1x1x16xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x16xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x16xf32>) -> tensor<128x1x1x16xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 16, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 552751}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x308xf32>, %3: tensor<1232x1x1x308xf32>, %7: tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>\n  return %ret : tensor<128x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x308xf32>, %arg1: tensor<1232x1x1x308xf32>, %arg2: tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1232x1x1x308xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x308xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<128x1x1x1232xf32> to memref<128x1x1x1232xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 308 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x308xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1232x1x1x308xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x1232xf32>\n    return %3 : tensor<128x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x308xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1232x1x1x308xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1232x1x1x308xf32>) -> tensor<1232x1x1x308xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 308, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 175536720}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1440xf32>, %3: tensor<128x1x1x1440xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1440xf32>, %arg1: tensor<128x1x1x1440xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1440xf32>) -> tensor<256x7x7x1440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1440xf32>) -> tensor<128x1x1x1440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8692545142}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x672xf32>, %3: tensor<128x1x1x672xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x672xf32>, %arg1: tensor<128x1x1x672xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x672xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 672 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x672xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x672xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x672xf32>) -> tensor<256x14x14x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x672xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x672xf32>) -> tensor<128x1x1x672xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16126311493}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x24xf32>, %3: tensor<56x1x1x24xf32>, %7: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n  return %ret : tensor<512x56x56x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x24xf32>, %arg1: tensor<56x1x1x24xf32>, %arg2: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x56xf32>\n    memref.copy %2, %alloc : memref<512x56x56x56xf32> to memref<512x56x56x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x56xf32>\n    return %3 : tensor<512x56x56x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x24xf32>) -> tensor<56x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5684915377}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<168x1x1x32xf32>, %7: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n  return %ret : tensor<512x56x56x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<168x1x1x32xf32>, %arg2: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x168xf32>\n    memref.copy %2, %alloc : memref<512x56x56x168xf32> to memref<512x56x56x168xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x168xf32>\n    return %3 : tensor<512x56x56x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x32xf32>) -> tensor<168x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23990156465}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x39x39x728xf32>, %3: tensor<3x3x728x1xf32>, %7: tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32>\n  return %ret : tensor<128x37x37x728x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x39x39x728xf32>, %arg1: tensor<3x3x728x1xf32>, %arg2: tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x728x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x39x39x728xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x37x37x728x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x37x37x728x1xf32>\n    memref.copy %2, %alloc : memref<128x37x37x728x1xf32> to memref<128x37x37x728x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 37 {\n        affine.for %arg5 = 0 to 37 {\n          affine.for %arg6 = 0 to 728 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x39x39x728xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x728x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x37x37x728x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x37x37x728x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x37x37x728x1xf32>\n    return %3 : tensor<128x37x37x728x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x37x37x728x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x39x39x728xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x39x39x728xf32>) -> tensor<128x39x39x728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x728x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x728x1xf32>) -> tensor<3x3x728x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x37x37x728x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<128x37x37x728x1xf32>) -> tensor<128x37x37x728x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x37x37x728x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x37x37x728x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 37, 1], ["%arg5", 0, 37, 1], ["%arg6", 0, 728, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2844134585}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x1232xf32>, %3: tensor<112x1x1x1232xf32>, %7: tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n  return %ret : tensor<256x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x1232xf32>, %arg1: tensor<112x1x1x1232xf32>, %arg2: tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x1232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x1232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x112xf32>\n    memref.copy %2, %alloc : memref<256x1x1x112xf32> to memref<256x1x1x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x1232xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x1232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x112xf32>\n    return %3 : tensor<256x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x1232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x1232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x1232xf32>) -> tensor<112x1x1x1232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 132029721}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x768xf32>, %3: tensor<128x1x1x768xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x768xf32>, %arg1: tensor<128x1x1x768xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2306336628}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1504xf32>, %3: tensor<128x1x1x1504xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1504xf32>, %arg1: tensor<128x1x1x1504xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1504xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1504xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1504 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1504xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1504xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1504xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1504xf32>) -> tensor<512x7x7x1504xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1504xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1504xf32>) -> tensor<128x1x1x1504xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1504, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18157660034}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x89x89x84xf32>, %3: tensor<7x7x84x1xf32>, %7: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n  return %ret : tensor<128x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x89x89x84xf32>, %arg1: tensor<7x7x84x1xf32>, %arg2: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x89x89x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<128x42x42x84x1xf32> to memref<128x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x89x89x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x42x42x84x1xf32>\n    return %3 : tensor<128x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x89x89x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x89x89x84xf32>) -> tensor<128x89x89x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x84x1xf32>) -> tensor<7x7x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3031467799}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x144xf32>, %3: tensor<576x1x1x144xf32>, %7: tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n  return %ret : tensor<128x1x1x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x144xf32>, %arg1: tensor<576x1x1x144xf32>, %arg2: tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<576x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x576xf32>\n    memref.copy %2, %alloc : memref<128x1x1x576xf32> to memref<128x1x1x576xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<576x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x576xf32>\n    return %3 : tensor<128x1x1x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x144xf32>) -> tensor<128x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<576x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<576x1x1x144xf32>) -> tensor<576x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 36835842}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x24xf32>, %3: tensor<56x1x1x24xf32>, %7: tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n  return %ret : tensor<128x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x24xf32>, %arg1: tensor<56x1x1x24xf32>, %arg2: tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x56xf32>\n    memref.copy %2, %alloc : memref<128x28x28x56xf32> to memref<128x28x28x56xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x56xf32>\n    return %3 : tensor<128x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x24xf32>) -> tensor<56x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 356371022}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x512xf32>, %3: tensor<128x1x1x512xf32>, %7: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n  return %ret : tensor<128x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x512xf32>, %arg1: tensor<128x1x1x512xf32>, %arg2: tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x128xf32>\n    memref.copy %2, %alloc : memref<128x1x1x128xf32> to memref<128x1x1x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x128xf32>\n    return %3 : tensor<128x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 30877753}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1792xf32>, %3: tensor<128x1x1x1792xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1792xf32>, %arg1: tensor<128x1x1x1792xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1792xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1792xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1792 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1792xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1792xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1792xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1792xf32>) -> tensor<512x7x7x1792xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1792xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1792xf32>) -> tensor<128x1x1x1792xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1792, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21653324030}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x168xf32>, %3: tensor<8x1x1x168xf32>, %7: tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n  return %ret : tensor<128x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x168xf32>, %arg1: tensor<8x1x1x168xf32>, %arg2: tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x8xf32>\n    memref.copy %2, %alloc : memref<128x1x1x8xf32> to memref<128x1x1x8xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x8xf32>\n    return %3 : tensor<128x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x168xf32>) -> tensor<128x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x168xf32>) -> tensor<8x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 590070}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x768xf32>, %3: tensor<80x1x1x768xf32>, %7: tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n  return %ret : tensor<256x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x768xf32>, %arg1: tensor<80x1x1x768xf32>, %arg2: tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x80xf32>\n    memref.copy %2, %alloc : memref<256x1x1x80xf32> to memref<256x1x1x80xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x80xf32>\n    return %3 : tensor<256x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x768xf32>) -> tensor<256x1x1x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x768xf32>) -> tensor<80x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 58598373}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x48xf32>, %3: tensor<104x1x1x48xf32>, %7: tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32>\n  return %ret : tensor<256x56x56x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x48xf32>, %arg1: tensor<104x1x1x48xf32>, %arg2: tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x104xf32>\n    memref.copy %2, %alloc : memref<256x56x56x104xf32> to memref<256x56x56x104xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x104xf32>\n    return %3 : tensor<256x56x56x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x48xf32>) -> tensor<256x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x48xf32>) -> tensor<104x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<256x56x56x104xf32>) -> tensor<256x56x56x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12532050692}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x672xf32>, %3: tensor<128x1x1x672xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x672xf32>, %arg1: tensor<128x1x1x672xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x672xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 672 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x672xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x672xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x672xf32>) -> tensor<256x7x7x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x672xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x672xf32>) -> tensor<128x1x1x672xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4030039926}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<768x2x2x384xf32>) outs(%7 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<768x2x2x384xf32>) outs(%7 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x384xf32>, %3: tensor<768x2x2x384xf32>, %7: tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<768x2x2x384xf32>) outs(%7 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>\n  return %ret : tensor<128x7x7x768xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x384xf32>, %arg1: tensor<768x2x2x384xf32>, %arg2: tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<768x2x2x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x768xf32>\n    memref.copy %2, %alloc : memref<128x7x7x768xf32> to memref<128x7x7x768xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 768 {\n            affine.for %arg7 = 0 to 2 {\n              affine.for %arg8 = 0 to 2 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<768x2x2x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x768xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x768xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x768xf32>\n    return %3 : tensor<128x7x7x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x384xf32>) -> tensor<128x14x14x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<768x2x2x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<768x2x2x384xf32>) -> tensor<768x2x2x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x768xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<768x2x2x384xf32>) outs(%7 : tensor<128x7x7x768xf32>) -> tensor<128x7x7x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 768, 1], ["%arg7", 0, 2, 1], ["%arg8", 0, 2, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27853470531}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<52x1x1x440xf32>, %7: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n  return %ret : tensor<512x1x1x52xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<52x1x1x440xf32>, %arg2: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<52x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x52xf32>\n    memref.copy %2, %alloc : memref<512x1x1x52xf32> to memref<512x1x1x52xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 52 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<52x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x52xf32>\n    return %3 : tensor<512x1x1x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<52x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<52x1x1x440xf32>) -> tensor<52x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x52xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 52, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 42836510}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<38x1x1x368xf32>, %7: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n  return %ret : tensor<512x1x1x38xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<38x1x1x368xf32>, %arg2: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<38x1x1x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x38xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x38xf32>\n    memref.copy %2, %alloc : memref<512x1x1x38xf32> to memref<512x1x1x38xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 38 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<38x1x1x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x38xf32>\n    return %3 : tensor<512x1x1x38xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x38xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<38x1x1x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<38x1x1x368xf32>) -> tensor<38x1x1x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x38xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x38xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 38, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26021891}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x608xf32>, %3: tensor<64x1x1x608xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x608xf32>, %arg1: tensor<64x1x1x608xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x608xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x608xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x608xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x608xf32>) -> tensor<512x1x1x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x608xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x608xf32>) -> tensor<64x1x1x608xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 73761617}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x21x21x728xf32>, %3: tensor<3x3x728x1xf32>, %7: tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n  return %ret : tensor<512x19x19x728x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x21x21x728xf32>, %arg1: tensor<3x3x728x1xf32>, %arg2: tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x728x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x21x21x728xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x19x19x728x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x19x19x728x1xf32>\n    memref.copy %2, %alloc : memref<512x19x19x728x1xf32> to memref<512x19x19x728x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 19 {\n        affine.for %arg5 = 0 to 19 {\n          affine.for %arg6 = 0 to 728 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x21x21x728xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x728x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x19x19x728x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x19x19x728x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x19x19x728x1xf32>\n    return %3 : tensor<512x19x19x728x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x19x19x728x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x21x21x728xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x21x21x728xf32>) -> tensor<512x21x21x728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x728x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x728x1xf32>) -> tensor<3x3x728x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x19x19x728x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x19x19x728x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x19x19x728x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 19, 1], ["%arg5", 0, 19, 1], ["%arg6", 0, 728, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3002181362}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<110x1x1x440xf32>, %7: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n  return %ret : tensor<512x1x1x110xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<110x1x1x440xf32>, %arg2: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<110x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x110xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x110xf32>\n    memref.copy %2, %alloc : memref<512x1x1x110xf32> to memref<512x1x1x110xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 110 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<110x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x110xf32>\n    return %3 : tensor<512x1x1x110xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x110xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<110x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<110x1x1x440xf32>) -> tensor<110x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x110xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x110xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x110xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 110, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 90810120}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<432x1x1x432xf32>) outs(%7 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<432x1x1x432xf32>) outs(%7 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x432xf32>, %3: tensor<432x1x1x432xf32>, %7: tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<432x1x1x432xf32>) outs(%7 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>\n  return %ret : tensor<128x14x14x432xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x432xf32>, %arg1: tensor<432x1x1x432xf32>, %arg2: tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<432x1x1x432xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x432xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x432xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x432xf32>\n    memref.copy %2, %alloc : memref<128x14x14x432xf32> to memref<128x14x14x432xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 432 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 432 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x432xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<432x1x1x432xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x432xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x432xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x432xf32>\n    return %3 : tensor<128x14x14x432xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x432xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x432xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<432x1x1x432xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<432x1x1x432xf32>) -> tensor<432x1x1x432xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x432xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<432x1x1x432xf32>) outs(%7 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x432xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x432xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 432, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 432, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17368598226}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x35x35x192xf32>, %3: tensor<64x1x1x192xf32>, %7: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n  return %ret : tensor<128x35x35x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x35x35x192xf32>, %arg1: tensor<64x1x1x192xf32>, %arg2: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x64xf32>\n    memref.copy %2, %alloc : memref<128x35x35x64xf32> to memref<128x35x35x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x35x35x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x64xf32>\n    return %3 : tensor<128x35x35x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x35x35x192xf32>) -> tensor<128x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x192xf32>) -> tensor<64x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6983171464}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x6xf32>, %3: tensor<56x1x1x6xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x6xf32>, %arg1: tensor<56x1x1x6xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x6xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x6xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 6 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x6xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x6xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x6xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x6xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x6xf32>) -> tensor<56x1x1x6xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 6, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 272934}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1600xf32>, %3: tensor<128x1x1x1600xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1600xf32>, %arg1: tensor<128x1x1x1600xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1600xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1600xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1600 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1600xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1600xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1600xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1600xf32>) -> tensor<128x14x14x1600xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1600xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1600xf32>) -> tensor<128x1x1x1600xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1600, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19323466677}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x44xf32>, %3: tensor<44x1x1x44xf32>, %7: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n  return %ret : tensor<512x28x28x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x44xf32>, %arg1: tensor<44x1x1x44xf32>, %arg2: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x44xf32>\n    memref.copy %2, %alloc : memref<512x28x28x44xf32> to memref<512x28x28x44xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 44 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x44xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x44xf32>\n    return %3 : tensor<512x28x28x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x44xf32>) -> tensor<44x1x1x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 44, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2431321297}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x76x76x128xf32>, %3: tensor<3x3x128x1xf32>, %7: tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32>\n  return %ret : tensor<128x74x74x128x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x76x76x128xf32>, %arg1: tensor<3x3x128x1xf32>, %arg2: tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x128x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x76x76x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x74x74x128x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x74x74x128x1xf32>\n    memref.copy %2, %alloc : memref<128x74x74x128x1xf32> to memref<128x74x74x128x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 74 {\n        affine.for %arg5 = 0 to 74 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x76x76x128xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x128x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x74x74x128x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x74x74x128x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x74x74x128x1xf32>\n    return %3 : tensor<128x74x74x128x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x74x74x128x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x76x76x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x76x76x128xf32>) -> tensor<128x76x76x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x128x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x128x1xf32>) -> tensor<3x3x128x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x74x74x128x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<128x74x74x128x1xf32>) -> tensor<128x74x74x128x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x74x74x128x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x74x74x128x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 74, 1], ["%arg5", 0, 74, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2113681735}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x336xf32>, %3: tensor<84x1x1x336xf32>, %7: tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>\n  return %ret : tensor<128x1x1x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x336xf32>, %arg1: tensor<84x1x1x336xf32>, %arg2: tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x84xf32>\n    memref.copy %2, %alloc : memref<128x1x1x84xf32> to memref<128x1x1x84xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x84xf32>\n    return %3 : tensor<128x1x1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x336xf32>) -> tensor<84x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13276676}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x9x9x176xf32>, %3: tensor<3x3x176x1xf32>, %7: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n  return %ret : tensor<512x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x9x9x176xf32>, %arg1: tensor<3x3x176x1xf32>, %arg2: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x9x9x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176x1xf32> to memref<512x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x9x9x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176x1xf32>\n    return %3 : tensor<512x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x9x9x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x9x9x176xf32>) -> tensor<512x9x9x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x176x1xf32>) -> tensor<3x3x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 100218812}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1696xf32>, %3: tensor<128x1x1x1696xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1696xf32>, %arg1: tensor<128x1x1x1696xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1696xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1696xf32>) -> tensor<128x7x7x1696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1696xf32>) -> tensor<128x1x1x1696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5122217596}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x8xf32>, %3: tensor<48x1x1x8xf32>, %7: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n  return %ret : tensor<128x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x8xf32>, %arg1: tensor<48x1x1x8xf32>, %arg2: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x48xf32>\n    memref.copy %2, %alloc : memref<128x1x1x48xf32> to memref<128x1x1x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x48xf32>\n    return %3 : tensor<128x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x8xf32>) -> tensor<48x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 88417}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x222xf32>, %3: tensor<888x1x1x222xf32>, %7: tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32>\n  return %ret : tensor<128x1x1x888xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x222xf32>, %arg1: tensor<888x1x1x222xf32>, %arg2: tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<888x1x1x222xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x222xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x888xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x888xf32>\n    memref.copy %2, %alloc : memref<128x1x1x888xf32> to memref<128x1x1x888xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 888 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 222 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x222xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<888x1x1x222xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x888xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x888xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x888xf32>\n    return %3 : tensor<128x1x1x888xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x888xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x222xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x222xf32>) -> tensor<128x1x1x222xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<888x1x1x222xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<888x1x1x222xf32>) -> tensor<888x1x1x222xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x888xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<128x1x1x888xf32>) -> tensor<128x1x1x888xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x888xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x888xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 888, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 222, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 89495996}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x320xf32>, %3: tensor<128x1x1x320xf32>, %7: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n  return %ret : tensor<128x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x320xf32>, %arg1: tensor<128x1x1x320xf32>, %arg2: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x128xf32>\n    memref.copy %2, %alloc : memref<128x28x28x128xf32> to memref<128x28x28x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x128xf32>\n    return %3 : tensor<128x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x320xf32>) -> tensor<128x28x28x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15156912207}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x46x46x168xf32>, %3: tensor<5x5x168x1xf32>, %7: tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32>\n  return %ret : tensor<256x42x42x168x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x46x46x168xf32>, %arg1: tensor<5x5x168x1xf32>, %arg2: tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x168x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x46x46x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x42x42x168x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x42x42x168x1xf32>\n    memref.copy %2, %alloc : memref<256x42x42x168x1xf32> to memref<256x42x42x168x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x46x46x168xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x168x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x168x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x168x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x42x42x168x1xf32>\n    return %3 : tensor<256x42x42x168x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x42x42x168x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x46x46x168xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x46x46x168xf32>) -> tensor<256x46x46x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x168x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x168x1xf32>) -> tensor<5x5x168x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x42x42x168x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<256x42x42x168x1xf32>) -> tensor<256x42x42x168x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x42x42x168x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x42x42x168x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5417789400}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x112xf32>, %3: tensor<1232x1x1x112xf32>, %7: tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>\n  return %ret : tensor<256x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x112xf32>, %arg1: tensor<1232x1x1x112xf32>, %arg2: tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1232x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<256x1x1x1232xf32> to memref<256x1x1x1232xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1232x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x1232xf32>\n    return %3 : tensor<256x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1232x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1232x1x1x112xf32>) -> tensor<1232x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<256x1x1x1232xf32>) -> tensor<256x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 117826387}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x544xf32>, %3: tensor<128x1x1x544xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x544xf32>, %arg1: tensor<128x1x1x544xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x544xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x544xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 544 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x544xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x544xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x544xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x544xf32>) -> tensor<128x14x14x544xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x544xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x544xf32>) -> tensor<128x1x1x544xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 544, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6511785021}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x10x8x224xf32>, %3: tensor<256x3x1x224xf32>, %7: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n  return %ret : tensor<512x8x8x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x10x8x224xf32>, %arg1: tensor<256x3x1x224xf32>, %arg2: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x3x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x10x8x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x8x8x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x8x8x256xf32>\n    memref.copy %2, %alloc : memref<512x8x8x256xf32> to memref<512x8x8x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x10x8x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x3x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x8x8x256xf32>\n    return %3 : tensor<512x8x8x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x8x8x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x10x8x224xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x10x8x224xf32>) -> tensor<512x10x8x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x3x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x3x1x224xf32>) -> tensor<256x3x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x8x8x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x8x8x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x8x8x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21132970056}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x168xf32>, %3: tensor<408x1x1x168xf32>, %7: tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32>\n  return %ret : tensor<128x28x28x408xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x168xf32>, %arg1: tensor<408x1x1x168xf32>, %arg2: tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<408x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x408xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x408xf32>\n    memref.copy %2, %alloc : memref<128x28x28x408xf32> to memref<128x28x28x408xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 408 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<408x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x408xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x408xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x408xf32>\n    return %3 : tensor<128x28x28x408xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x408xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<408x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<408x1x1x168xf32>) -> tensor<408x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x408xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<128x28x28x408xf32>) -> tensor<128x28x28x408xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x408xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x408xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 408, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24787064867}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1632xf32>, %3: tensor<128x1x1x1632xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1632xf32>, %arg1: tensor<128x1x1x1632xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1632xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1632xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1632 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1632xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1632xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1632xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1632xf32>) -> tensor<256x7x7x1632xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1632xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1632xf32>) -> tensor<128x1x1x1632xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1632, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9855606713}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x768xf32>, tensor<768x1x1x768xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x768xf32>, tensor<768x1x1x768xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x768xf32>, %3: tensor<768x1x1x768xf32>, %7: tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x768xf32>, tensor<768x1x1x768xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n  return %ret : tensor<256x7x7x768xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x768xf32>, %arg1: tensor<768x1x1x768xf32>, %arg2: tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<768x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x768xf32>\n    memref.copy %2, %alloc : memref<256x7x7x768xf32> to memref<256x7x7x768xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 768 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<768x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x768xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x768xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x768xf32>\n    return %3 : tensor<256x7x7x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<768x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<768x1x1x768xf32>) -> tensor<768x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x768xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x768xf32>, tensor<768x1x1x768xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 768, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27689057997}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x208xf32>, %3: tensor<440x1x1x208xf32>, %7: tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32>\n  return %ret : tensor<256x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x208xf32>, %arg1: tensor<440x1x1x208xf32>, %arg2: tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x440xf32>\n    memref.copy %2, %alloc : memref<256x7x7x440xf32> to memref<256x7x7x440xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x440xf32>\n    return %3 : tensor<256x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x208xf32>) -> tensor<440x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<256x7x7x440xf32>) -> tensor<256x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4169715474}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x384xf32>, tensor<384x1x3x384xf32>) outs(%7 : tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x384xf32>, tensor<384x1x3x384xf32>) outs(%7 : tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x8x10x384xf32>, %3: tensor<384x1x3x384xf32>, %7: tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x384xf32>, tensor<384x1x3x384xf32>) outs(%7 : tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32>\n  return %ret : tensor<128x8x8x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x8x10x384xf32>, %arg1: tensor<384x1x3x384xf32>, %arg2: tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x3x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x8x10x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x8x8x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x8x8x384xf32>\n    memref.copy %2, %alloc : memref<128x8x8x384xf32> to memref<128x8x8x384xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x8x10x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x3x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x8x8x384xf32>\n    return %3 : tensor<128x8x8x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x8x8x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x8x10x384xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x8x10x384xf32>) -> tensor<128x8x10x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x3x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x3x384xf32>) -> tensor<384x1x3x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x8x8x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x8x10x384xf32>, tensor<384x1x3x384xf32>) outs(%7 : tensor<128x8x8x384xf32>) -> tensor<128x8x8x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x8x8x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x8x8x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13632999619}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x8xf32>, %3: tensor<168x1x1x8xf32>, %7: tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n  return %ret : tensor<256x1x1x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x8xf32>, %arg1: tensor<168x1x1x8xf32>, %arg2: tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x168xf32>\n    memref.copy %2, %alloc : memref<256x1x1x168xf32> to memref<256x1x1x168xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x168xf32>\n    return %3 : tensor<256x1x1x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x8xf32>) -> tensor<168x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 563052}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1568xf32>, %3: tensor<128x1x1x1568xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1568xf32>, %arg1: tensor<128x1x1x1568xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1568xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1568xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1568 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1568xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1568xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1568xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1568xf32>) -> tensor<512x7x7x1568xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1568xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1568xf32>) -> tensor<128x1x1x1568xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1568, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18933884724}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x48xf32>, %3: tensor<112x1x1x48xf32>, %7: tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32>\n  return %ret : tensor<256x56x56x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x48xf32>, %arg1: tensor<112x1x1x48xf32>, %arg2: tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x112xf32>\n    memref.copy %2, %alloc : memref<256x56x56x112xf32> to memref<256x56x56x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x112xf32>\n    return %3 : tensor<256x56x56x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x48xf32>) -> tensor<256x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x48xf32>) -> tensor<112x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<256x56x56x112xf32>) -> tensor<256x56x56x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13493324833}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x168xf32>, %3: tensor<42x1x1x168xf32>, %7: tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>\n  return %ret : tensor<128x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x168xf32>, %arg1: tensor<42x1x1x168xf32>, %arg2: tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x42xf32>\n    memref.copy %2, %alloc : memref<128x1x1x42xf32> to memref<128x1x1x42xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x42xf32>\n    return %3 : tensor<128x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x168xf32>) -> tensor<128x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x168xf32>) -> tensor<42x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3142964}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x64xf32>, %3: tensor<64x1x1x64xf32>, %7: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n  return %ret : tensor<512x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x64xf32>, %arg1: tensor<64x1x1x64xf32>, %arg2: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x64xf32>\n    memref.copy %2, %alloc : memref<512x56x56x64xf32> to memref<512x56x56x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x64xf32>\n    return %3 : tensor<512x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x64xf32>) -> tensor<64x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21919136464}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x48xf32>, %3: tensor<8x1x1x48xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x48xf32>, %arg1: tensor<8x1x1x48xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x48xf32>) -> tensor<8x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 244981}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x224x224x3xf32>, %3: tensor<32x3x3x3xf32>, %7: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n  return %ret : tensor<512x111x111x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x224x224x3xf32>, %arg1: tensor<32x3x3x3xf32>, %arg2: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x111x111x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x111x111x32xf32>\n    memref.copy %2, %alloc : memref<512x111x111x32xf32> to memref<512x111x111x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 111 {\n        affine.for %arg5 = 0 to 111 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x111x111x32xf32>\n    return %3 : tensor<512x111x111x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x111x111x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x224x224x3xf32>) -> tensor<512x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x3xf32>) -> tensor<32x3x3x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x111x111x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x111x111x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x111x111x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 111, 1], ["%arg5", 0, 111, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16074125793}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x30x30x192xf32>, %3: tensor<3x3x192x1xf32>, %7: tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32>\n  return %ret : tensor<128x28x28x192x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x30x30x192xf32>, %arg1: tensor<3x3x192x1xf32>, %arg2: tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x192x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x30x30x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x192x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x192x1xf32>\n    memref.copy %2, %alloc : memref<128x28x28x192x1xf32> to memref<128x28x28x192x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x30x30x192xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x192x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x28x28x192x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x28x28x192x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x192x1xf32>\n    return %3 : tensor<128x28x28x192x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x192x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x30x30x192xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x30x30x192xf32>) -> tensor<128x30x30x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x192x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x192x1xf32>) -> tensor<3x3x192x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x192x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<128x28x28x192x1xf32>) -> tensor<128x28x28x192x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x192x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x192x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 452393420}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x448xf32>, %3: tensor<128x1x1x448xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x448xf32>, %arg1: tensor<128x1x1x448xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x448xf32>) -> tensor<128x14x14x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5340987223}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x160xf32>, %3: tensor<384x1x1x160xf32>, %7: tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32>\n  return %ret : tensor<256x14x14x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x160xf32>, %arg1: tensor<384x1x1x160xf32>, %arg2: tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x384xf32>\n    memref.copy %2, %alloc : memref<256x14x14x384xf32> to memref<256x14x14x384xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x384xf32>\n    return %3 : tensor<256x14x14x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x1x160xf32>) -> tensor<384x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x14x14x384xf32>) -> tensor<256x14x14x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11114516782}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<16x1x1x128xf32>, %7: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n  return %ret : tensor<512x1x1x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<16x1x1x128xf32>, %arg2: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x16xf32>\n    memref.copy %2, %alloc : memref<512x1x1x16xf32> to memref<512x1x1x16xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<16x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x16xf32>\n    return %3 : tensor<512x1x1x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<16x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<16x1x1x128xf32>) -> tensor<16x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x16xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x16xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3560698}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x31x31x88xf32>, %3: tensor<5x5x88x1xf32>, %7: tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n  return %ret : tensor<256x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x31x31x88xf32>, %arg1: tensor<5x5x88x1xf32>, %arg2: tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x31x31x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<256x14x14x88x1xf32> to memref<256x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x31x31x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x88x1xf32>\n    return %3 : tensor<256x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x31x31x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x31x31x88xf32>) -> tensor<256x31x31x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x88x1xf32>) -> tensor<5x5x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 319312119}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x528xf32>, %3: tensor<528x1x1x528xf32>, %7: tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n  return %ret : tensor<256x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x528xf32>, %arg1: tensor<528x1x1x528xf32>, %arg2: tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<528x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x528xf32>\n    memref.copy %2, %alloc : memref<256x7x7x528xf32> to memref<256x7x7x528xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<528x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x528xf32>\n    return %3 : tensor<256x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<528x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<528x1x1x528xf32>) -> tensor<528x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<256x7x7x528xf32>) -> tensor<256x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13015143285}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x696xf32>, tensor<58x1x1x696xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x696xf32>, tensor<58x1x1x696xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x696xf32>, %3: tensor<58x1x1x696xf32>, %7: tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x696xf32>, tensor<58x1x1x696xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n  return %ret : tensor<128x1x1x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x696xf32>, %arg1: tensor<58x1x1x696xf32>, %arg2: tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<58x1x1x696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x58xf32>\n    memref.copy %2, %alloc : memref<128x1x1x58xf32> to memref<128x1x1x58xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x696xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<58x1x1x696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x58xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x58xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x58xf32>\n    return %3 : tensor<128x1x1x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x696xf32>) -> tensor<128x1x1x696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<58x1x1x696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<58x1x1x696xf32>) -> tensor<58x1x1x696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x58xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x696xf32>, tensor<58x1x1x696xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x58xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19117425}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<2016x1x1x224xf32>) outs(%7 : tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<2016x1x1x224xf32>) outs(%7 : tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x224xf32>, %3: tensor<2016x1x1x224xf32>, %7: tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<2016x1x1x224xf32>) outs(%7 : tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32>\n  return %ret : tensor<128x1x1x2016xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x224xf32>, %arg1: tensor<2016x1x1x224xf32>, %arg2: tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<2016x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x2016xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x2016xf32>\n    memref.copy %2, %alloc : memref<128x1x1x2016xf32> to memref<128x1x1x2016xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 2016 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<2016x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x2016xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x2016xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x2016xf32>\n    return %3 : tensor<128x1x1x2016xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x2016xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<2016x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<2016x1x1x224xf32>) -> tensor<2016x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x2016xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x224xf32>, tensor<2016x1x1x224xf32>) outs(%7 : tensor<128x1x1x2016xf32>) -> tensor<128x1x1x2016xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x2016xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x2016xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 2016, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 205218657}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x48xf32>, %3: tensor<120x1x1x48xf32>, %7: tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32>\n  return %ret : tensor<128x56x56x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x48xf32>, %arg1: tensor<120x1x1x48xf32>, %arg2: tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x120xf32>\n    memref.copy %2, %alloc : memref<128x56x56x120xf32> to memref<128x56x56x120xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x120xf32>\n    return %3 : tensor<128x56x56x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x48xf32>) -> tensor<120x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<128x56x56x120xf32>) -> tensor<128x56x56x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7226335299}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x110xf32>, tensor<440x1x1x110xf32>) outs(%7 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x110xf32>, tensor<440x1x1x110xf32>) outs(%7 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x110xf32>, %3: tensor<440x1x1x110xf32>, %7: tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x110xf32>, tensor<440x1x1x110xf32>) outs(%7 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n  return %ret : tensor<512x1x1x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x110xf32>, %arg1: tensor<440x1x1x110xf32>, %arg2: tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x110xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x110xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x440xf32>\n    memref.copy %2, %alloc : memref<512x1x1x440xf32> to memref<512x1x1x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 110 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x110xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x110xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x440xf32>\n    return %3 : tensor<512x1x1x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x110xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x110xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x110xf32>) -> tensor<440x1x1x110xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x110xf32>, tensor<440x1x1x110xf32>) outs(%7 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 110, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 82297906}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x32x32x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x32x32x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x32x32x22xf32>, %3: tensor<5x5x22x1xf32>, %7: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x32x32x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n  return %ret : tensor<512x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x32x22xf32>, %arg1: tensor<5x5x22x1xf32>, %arg2: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x32x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x22x1xf32> to memref<512x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x32x32x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x22x1xf32>\n    return %3 : tensor<512x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x32x32x22xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x32x32x22xf32>) -> tensor<512x32x32x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x22x1xf32>) -> tensor<5x5x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x32x32x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 642711820}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1248xf32>, %3: tensor<128x1x1x1248xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1248xf32>, %arg1: tensor<128x1x1x1248xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1248xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1248xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1248 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1248xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1248xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1248xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1248xf32>) -> tensor<128x7x7x1248xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1248xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1248xf32>) -> tensor<128x1x1x1248xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1248, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3762547225}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x104xf32>, %3: tensor<208x1x1x104xf32>, %7: tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32>\n  return %ret : tensor<256x28x28x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x104xf32>, %arg1: tensor<208x1x1x104xf32>, %arg2: tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x208xf32>\n    memref.copy %2, %alloc : memref<256x28x28x208xf32> to memref<256x28x28x208xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x208xf32>\n    return %3 : tensor<256x28x28x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x104xf32>) -> tensor<256x28x28x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x104xf32>) -> tensor<208x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<256x28x28x208xf32>) -> tensor<256x28x28x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15108078803}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<54x1x1x216xf32>) outs(%7 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<54x1x1x216xf32>) outs(%7 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x216xf32>, %3: tensor<54x1x1x216xf32>, %7: tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<54x1x1x216xf32>) outs(%7 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>\n  return %ret : tensor<512x1x1x54xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x216xf32>, %arg1: tensor<54x1x1x216xf32>, %arg2: tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<54x1x1x216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x54xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x54xf32>\n    memref.copy %2, %alloc : memref<512x1x1x54xf32> to memref<512x1x1x54xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 54 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x216xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<54x1x1x216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x54xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x54xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x54xf32>\n    return %3 : tensor<512x1x1x54xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x54xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<54x1x1x216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<54x1x1x216xf32>) -> tensor<54x1x1x216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x54xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<54x1x1x216xf32>) outs(%7 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x54xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x54xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 54, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21139153}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x56xf32>, %3: tensor<152x1x1x56xf32>, %7: tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>\n  return %ret : tensor<128x14x14x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x56xf32>, %arg1: tensor<152x1x1x56xf32>, %arg2: tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x152xf32>\n    memref.copy %2, %alloc : memref<128x14x14x152xf32> to memref<128x14x14x152xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x152xf32>\n    return %3 : tensor<128x14x14x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x56xf32>) -> tensor<152x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 688958953}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<14x1x1x152xf32>) outs(%7 : tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<14x1x1x152xf32>) outs(%7 : tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x152xf32>, %3: tensor<14x1x1x152xf32>, %7: tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<14x1x1x152xf32>) outs(%7 : tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32>\n  return %ret : tensor<128x1x1x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x152xf32>, %arg1: tensor<14x1x1x152xf32>, %arg2: tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<14x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x14xf32>\n    memref.copy %2, %alloc : memref<128x1x1x14xf32> to memref<128x1x1x14xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<14x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x14xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x14xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x14xf32>\n    return %3 : tensor<128x1x1x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<14x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<14x1x1x152xf32>) -> tensor<14x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x14xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<14x1x1x152xf32>) outs(%7 : tensor<128x1x1x14xf32>) -> tensor<128x1x1x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x14xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 960835}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<288x1x1x72xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<288x1x1x72xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x72xf32>, %3: tensor<288x1x1x72xf32>, %7: tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<288x1x1x72xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n  return %ret : tensor<128x1x1x288xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x72xf32>, %arg1: tensor<288x1x1x72xf32>, %arg2: tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x288xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x288xf32>\n    memref.copy %2, %alloc : memref<128x1x1x288xf32> to memref<128x1x1x288xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 288 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<288x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x288xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x288xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x288xf32>\n    return %3 : tensor<128x1x1x288xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x288xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<288x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<288x1x1x72xf32>) -> tensor<288x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x288xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<288x1x1x72xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x288xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x288xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 288, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8499133}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1152xf32>, tensor<128x1x1x1152xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1152xf32>, tensor<128x1x1x1152xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1152xf32>, %3: tensor<128x1x1x1152xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1152xf32>, tensor<128x1x1x1152xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1152xf32>, %arg1: tensor<128x1x1x1152xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1152xf32>) -> tensor<128x7x7x1152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1152xf32>) -> tensor<128x1x1x1152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1152xf32>, tensor<128x1x1x1152xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3471151886}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x992xf32>, tensor<128x1x1x992xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x992xf32>, tensor<128x1x1x992xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x992xf32>, %3: tensor<128x1x1x992xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x992xf32>, tensor<128x1x1x992xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x992xf32>, %arg1: tensor<128x1x1x992xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x992xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x992xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 992 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x992xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x992xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x992xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x992xf32>) -> tensor<512x7x7x992xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x992xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x992xf32>) -> tensor<128x1x1x992xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x992xf32>, tensor<128x1x1x992xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 992, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11944214624}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x111x111x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x111x111x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x111x111x32xf32>, %3: tensor<11x1x1x32xf32>, %7: tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x111x111x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32>\n  return %ret : tensor<128x111x111x11xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x111x111x32xf32>, %arg1: tensor<11x1x1x32xf32>, %arg2: tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<11x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x111x111x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x111x111x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x111x111x11xf32>\n    memref.copy %2, %alloc : memref<128x111x111x11xf32> to memref<128x111x111x11xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 111 {\n        affine.for %arg5 = 0 to 111 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x111x111x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<11x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x111x111x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x111x111x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x111x111x11xf32>\n    return %3 : tensor<128x111x111x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x111x111x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x111x111x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x111x111x32xf32>) -> tensor<128x111x111x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<11x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<11x1x1x32xf32>) -> tensor<11x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x111x111x11xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x111x111x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<128x111x111x11xf32>) -> tensor<128x111x111x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x111x111x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x111x111x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 111, 1], ["%arg5", 0, 111, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1576394807}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x28xf32>, tensor<112x1x1x28xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x28xf32>, tensor<112x1x1x28xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x28xf32>, %3: tensor<112x1x1x28xf32>, %7: tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x28xf32>, tensor<112x1x1x28xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n  return %ret : tensor<256x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x28xf32>, %arg1: tensor<112x1x1x28xf32>, %arg2: tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x28xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x112xf32>\n    memref.copy %2, %alloc : memref<256x1x1x112xf32> to memref<256x1x1x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 28 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x28xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x28xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x112xf32>\n    return %3 : tensor<256x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x28xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x28xf32>) -> tensor<256x1x1x28xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x28xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x28xf32>) -> tensor<112x1x1x28xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x28xf32>, tensor<112x1x1x28xf32>) outs(%7 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 28, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1861580}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x48xf32>, %3: tensor<112x1x1x48xf32>, %7: tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>\n  return %ret : tensor<128x28x28x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x48xf32>, %arg1: tensor<112x1x1x48xf32>, %arg2: tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x112xf32>\n    memref.copy %2, %alloc : memref<128x28x28x112xf32> to memref<128x28x28x112xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x112xf32>\n    return %3 : tensor<128x28x28x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x48xf32>) -> tensor<112x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1687042825}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x64xf32>, %3: tensor<160x1x1x64xf32>, %7: tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32>\n  return %ret : tensor<128x28x28x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x64xf32>, %arg1: tensor<160x1x1x64xf32>, %arg2: tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x160xf32>\n    memref.copy %2, %alloc : memref<128x28x28x160xf32> to memref<128x28x28x160xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x160xf32>\n    return %3 : tensor<128x28x28x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x64xf32>) -> tensor<160x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<128x28x28x160xf32>) -> tensor<128x28x28x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3436248713}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<120x1x1x30xf32>) outs(%7 : tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<120x1x1x30xf32>) outs(%7 : tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x30xf32>, %3: tensor<120x1x1x30xf32>, %7: tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<120x1x1x30xf32>) outs(%7 : tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32>\n  return %ret : tensor<128x1x1x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x30xf32>, %arg1: tensor<120x1x1x30xf32>, %arg2: tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x30xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x30xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x120xf32>\n    memref.copy %2, %alloc : memref<128x1x1x120xf32> to memref<128x1x1x120xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 30 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x30xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x30xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x120xf32>\n    return %3 : tensor<128x1x1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x30xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x30xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x30xf32>) -> tensor<120x1x1x30xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x30xf32>, tensor<120x1x1x30xf32>) outs(%7 : tensor<128x1x1x120xf32>) -> tensor<128x1x1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 30, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1084174}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x72xf32>, tensor<168x1x1x72xf32>) outs(%7 : tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x72xf32>, tensor<168x1x1x72xf32>) outs(%7 : tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x72xf32>, %3: tensor<168x1x1x72xf32>, %7: tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x72xf32>, tensor<168x1x1x72xf32>) outs(%7 : tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32>\n  return %ret : tensor<256x28x28x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x72xf32>, %arg1: tensor<168x1x1x72xf32>, %arg2: tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x168xf32>\n    memref.copy %2, %alloc : memref<256x28x28x168xf32> to memref<256x28x28x168xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x168xf32>\n    return %3 : tensor<256x28x28x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x72xf32>) -> tensor<168x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x72xf32>, tensor<168x1x1x72xf32>) outs(%7 : tensor<256x28x28x168xf32>) -> tensor<256x28x28x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8153200491}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x256xf32>, %3: tensor<128x1x1x256xf32>, %7: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n  return %ret : tensor<256x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x256xf32>, %arg1: tensor<128x1x1x256xf32>, %arg2: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x128xf32>\n    memref.copy %2, %alloc : memref<256x28x28x128xf32> to memref<256x28x28x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x128xf32>\n    return %3 : tensor<256x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x256xf32>) -> tensor<256x28x28x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24084146241}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<320x1x1x320xf32>) outs(%7 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<320x1x1x320xf32>) outs(%7 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x320xf32>, %3: tensor<320x1x1x320xf32>, %7: tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<320x1x1x320xf32>) outs(%7 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>\n  return %ret : tensor<256x14x14x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x320xf32>, %arg1: tensor<320x1x1x320xf32>, %arg2: tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x320xf32>\n    memref.copy %2, %alloc : memref<256x14x14x320xf32> to memref<256x14x14x320xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x320xf32>\n    return %3 : tensor<256x14x14x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x320xf32>) -> tensor<320x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<320x1x1x320xf32>) outs(%7 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18966062580}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x896xf32>, tensor<224x1x1x896xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x896xf32>, tensor<224x1x1x896xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x896xf32>, %3: tensor<224x1x1x896xf32>, %7: tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x896xf32>, tensor<224x1x1x896xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n  return %ret : tensor<256x1x1x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x896xf32>, %arg1: tensor<224x1x1x896xf32>, %arg2: tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x896xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x896xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x224xf32>\n    memref.copy %2, %alloc : memref<256x1x1x224xf32> to memref<256x1x1x224xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 896 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x896xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x896xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x224xf32>\n    return %3 : tensor<256x1x1x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x896xf32>) -> tensor<256x1x1x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x896xf32>) -> tensor<224x1x1x896xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x896xf32>, tensor<224x1x1x896xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 191303599}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<96x1x1x384xf32>) outs(%7 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<96x1x1x384xf32>) outs(%7 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x384xf32>, %3: tensor<96x1x1x384xf32>, %7: tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<96x1x1x384xf32>) outs(%7 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>\n  return %ret : tensor<128x14x14x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x384xf32>, %arg1: tensor<96x1x1x384xf32>, %arg2: tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x96xf32>\n    memref.copy %2, %alloc : memref<128x14x14x96xf32> to memref<128x14x14x96xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x96xf32>\n    return %3 : tensor<128x14x14x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x384xf32>) -> tensor<128x14x14x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x384xf32>) -> tensor<96x1x1x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<96x1x1x384xf32>) outs(%7 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3429036759}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<232x1x1x8xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<232x1x1x8xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x8xf32>, %3: tensor<232x1x1x8xf32>, %7: tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<232x1x1x8xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n  return %ret : tensor<256x1x1x232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x8xf32>, %arg1: tensor<232x1x1x8xf32>, %arg2: tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<232x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x232xf32>\n    memref.copy %2, %alloc : memref<256x1x1x232xf32> to memref<256x1x1x232xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<232x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x232xf32>\n    return %3 : tensor<256x1x1x232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<232x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<232x1x1x8xf32>) -> tensor<232x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<232x1x1x8xf32>) outs(%7 : tensor<256x1x1x232xf32>) -> tensor<256x1x1x232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 774890}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x1x1x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x1x1x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x96xf32>, %3: tensor<192x1x1x96xf32>, %7: tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x1x1x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n  return %ret : tensor<128x28x28x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x96xf32>, %arg1: tensor<192x1x1x96xf32>, %arg2: tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x192xf32>\n    memref.copy %2, %alloc : memref<128x28x28x192xf32> to memref<128x28x28x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x192xf32>\n    return %3 : tensor<128x28x28x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x96xf32>) -> tensor<128x56x56x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x96xf32>) -> tensor<192x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<192x1x1x96xf32>) outs(%7 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6454464769}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x64xf32>, tensor<608x1x1x64xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x64xf32>, tensor<608x1x1x64xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x64xf32>, %3: tensor<608x1x1x64xf32>, %7: tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x64xf32>, tensor<608x1x1x64xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n  return %ret : tensor<128x1x1x608xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x64xf32>, %arg1: tensor<608x1x1x64xf32>, %arg2: tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<608x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x608xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x608xf32>\n    memref.copy %2, %alloc : memref<128x1x1x608xf32> to memref<128x1x1x608xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 608 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<608x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x608xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x608xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x608xf32>\n    return %3 : tensor<128x1x1x608xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x608xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x64xf32>) -> tensor<128x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<608x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<608x1x1x64xf32>) -> tensor<608x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x608xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x64xf32>, tensor<608x1x1x64xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x608xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x608xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 608, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14950000}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x960xf32>, %3: tensor<128x1x1x960xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x960xf32>, %arg1: tensor<128x1x1x960xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x960xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 960 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x960xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x960xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x960xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x960xf32>) -> tensor<128x7x7x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x960xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x960xf32>) -> tensor<128x1x1x960xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 960, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2888959423}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x152xf32>, %3: tensor<368x1x1x152xf32>, %7: tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n  return %ret : tensor<128x7x7x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x152xf32>, %arg1: tensor<368x1x1x152xf32>, %arg2: tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x368xf32>\n    memref.copy %2, %alloc : memref<128x7x7x368xf32> to memref<128x7x7x368xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x368xf32>\n    return %3 : tensor<128x7x7x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x152xf32>) -> tensor<128x14x14x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x152xf32>) -> tensor<368x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1254262707}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x64xf32>, tensor<384x1x1x64xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x64xf32>, tensor<384x1x1x64xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x64xf32>, %3: tensor<384x1x1x64xf32>, %7: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x64xf32>, tensor<384x1x1x64xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n  return %ret : tensor<512x14x14x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x64xf32>, %arg1: tensor<384x1x1x64xf32>, %arg2: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x384xf32>\n    memref.copy %2, %alloc : memref<512x14x14x384xf32> to memref<512x14x14x384xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x384xf32>\n    return %3 : tensor<512x14x14x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x64xf32>) -> tensor<512x14x14x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x1x64xf32>) -> tensor<384x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x64xf32>, tensor<384x1x1x64xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8244762994}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x368xf32>, tensor<368x1x1x368xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x368xf32>, tensor<368x1x1x368xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x368xf32>, %3: tensor<368x1x1x368xf32>, %7: tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x368xf32>, tensor<368x1x1x368xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n  return %ret : tensor<128x7x7x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x368xf32>, %arg1: tensor<368x1x1x368xf32>, %arg2: tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x368xf32>\n    memref.copy %2, %alloc : memref<128x7x7x368xf32> to memref<128x7x7x368xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x368xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x368xf32>\n    return %3 : tensor<128x7x7x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x368xf32>) -> tensor<368x1x1x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x368xf32>, tensor<368x1x1x368xf32>) outs(%7 : tensor<128x7x7x368xf32>) -> tensor<128x7x7x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3138105097}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<96x1x1x32xf32>, %7: tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32>\n  return %ret : tensor<128x112x112x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<96x1x1x32xf32>, %arg2: tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x112x112x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x112x112x96xf32>\n    memref.copy %2, %alloc : memref<128x112x112x96xf32> to memref<128x112x112x96xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x112x112x96xf32>\n    return %3 : tensor<128x112x112x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x112x112x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x32xf32>) -> tensor<96x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x112x112x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<128x112x112x96xf32>) -> tensor<128x112x112x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x112x112x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x112x112x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13901818877}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x112xf32>, %3: tensor<256x1x1x112xf32>, %7: tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n  return %ret : tensor<128x28x28x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x112xf32>, %arg1: tensor<256x1x1x112xf32>, %arg2: tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x256xf32>\n    memref.copy %2, %alloc : memref<128x28x28x256xf32> to memref<128x28x28x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x256xf32>\n    return %3 : tensor<128x28x28x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x112xf32>) -> tensor<128x28x28x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10107941045}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1728xf32>, tensor<128x1x1x1728xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1728xf32>, tensor<128x1x1x1728xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1728xf32>, %3: tensor<128x1x1x1728xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1728xf32>, tensor<128x1x1x1728xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1728xf32>, %arg1: tensor<128x1x1x1728xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1728xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1728xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1728 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1728xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1728xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1728xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1728xf32>) -> tensor<128x7x7x1728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1728xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1728xf32>) -> tensor<128x1x1x1728xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1728xf32>, tensor<128x1x1x1728xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1728, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5218558147}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x192xf32>, %3: tensor<32x1x1x192xf32>, %7: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n  return %ret : tensor<256x1x1x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x192xf32>, %arg1: tensor<32x1x1x192xf32>, %arg2: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x32xf32>\n    memref.copy %2, %alloc : memref<256x1x1x32xf32> to memref<256x1x1x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x32xf32>\n    return %3 : tensor<256x1x1x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x192xf32>) -> tensor<32x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5540592}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x352xf32>, %3: tensor<128x1x1x352xf32>, %7: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n  return %ret : tensor<128x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x352xf32>, %arg1: tensor<128x1x1x352xf32>, %arg2: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x352xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x352xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x128xf32>\n    memref.copy %2, %alloc : memref<128x28x28x128xf32> to memref<128x28x28x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 352 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x352xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x352xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x128xf32>\n    return %3 : tensor<128x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x352xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x352xf32>) -> tensor<128x28x28x352xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x352xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x352xf32>) -> tensor<128x1x1x352xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 352, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16707099764}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x80xf32>, tensor<320x1x1x80xf32>) outs(%7 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x80xf32>, tensor<320x1x1x80xf32>) outs(%7 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x80xf32>, %3: tensor<320x1x1x80xf32>, %7: tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x80xf32>, tensor<320x1x1x80xf32>) outs(%7 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>\n  return %ret : tensor<256x1x1x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x80xf32>, %arg1: tensor<320x1x1x80xf32>, %arg2: tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x80xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x320xf32>\n    memref.copy %2, %alloc : memref<256x1x1x320xf32> to memref<256x1x1x320xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 80 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x80xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x320xf32>\n    return %3 : tensor<256x1x1x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x80xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x80xf32>) -> tensor<256x1x1x80xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x80xf32>) -> tensor<320x1x1x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x80xf32>, tensor<320x1x1x80xf32>) outs(%7 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 80, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20924743}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x696xf32>, tensor<174x1x1x696xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x696xf32>, tensor<174x1x1x696xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x696xf32>, %3: tensor<174x1x1x696xf32>, %7: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x696xf32>, tensor<174x1x1x696xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n  return %ret : tensor<512x1x1x174xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x696xf32>, %arg1: tensor<174x1x1x696xf32>, %arg2: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<174x1x1x696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x174xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x174xf32>\n    memref.copy %2, %alloc : memref<512x1x1x174xf32> to memref<512x1x1x174xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 174 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x696xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<174x1x1x696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x174xf32>\n    return %3 : tensor<512x1x1x174xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x174xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<174x1x1x696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<174x1x1x696xf32>) -> tensor<174x1x1x696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x174xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x696xf32>, tensor<174x1x1x696xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x174xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x174xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 174, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 229805976}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x768xf32>, tensor<192x1x1x768xf32>) outs(%7 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x768xf32>, tensor<192x1x1x768xf32>) outs(%7 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x768xf32>, %3: tensor<192x1x1x768xf32>, %7: tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x768xf32>, tensor<192x1x1x768xf32>) outs(%7 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n  return %ret : tensor<128x1x1x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x768xf32>, %arg1: tensor<192x1x1x768xf32>, %arg2: tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x192xf32>\n    memref.copy %2, %alloc : memref<128x1x1x192xf32> to memref<128x1x1x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x192xf32>\n    return %3 : tensor<128x1x1x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x768xf32>) -> tensor<192x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x768xf32>, tensor<192x1x1x768xf32>) outs(%7 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 70261976}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<320x1x1x960xf32>) outs(%7 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<320x1x1x960xf32>) outs(%7 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x960xf32>, %3: tensor<320x1x1x960xf32>, %7: tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<320x1x1x960xf32>) outs(%7 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>\n  return %ret : tensor<256x7x7x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x960xf32>, %arg1: tensor<320x1x1x960xf32>, %arg2: tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x960xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x320xf32>\n    memref.copy %2, %alloc : memref<256x7x7x320xf32> to memref<256x7x7x320xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 960 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x960xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x960xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x320xf32>\n    return %3 : tensor<256x7x7x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x960xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x960xf32>) -> tensor<256x7x7x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x960xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x960xf32>) -> tensor<320x1x1x960xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<320x1x1x960xf32>) outs(%7 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 960, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14453312071}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<576x1x1x54xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<576x1x1x54xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x54xf32>, %3: tensor<576x1x1x54xf32>, %7: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<576x1x1x54xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n  return %ret : tensor<512x1x1x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x54xf32>, %arg1: tensor<576x1x1x54xf32>, %arg2: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<576x1x1x54xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x54xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x576xf32>\n    memref.copy %2, %alloc : memref<512x1x1x576xf32> to memref<512x1x1x576xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 54 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x54xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<576x1x1x54xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x576xf32>\n    return %3 : tensor<512x1x1x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x54xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<576x1x1x54xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<576x1x1x54xf32>) -> tensor<576x1x1x54xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<576x1x1x54xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 54, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 46423040}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x240xf32>, tensor<560x1x1x240xf32>) outs(%7 : tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x240xf32>, tensor<560x1x1x240xf32>) outs(%7 : tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x240xf32>, %3: tensor<560x1x1x240xf32>, %7: tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x240xf32>, tensor<560x1x1x240xf32>) outs(%7 : tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32>\n  return %ret : tensor<256x14x14x560xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x240xf32>, %arg1: tensor<560x1x1x240xf32>, %arg2: tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<560x1x1x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x560xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x560xf32>\n    memref.copy %2, %alloc : memref<256x14x14x560xf32> to memref<256x14x14x560xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 560 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<560x1x1x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x560xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x560xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x560xf32>\n    return %3 : tensor<256x14x14x560xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x560xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x240xf32>) -> tensor<256x28x28x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<560x1x1x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<560x1x1x240xf32>) -> tensor<560x1x1x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x560xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x240xf32>, tensor<560x1x1x240xf32>) outs(%7 : tensor<256x14x14x560xf32>) -> tensor<256x14x14x560xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x560xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x560xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 560, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24644813065}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<192x4x4x3xf32>) outs(%7 : tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<192x4x4x3xf32>) outs(%7 : tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x224x224x3xf32>, %3: tensor<192x4x4x3xf32>, %7: tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<192x4x4x3xf32>) outs(%7 : tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32>\n  return %ret : tensor<128x56x56x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 4 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x224x224x3xf32>, %arg1: tensor<192x4x4x3xf32>, %arg2: tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x4x4x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x192xf32>\n    memref.copy %2, %alloc : memref<128x56x56x192xf32> to memref<128x56x56x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 4 {\n              affine.for %arg8 = 0 to 4 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x4x4x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x192xf32>\n    return %3 : tensor<128x56x56x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x224x224x3xf32>) -> tensor<128x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x4x4x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x4x4x3xf32>) -> tensor<192x4x4x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<128x224x224x3xf32>, tensor<192x4x4x3xf32>) outs(%7 : tensor<128x56x56x192xf32>) -> tensor<128x56x56x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 4, 1], ["%arg8", 0, 4, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 4 + %arg7", "%arg5 * 4 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12244764770}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1056xf32>, %3: tensor<128x1x1x1056xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1056xf32>, %arg1: tensor<128x1x1x1056xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1056xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1056 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1056xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1056xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1056xf32>) -> tensor<256x7x7x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1056xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1056xf32>) -> tensor<128x1x1x1056xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6359497509}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x35x35x192xf32>, %3: tensor<48x1x1x192xf32>, %7: tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32>\n  return %ret : tensor<512x35x35x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x35x35x192xf32>, %arg1: tensor<48x1x1x192xf32>, %arg2: tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x35x35x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x35x35x48xf32>\n    memref.copy %2, %alloc : memref<512x35x35x48xf32> to memref<512x35x35x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x35x35x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x35x35x48xf32>\n    return %3 : tensor<512x35x35x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x35x35x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x35x35x192xf32>) -> tensor<512x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x192xf32>) -> tensor<48x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x35x35x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x35x35x48xf32>) -> tensor<512x35x35x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x35x35x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x35x35x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21013372983}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x256xf32>, %3: tensor<608x1x1x256xf32>, %7: tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32>\n  return %ret : tensor<128x7x7x608xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x256xf32>, %arg1: tensor<608x1x1x256xf32>, %arg2: tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<608x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x608xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x608xf32>\n    memref.copy %2, %alloc : memref<128x7x7x608xf32> to memref<128x7x7x608xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 608 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<608x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x608xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x608xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x608xf32>\n    return %3 : tensor<128x7x7x608xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x608xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<608x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<608x1x1x256xf32>) -> tensor<608x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x608xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x256xf32>, tensor<608x1x1x256xf32>) outs(%7 : tensor<128x7x7x608xf32>) -> tensor<128x7x7x608xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x608xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x608xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 608, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3572895316}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1536xf32>, tensor<3x3x1536x1xf32>) outs(%7 : tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1536xf32>, tensor<3x3x1536x1xf32>) outs(%7 : tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x12x12x1536xf32>, %3: tensor<3x3x1536x1xf32>, %7: tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1536xf32>, tensor<3x3x1536x1xf32>) outs(%7 : tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32>\n  return %ret : tensor<256x10x10x1536x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x12x12x1536xf32>, %arg1: tensor<3x3x1536x1xf32>, %arg2: tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x1536x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x12x12x1536xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x10x10x1536x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x10x10x1536x1xf32>\n    memref.copy %2, %alloc : memref<256x10x10x1536x1xf32> to memref<256x10x10x1536x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 10 {\n        affine.for %arg5 = 0 to 10 {\n          affine.for %arg6 = 0 to 1536 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x12x12x1536xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x1536x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x10x10x1536x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x10x10x1536x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x10x10x1536x1xf32>\n    return %3 : tensor<256x10x10x1536x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x10x10x1536x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x12x12x1536xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x12x12x1536xf32>) -> tensor<256x12x12x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x1536x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x1536x1xf32>) -> tensor<3x3x1536x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x10x10x1536x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1536xf32>, tensor<3x3x1536x1xf32>) outs(%7 : tensor<256x10x10x1536x1xf32>) -> tensor<256x10x10x1536x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x10x10x1536x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x10x10x1536x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 10, 1], ["%arg5", 0, 10, 1], ["%arg6", 0, 1536, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 991806546}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<208x1x1x208xf32>) outs(%7 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<208x1x1x208xf32>) outs(%7 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x208xf32>, %3: tensor<208x1x1x208xf32>, %7: tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<208x1x1x208xf32>) outs(%7 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>\n  return %ret : tensor<256x14x14x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x208xf32>, %arg1: tensor<208x1x1x208xf32>, %arg2: tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x208xf32>\n    memref.copy %2, %alloc : memref<256x14x14x208xf32> to memref<256x14x14x208xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x208xf32>\n    return %3 : tensor<256x14x14x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x208xf32>) -> tensor<208x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x208xf32>, tensor<208x1x1x208xf32>) outs(%7 : tensor<256x14x14x208xf32>) -> tensor<256x14x14x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7877757795}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<48x1x1x32xf32>, %7: tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n  return %ret : tensor<512x56x56x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<48x1x1x32xf32>, %arg2: tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x48xf32>\n    memref.copy %2, %alloc : memref<512x56x56x48xf32> to memref<512x56x56x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x48xf32>\n    return %3 : tensor<512x56x56x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x32xf32>) -> tensor<48x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6961033640}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x336xf32>, %3: tensor<336x1x1x336xf32>, %7: tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>\n  return %ret : tensor<128x14x14x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x336xf32>, %arg1: tensor<336x1x1x336xf32>, %arg2: tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x336xf32>\n    memref.copy %2, %alloc : memref<128x14x14x336xf32> to memref<128x14x14x336xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x336xf32>\n    return %3 : tensor<128x14x14x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x336xf32>) -> tensor<336x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x14x14x336xf32>) -> tensor<128x14x14x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10450427583}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x264xf32>, %3: tensor<44x1x1x264xf32>, %7: tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n  return %ret : tensor<128x28x28x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x264xf32>, %arg1: tensor<44x1x1x264xf32>, %arg2: tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x264xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x264xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x44xf32>\n    memref.copy %2, %alloc : memref<128x28x28x44xf32> to memref<128x28x28x44xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 264 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x264xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x264xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x44xf32>\n    return %3 : tensor<128x28x28x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x264xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x264xf32>) -> tensor<128x28x28x264xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x264xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x264xf32>) -> tensor<44x1x1x264xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 264, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4267040052}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x2240xf32>, tensor<224x1x1x2240xf32>) outs(%7 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x2240xf32>, tensor<224x1x1x2240xf32>) outs(%7 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x2240xf32>, %3: tensor<224x1x1x2240xf32>, %7: tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x2240xf32>, tensor<224x1x1x2240xf32>) outs(%7 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>\n  return %ret : tensor<128x1x1x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x2240xf32>, %arg1: tensor<224x1x1x2240xf32>, %arg2: tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x2240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x2240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x224xf32>\n    memref.copy %2, %alloc : memref<128x1x1x224xf32> to memref<128x1x1x224xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 2240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x2240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x2240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x224xf32>\n    return %3 : tensor<128x1x1x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x2240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x2240xf32>) -> tensor<128x1x1x2240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x2240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x2240xf32>) -> tensor<224x1x1x2240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x2240xf32>, tensor<224x1x1x2240xf32>) outs(%7 : tensor<128x1x1x224xf32>) -> tensor<128x1x1x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 2240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 241258696}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x73x73x64xf32>, tensor<80x1x1x64xf32>) outs(%7 : tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x73x73x64xf32>, tensor<80x1x1x64xf32>) outs(%7 : tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x73x73x64xf32>, %3: tensor<80x1x1x64xf32>, %7: tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x73x73x64xf32>, tensor<80x1x1x64xf32>) outs(%7 : tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32>\n  return %ret : tensor<256x73x73x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x73x73x64xf32>, %arg1: tensor<80x1x1x64xf32>, %arg2: tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x73x73x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x73x73x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x73x73x80xf32>\n    memref.copy %2, %alloc : memref<256x73x73x80xf32> to memref<256x73x73x80xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 73 {\n        affine.for %arg5 = 0 to 73 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x73x73x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x73x73x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x73x73x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x73x73x80xf32>\n    return %3 : tensor<256x73x73x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x73x73x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x73x73x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x73x73x64xf32>) -> tensor<256x73x73x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x64xf32>) -> tensor<80x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x73x73x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x73x73x64xf32>, tensor<80x1x1x64xf32>) outs(%7 : tensor<256x73x73x80xf32>) -> tensor<256x73x73x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x73x73x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x73x73x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 73, 1], ["%arg5", 0, 73, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23443307580}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x288xf32>, tensor<36x1x1x288xf32>) outs(%7 : tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x288xf32>, tensor<36x1x1x288xf32>) outs(%7 : tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x288xf32>, %3: tensor<36x1x1x288xf32>, %7: tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x288xf32>, tensor<36x1x1x288xf32>) outs(%7 : tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32>\n  return %ret : tensor<256x1x1x36xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x288xf32>, %arg1: tensor<36x1x1x288xf32>, %arg2: tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<36x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x36xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x36xf32>\n    memref.copy %2, %alloc : memref<256x1x1x36xf32> to memref<256x1x1x36xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 36 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<36x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x36xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x36xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x36xf32>\n    return %3 : tensor<256x1x1x36xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x36xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x288xf32>) -> tensor<256x1x1x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<36x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<36x1x1x288xf32>) -> tensor<36x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x36xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x288xf32>, tensor<36x1x1x288xf32>) outs(%7 : tensor<256x1x1x36xf32>) -> tensor<256x1x1x36xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x36xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x36xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 36, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9535448}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x169x169x96xf32>, tensor<5x5x96x1xf32>) outs(%7 : tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x169x169x96xf32>, tensor<5x5x96x1xf32>) outs(%7 : tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x169x169x96xf32>, %3: tensor<5x5x96x1xf32>, %7: tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x169x169x96xf32>, tensor<5x5x96x1xf32>) outs(%7 : tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32>\n  return %ret : tensor<256x83x83x96x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x169x169x96xf32>, %arg1: tensor<5x5x96x1xf32>, %arg2: tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x96x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x169x169x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x83x83x96x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x83x83x96x1xf32>\n    memref.copy %2, %alloc : memref<256x83x83x96x1xf32> to memref<256x83x83x96x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x169x169x96xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x96x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x83x83x96x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x83x83x96x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x83x83x96x1xf32>\n    return %3 : tensor<256x83x83x96x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x83x83x96x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x169x169x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x169x169x96xf32>) -> tensor<256x169x169x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x96x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x96x1xf32>) -> tensor<5x5x96x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x83x83x96x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x169x169x96xf32>, tensor<5x5x96x1xf32>) outs(%7 : tensor<256x83x83x96x1xf32>) -> tensor<256x83x83x96x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x83x83x96x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x83x83x96x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 12713144618}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x44x44x84xf32>, tensor<3x3x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x44x44x84xf32>, tensor<3x3x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x44x44x84xf32>, %3: tensor<3x3x84x1xf32>, %7: tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x44x44x84xf32>, tensor<3x3x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n  return %ret : tensor<256x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x44x44x84xf32>, %arg1: tensor<3x3x84x1xf32>, %arg2: tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x44x44x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<256x42x42x84x1xf32> to memref<256x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x44x44x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x42x42x84x1xf32>\n    return %3 : tensor<256x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x44x44x84xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x44x44x84xf32>) -> tensor<256x44x44x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x84x1xf32>) -> tensor<3x3x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x44x44x84xf32>, tensor<3x3x84x1xf32>) outs(%7 : tensor<256x42x42x84x1xf32>) -> tensor<256x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 851703129}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<608x1x1x152xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<608x1x1x152xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x152xf32>, %3: tensor<608x1x1x152xf32>, %7: tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<608x1x1x152xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n  return %ret : tensor<128x1x1x608xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x152xf32>, %arg1: tensor<608x1x1x152xf32>, %arg2: tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<608x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x608xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x608xf32>\n    memref.copy %2, %alloc : memref<128x1x1x608xf32> to memref<128x1x1x608xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 608 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<608x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x608xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x608xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x608xf32>\n    return %3 : tensor<128x1x1x608xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x608xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<608x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<608x1x1x152xf32>) -> tensor<608x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x608xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x152xf32>, tensor<608x1x1x152xf32>) outs(%7 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x608xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x608xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 608, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 40953370}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x17x17x768xf32>, %3: tensor<128x1x1x768xf32>, %7: tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32>\n  return %ret : tensor<256x17x17x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x17x17x768xf32>, %arg1: tensor<128x1x1x768xf32>, %arg2: tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x17x17x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x17x17x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x17x17x128xf32>\n    memref.copy %2, %alloc : memref<256x17x17x128xf32> to memref<256x17x17x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x17x17x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x17x17x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x17x17x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x17x17x128xf32>\n    return %3 : tensor<256x17x17x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x17x17x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x17x17x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x17x17x768xf32>) -> tensor<256x17x17x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x17x17x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<256x17x17x128xf32>) -> tensor<256x17x17x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x17x17x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x17x17x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27200828555}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x888xf32>, tensor<84x1x1x888xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x888xf32>, tensor<84x1x1x888xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x888xf32>, %3: tensor<84x1x1x888xf32>, %7: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x888xf32>, tensor<84x1x1x888xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n  return %ret : tensor<512x1x1x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x888xf32>, %arg1: tensor<84x1x1x888xf32>, %arg2: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x888xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x888xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x84xf32>\n    memref.copy %2, %alloc : memref<512x1x1x84xf32> to memref<512x1x1x84xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 888 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x888xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x888xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x84xf32>\n    return %3 : tensor<512x1x1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x888xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x888xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x888xf32>) -> tensor<84x1x1x888xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x888xf32>, tensor<84x1x1x888xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 888, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 142074683}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x42x42x168xf32>, tensor<84x1x1x168xf32>) outs(%7 : tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x42x42x168xf32>, tensor<84x1x1x168xf32>) outs(%7 : tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x42x42x168xf32>, %3: tensor<84x1x1x168xf32>, %7: tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x42x42x168xf32>, tensor<84x1x1x168xf32>) outs(%7 : tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32>\n  return %ret : tensor<128x42x42x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x42x42x168xf32>, %arg1: tensor<84x1x1x168xf32>, %arg2: tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x42x42x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x42x42x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x42x42x84xf32>\n    memref.copy %2, %alloc : memref<128x42x42x84xf32> to memref<128x42x42x84xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x42x42x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x42x42x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x42x42x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x42x42x84xf32>\n    return %3 : tensor<128x42x42x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x42x42x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x42x42x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x42x42x168xf32>) -> tensor<128x42x42x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x168xf32>) -> tensor<84x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x42x42x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x42x42x168xf32>, tensor<84x1x1x168xf32>) outs(%7 : tensor<128x42x42x84xf32>) -> tensor<128x42x42x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x42x42x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x42x42x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11452813374}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x96xf32>, tensor<576x1x1x96xf32>) outs(%7 : tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x96xf32>, tensor<576x1x1x96xf32>) outs(%7 : tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x96xf32>, %3: tensor<576x1x1x96xf32>, %7: tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x96xf32>, tensor<576x1x1x96xf32>) outs(%7 : tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32>\n  return %ret : tensor<128x14x14x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x96xf32>, %arg1: tensor<576x1x1x96xf32>, %arg2: tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<576x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x576xf32>\n    memref.copy %2, %alloc : memref<128x14x14x576xf32> to memref<128x14x14x576xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<576x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x576xf32>\n    return %3 : tensor<128x14x14x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x96xf32>) -> tensor<128x14x14x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<576x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<576x1x1x96xf32>) -> tensor<576x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x96xf32>, tensor<576x1x1x96xf32>) outs(%7 : tensor<128x14x14x576xf32>) -> tensor<128x14x14x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4840518973}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x27x27x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x27x27x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x27x27x672xf32>, %3: tensor<7x7x672x1xf32>, %7: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x27x27x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n  return %ret : tensor<512x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x27x27x672xf32>, %arg1: tensor<7x7x672x1xf32>, %arg2: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x27x27x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<512x11x11x672x1xf32> to memref<512x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x27x27x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x11x11x672x1xf32>\n    return %3 : tensor<512x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x27x27x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x27x27x672xf32>) -> tensor<512x27x27x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x672x1xf32>) -> tensor<7x7x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x27x27x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 7205190414}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x24xf32>, tensor<8x1x1x24xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x24xf32>, tensor<8x1x1x24xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x24xf32>, %3: tensor<8x1x1x24xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x24xf32>, tensor<8x1x1x24xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x24xf32>, %arg1: tensor<8x1x1x24xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x24xf32>) -> tensor<256x1x1x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x24xf32>) -> tensor<8x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x24xf32>, tensor<8x1x1x24xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 115734}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x640xf32>, %3: tensor<128x1x1x640xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x640xf32>, %arg1: tensor<128x1x1x640xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x640xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x640xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 640 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x640xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x640xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x640xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x640xf32>) -> tensor<256x7x7x640xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x640xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x640xf32>) -> tensor<128x1x1x640xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 640, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3834718416}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<576x1x1x72xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<576x1x1x72xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x72xf32>, %3: tensor<576x1x1x72xf32>, %7: tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<576x1x1x72xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n  return %ret : tensor<128x1x1x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x72xf32>, %arg1: tensor<576x1x1x72xf32>, %arg2: tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<576x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x576xf32>\n    memref.copy %2, %alloc : memref<128x1x1x576xf32> to memref<128x1x1x576xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<576x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x576xf32>\n    return %3 : tensor<128x1x1x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<576x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<576x1x1x72xf32>) -> tensor<576x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x72xf32>, tensor<576x1x1x72xf32>) outs(%7 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16594348}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x232xf32>, tensor<8x1x1x232xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x232xf32>, tensor<8x1x1x232xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x232xf32>, %3: tensor<8x1x1x232xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x232xf32>, tensor<8x1x1x232xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x232xf32>, %arg1: tensor<8x1x1x232xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x232xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x232xf32>) -> tensor<8x1x1x232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x232xf32>, tensor<8x1x1x232xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3342719}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x128xf32>, %3: tensor<288x1x1x128xf32>, %7: tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32>\n  return %ret : tensor<512x14x14x288xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x128xf32>, %arg1: tensor<288x1x1x128xf32>, %arg2: tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x288xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x288xf32>\n    memref.copy %2, %alloc : memref<512x14x14x288xf32> to memref<512x14x14x288xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 288 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<288x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x288xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x288xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x288xf32>\n    return %3 : tensor<512x14x14x288xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x288xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<288x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<288x1x1x128xf32>) -> tensor<288x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x288xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<288x1x1x128xf32>) outs(%7 : tensor<512x14x14x288xf32>) -> tensor<512x14x14x288xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x288xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x288xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 288, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13174634162}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x35x35x256xf32>, %3: tensor<64x1x1x256xf32>, %7: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n  return %ret : tensor<128x35x35x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x35x35x256xf32>, %arg1: tensor<64x1x1x256xf32>, %arg2: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x35x35x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x64xf32>\n    memref.copy %2, %alloc : memref<128x35x35x64xf32> to memref<128x35x35x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x35x35x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x64xf32>\n    return %3 : tensor<128x35x35x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x35x35x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x35x35x256xf32>) -> tensor<128x35x35x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x256xf32>) -> tensor<64x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9409982836}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x88xf32>, tensor<44x1x1x88xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x88xf32>, tensor<44x1x1x88xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x88xf32>, %3: tensor<44x1x1x88xf32>, %7: tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x88xf32>, tensor<44x1x1x88xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n  return %ret : tensor<128x28x28x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x88xf32>, %arg1: tensor<44x1x1x88xf32>, %arg2: tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x88xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x44xf32>\n    memref.copy %2, %alloc : memref<128x28x28x44xf32> to memref<128x28x28x44xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 88 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x88xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x88xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x44xf32>\n    return %3 : tensor<128x28x28x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x88xf32>) -> tensor<128x28x28x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x88xf32>) -> tensor<44x1x1x88xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x88xf32>, tensor<44x1x1x88xf32>) outs(%7 : tensor<128x28x28x44xf32>) -> tensor<128x28x28x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 88, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1331069044}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x208xf32>, tensor<52x1x1x208xf32>) outs(%7 : tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x208xf32>, tensor<52x1x1x208xf32>) outs(%7 : tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x208xf32>, %3: tensor<52x1x1x208xf32>, %7: tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x208xf32>, tensor<52x1x1x208xf32>) outs(%7 : tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32>\n  return %ret : tensor<256x1x1x52xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x208xf32>, %arg1: tensor<52x1x1x208xf32>, %arg2: tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<52x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x52xf32>\n    memref.copy %2, %alloc : memref<256x1x1x52xf32> to memref<256x1x1x52xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 52 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<52x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x52xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x52xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x52xf32>\n    return %3 : tensor<256x1x1x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x208xf32>) -> tensor<256x1x1x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<52x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<52x1x1x208xf32>) -> tensor<52x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x52xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x208xf32>, tensor<52x1x1x208xf32>) outs(%7 : tensor<256x1x1x52xf32>) -> tensor<256x1x1x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 52, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9975927}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<112x1x1x448xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<112x1x1x448xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x448xf32>, %3: tensor<112x1x1x448xf32>, %7: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<112x1x1x448xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n  return %ret : tensor<512x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x448xf32>, %arg1: tensor<112x1x1x448xf32>, %arg2: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x112xf32>\n    memref.copy %2, %alloc : memref<512x1x1x112xf32> to memref<512x1x1x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x112xf32>\n    return %3 : tensor<512x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x448xf32>) -> tensor<512x1x1x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x448xf32>) -> tensor<112x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<112x1x1x448xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 94185350}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x12xf32>, tensor<48x1x1x12xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x12xf32>, tensor<48x1x1x12xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x12xf32>, %3: tensor<48x1x1x12xf32>, %7: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x12xf32>, tensor<48x1x1x12xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n  return %ret : tensor<128x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x12xf32>, %arg1: tensor<48x1x1x12xf32>, %arg2: tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x12xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x12xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x48xf32>\n    memref.copy %2, %alloc : memref<128x1x1x48xf32> to memref<128x1x1x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 12 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x12xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x12xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x48xf32>\n    return %3 : tensor<128x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x12xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x12xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x12xf32>) -> tensor<48x1x1x12xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x12xf32>, tensor<48x1x1x12xf32>) outs(%7 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 12, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 134493}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x232xf32>, tensor<58x1x1x232xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x232xf32>, tensor<58x1x1x232xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x232xf32>, %3: tensor<58x1x1x232xf32>, %7: tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x232xf32>, tensor<58x1x1x232xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n  return %ret : tensor<128x1x1x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x232xf32>, %arg1: tensor<58x1x1x232xf32>, %arg2: tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<58x1x1x232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x58xf32>\n    memref.copy %2, %alloc : memref<128x1x1x58xf32> to memref<128x1x1x58xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x232xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<58x1x1x232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x58xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x58xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x58xf32>\n    return %3 : tensor<128x1x1x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x232xf32>) -> tensor<128x1x1x232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<58x1x1x232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<58x1x1x232xf32>) -> tensor<58x1x1x232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x58xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x232xf32>, tensor<58x1x1x232xf32>) outs(%7 : tensor<128x1x1x58xf32>) -> tensor<128x1x1x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x58xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6120637}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x144xf32>, tensor<8x1x1x144xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x144xf32>, tensor<8x1x1x144xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x144xf32>, %3: tensor<8x1x1x144xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x144xf32>, tensor<8x1x1x144xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x144xf32>, %arg1: tensor<8x1x1x144xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x144xf32>) -> tensor<256x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x144xf32>) -> tensor<8x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x144xf32>, tensor<8x1x1x144xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 990020}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<256x1x1x512xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<256x1x1x512xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x512xf32>, %3: tensor<256x1x1x512xf32>, %7: tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<256x1x1x512xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n  return %ret : tensor<128x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x512xf32>, %arg1: tensor<256x1x1x512xf32>, %arg2: tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x256xf32>\n    memref.copy %2, %alloc : memref<128x14x14x256xf32> to memref<128x14x14x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x256xf32>\n    return %3 : tensor<128x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x512xf32>) -> tensor<256x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<256x1x1x512xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12242602526}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<256x1x1x32xf32>) outs(%7 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<256x1x1x32xf32>) outs(%7 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<256x1x1x32xf32>, %7: tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<256x1x1x32xf32>) outs(%7 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>\n  return %ret : tensor<128x56x56x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<256x1x1x32xf32>, %arg2: tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x256xf32>\n    memref.copy %2, %alloc : memref<128x56x56x256xf32> to memref<128x56x56x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x256xf32>\n    return %3 : tensor<128x56x56x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<256x1x1x32xf32>) outs(%7 : tensor<128x56x56x256xf32>) -> tensor<128x56x56x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9062298745}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x240xf32>, tensor<240x1x1x240xf32>) outs(%7 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x240xf32>, tensor<240x1x1x240xf32>) outs(%7 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x240xf32>, %3: tensor<240x1x1x240xf32>, %7: tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x240xf32>, tensor<240x1x1x240xf32>) outs(%7 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>\n  return %ret : tensor<128x14x14x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x240xf32>, %arg1: tensor<240x1x1x240xf32>, %arg2: tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x1x1x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x240xf32>\n    memref.copy %2, %alloc : memref<128x14x14x240xf32> to memref<128x14x14x240xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<240x1x1x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x240xf32>\n    return %3 : tensor<128x14x14x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<240x1x1x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<240x1x1x240xf32>) -> tensor<240x1x1x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x240xf32>, tensor<240x1x1x240xf32>) outs(%7 : tensor<128x14x14x240xf32>) -> tensor<128x14x14x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5279951142}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x608xf32>, %3: tensor<128x1x1x608xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x608xf32>, %arg1: tensor<128x1x1x608xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x608xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x608xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x608xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x608xf32>) -> tensor<512x7x7x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x608xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7284927067}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x320xf32>, %3: tensor<128x1x1x320xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x320xf32>, %arg1: tensor<128x1x1x320xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x320xf32>) -> tensor<128x14x14x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x320xf32>, tensor<128x1x1x320xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3788319705}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x84xf32>, tensor<336x1x1x84xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x84xf32>, tensor<336x1x1x84xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x84xf32>, %3: tensor<336x1x1x84xf32>, %7: tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x84xf32>, tensor<336x1x1x84xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n  return %ret : tensor<128x1x1x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x84xf32>, %arg1: tensor<336x1x1x84xf32>, %arg2: tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x336xf32>\n    memref.copy %2, %alloc : memref<128x1x1x336xf32> to memref<128x1x1x336xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 84 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x84xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x336xf32>\n    return %3 : tensor<128x1x1x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x84xf32>) -> tensor<128x1x1x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x84xf32>) -> tensor<336x1x1x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x84xf32>, tensor<336x1x1x84xf32>) outs(%7 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11490045}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<320x1x1x32xf32>) outs(%7 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<320x1x1x32xf32>) outs(%7 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x32xf32>, %3: tensor<320x1x1x32xf32>, %7: tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<320x1x1x32xf32>) outs(%7 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>\n  return %ret : tensor<128x1x1x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x32xf32>, %arg1: tensor<320x1x1x32xf32>, %arg2: tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x320xf32>\n    memref.copy %2, %alloc : memref<128x1x1x320xf32> to memref<128x1x1x320xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x320xf32>\n    return %3 : tensor<128x1x1x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x32xf32>) -> tensor<320x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x32xf32>, tensor<320x1x1x32xf32>) outs(%7 : tensor<128x1x1x320xf32>) -> tensor<128x1x1x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3130455}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<144x1x1x32xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<144x1x1x32xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<144x1x1x32xf32>, %7: tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<144x1x1x32xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n  return %ret : tensor<128x56x56x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<144x1x1x32xf32>, %arg2: tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x144xf32>\n    memref.copy %2, %alloc : memref<128x56x56x144xf32> to memref<128x56x56x144xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x144xf32>\n    return %3 : tensor<128x56x56x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x32xf32>) -> tensor<144x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<144x1x1x32xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5213358996}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x384xf32>, tensor<384x1x1x384xf32>) outs(%7 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x384xf32>, tensor<384x1x1x384xf32>) outs(%7 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x384xf32>, %3: tensor<384x1x1x384xf32>, %7: tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x384xf32>, tensor<384x1x1x384xf32>) outs(%7 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>\n  return %ret : tensor<512x7x7x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x384xf32>, %arg1: tensor<384x1x1x384xf32>, %arg2: tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x1x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x384xf32>\n    memref.copy %2, %alloc : memref<512x7x7x384xf32> to memref<512x7x7x384xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x1x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x384xf32>\n    return %3 : tensor<512x7x7x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x1x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x1x384xf32>) -> tensor<384x1x1x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x384xf32>, tensor<384x1x1x384xf32>) outs(%7 : tensor<512x7x7x384xf32>) -> tensor<512x7x7x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13796561523}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x96xf32>, %3: tensor<96x1x1x96xf32>, %7: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n  return %ret : tensor<512x28x28x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x96xf32>, %arg1: tensor<96x1x1x96xf32>, %arg2: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x96xf32>\n    memref.copy %2, %alloc : memref<512x28x28x96xf32> to memref<512x28x28x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x96xf32>\n    return %3 : tensor<512x28x28x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x96xf32>) -> tensor<96x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<96x1x1x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12942798355}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<768x1x1x192xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<768x1x1x192xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x192xf32>, %3: tensor<768x1x1x192xf32>, %7: tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<768x1x1x192xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n  return %ret : tensor<128x1x1x768xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x192xf32>, %arg1: tensor<768x1x1x192xf32>, %arg2: tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<768x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x768xf32>\n    memref.copy %2, %alloc : memref<128x1x1x768xf32> to memref<128x1x1x768xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 768 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<768x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x768xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x768xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x768xf32>\n    return %3 : tensor<128x1x1x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<768x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<768x1x1x192xf32>) -> tensor<768x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x192xf32>, tensor<768x1x1x192xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 768, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 66704278}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x84xf32>, tensor<888x1x1x84xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x84xf32>, tensor<888x1x1x84xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x84xf32>, %3: tensor<888x1x1x84xf32>, %7: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x84xf32>, tensor<888x1x1x84xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n  return %ret : tensor<512x1x1x888xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x84xf32>, %arg1: tensor<888x1x1x84xf32>, %arg2: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<888x1x1x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x888xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x888xf32>\n    memref.copy %2, %alloc : memref<512x1x1x888xf32> to memref<512x1x1x888xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 888 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 84 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x84xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<888x1x1x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x888xf32>\n    return %3 : tensor<512x1x1x888xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x888xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<888x1x1x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<888x1x1x84xf32>) -> tensor<888x1x1x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x888xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x84xf32>, tensor<888x1x1x84xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x888xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x888xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 888, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 121736116}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x16xf32>, tensor<96x1x1x16xf32>) outs(%7 : tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x16xf32>, tensor<96x1x1x16xf32>) outs(%7 : tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x16xf32>, %3: tensor<96x1x1x16xf32>, %7: tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x16xf32>, tensor<96x1x1x16xf32>) outs(%7 : tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32>\n  return %ret : tensor<256x112x112x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x16xf32>, %arg1: tensor<96x1x1x16xf32>, %arg2: tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x16xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x16xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x112x112x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x112x112x96xf32>\n    memref.copy %2, %alloc : memref<256x112x112x96xf32> to memref<256x112x112x96xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 16 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x16xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x16xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x112x112x96xf32>\n    return %3 : tensor<256x112x112x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x112x112x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x16xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x16xf32>) -> tensor<256x112x112x16xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x16xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x16xf32>) -> tensor<96x1x1x16xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x112x112x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x16xf32>, tensor<96x1x1x16xf32>) outs(%7 : tensor<256x112x112x96xf32>) -> tensor<256x112x112x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x112x112x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x112x112x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 16, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12269709776}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x544xf32>, %3: tensor<128x1x1x544xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x544xf32>, %arg1: tensor<128x1x1x544xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x544xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x544xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 544 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x544xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x544xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x544xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x544xf32>) -> tensor<512x7x7x544xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x544xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x544xf32>) -> tensor<128x1x1x544xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 544, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6511035374}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x32xf32>, tensor<5x5x32x1xf32>) outs(%7 : tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x32xf32>, tensor<5x5x32x1xf32>) outs(%7 : tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x115x115x32xf32>, %3: tensor<5x5x32x1xf32>, %7: tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x32xf32>, tensor<5x5x32x1xf32>) outs(%7 : tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32>\n  return %ret : tensor<256x56x56x32x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x115x115x32xf32>, %arg1: tensor<5x5x32x1xf32>, %arg2: tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x32x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x115x115x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x32x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x32x1xf32>\n    memref.copy %2, %alloc : memref<256x56x56x32x1xf32> to memref<256x56x56x32x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x115x115x32xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x32x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x32x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x32x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x32x1xf32>\n    return %3 : tensor<256x56x56x32x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x32x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x115x115x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x115x115x32xf32>) -> tensor<256x115x115x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x32x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x32x1xf32>) -> tensor<5x5x32x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x32x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x32xf32>, tensor<5x5x32x1xf32>) outs(%7 : tensor<256x56x56x32x1xf32>) -> tensor<256x56x56x32x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x32x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x32x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1921050774}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x160xf32>, %3: tensor<384x1x1x160xf32>, %7: tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32>\n  return %ret : tensor<256x7x7x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x160xf32>, %arg1: tensor<384x1x1x160xf32>, %arg2: tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x384xf32>\n    memref.copy %2, %alloc : memref<256x7x7x384xf32> to memref<256x7x7x384xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x384xf32>\n    return %3 : tensor<256x7x7x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x160xf32>) -> tensor<256x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x1x160xf32>) -> tensor<384x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<256x7x7x384xf32>) -> tensor<256x7x7x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2781173844}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<3024x1x1x308xf32>) outs(%7 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<3024x1x1x308xf32>) outs(%7 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x308xf32>, %3: tensor<3024x1x1x308xf32>, %7: tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<3024x1x1x308xf32>) outs(%7 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>\n  return %ret : tensor<512x1x1x3024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x308xf32>, %arg1: tensor<3024x1x1x308xf32>, %arg2: tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3024x1x1x308xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x308xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x3024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x3024xf32>\n    memref.copy %2, %alloc : memref<512x1x1x3024xf32> to memref<512x1x1x3024xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 3024 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 308 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x308xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<3024x1x1x308xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x3024xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x3024xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x3024xf32>\n    return %3 : tensor<512x1x1x3024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x3024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x308xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3024x1x1x308xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3024x1x1x308xf32>) -> tensor<3024x1x1x308xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x3024xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<3024x1x1x308xf32>) outs(%7 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x3024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x3024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 3024, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 308, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1725043106}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<104x1x1x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<104x1x1x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x104xf32>, %3: tensor<104x1x1x104xf32>, %7: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<104x1x1x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n  return %ret : tensor<512x28x28x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x104xf32>, %arg1: tensor<104x1x1x104xf32>, %arg2: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x104xf32>\n    memref.copy %2, %alloc : memref<512x28x28x104xf32> to memref<512x28x28x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x104xf32>\n    return %3 : tensor<512x28x28x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x104xf32>) -> tensor<104x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<104x1x1x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15132008917}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<8x1x1x128xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<8x1x1x128xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x128xf32>, %3: tensor<8x1x1x128xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<8x1x1x128xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x128xf32>, %arg1: tensor<8x1x1x128xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x128xf32>) -> tensor<256x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x128xf32>) -> tensor<8x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x128xf32>, tensor<8x1x1x128xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 866627}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1024xf32>, tensor<3x3x1024x1xf32>) outs(%7 : tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1024xf32>, tensor<3x3x1024x1xf32>) outs(%7 : tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x12x12x1024xf32>, %3: tensor<3x3x1024x1xf32>, %7: tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1024xf32>, tensor<3x3x1024x1xf32>) outs(%7 : tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32>\n  return %ret : tensor<256x10x10x1024x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x12x12x1024xf32>, %arg1: tensor<3x3x1024x1xf32>, %arg2: tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x1024x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x12x12x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x10x10x1024x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x10x10x1024x1xf32>\n    memref.copy %2, %alloc : memref<256x10x10x1024x1xf32> to memref<256x10x10x1024x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 10 {\n        affine.for %arg5 = 0 to 10 {\n          affine.for %arg6 = 0 to 1024 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x12x12x1024xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x1024x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x10x10x1024x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x10x10x1024x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x10x10x1024x1xf32>\n    return %3 : tensor<256x10x10x1024x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x10x10x1024x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x12x12x1024xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x12x12x1024xf32>) -> tensor<256x12x12x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x1024x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x1024x1xf32>) -> tensor<3x3x1024x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x10x10x1024x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x12x12x1024xf32>, tensor<3x3x1024x1xf32>) outs(%7 : tensor<256x10x10x1024x1xf32>) -> tensor<256x10x10x1024x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x10x10x1024x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x10x10x1024x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 10, 1], ["%arg5", 0, 10, 1], ["%arg6", 0, 1024, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 730869998}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<152x1x1x608xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<152x1x1x608xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x608xf32>, %3: tensor<152x1x1x608xf32>, %7: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<152x1x1x608xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n  return %ret : tensor<512x1x1x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x608xf32>, %arg1: tensor<152x1x1x608xf32>, %arg2: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x608xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x152xf32>\n    memref.copy %2, %alloc : memref<512x1x1x152xf32> to memref<512x1x1x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x608xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x608xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x152xf32>\n    return %3 : tensor<512x1x1x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x608xf32>) -> tensor<512x1x1x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x608xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x608xf32>) -> tensor<152x1x1x608xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<152x1x1x608xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 174720572}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<24x1x1x32xf32>, %7: tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32>\n  return %ret : tensor<256x56x56x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<24x1x1x32xf32>, %arg2: tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x24xf32>\n    memref.copy %2, %alloc : memref<256x56x56x24xf32> to memref<256x56x56x24xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x24xf32>\n    return %3 : tensor<256x56x56x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x32xf32>) -> tensor<24x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<256x56x56x24xf32>) -> tensor<256x56x56x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1714325287}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x64xf32>, %3: tensor<128x1x1x64xf32>, %7: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n  return %ret : tensor<256x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x64xf32>, %arg1: tensor<128x1x1x64xf32>, %arg2: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x128xf32>\n    memref.copy %2, %alloc : memref<256x28x28x128xf32> to memref<256x28x28x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x128xf32>\n    return %3 : tensor<256x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x64xf32>) -> tensor<128x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x64xf32>, tensor<128x1x1x64xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5458056327}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<88x1x1x352xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<88x1x1x352xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x352xf32>, %3: tensor<88x1x1x352xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<88x1x1x352xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x352xf32>, %arg1: tensor<88x1x1x352xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x352xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x352xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 352 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x352xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x352xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x352xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x352xf32>) -> tensor<512x14x14x352xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x352xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x352xf32>) -> tensor<88x1x1x352xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<88x1x1x352xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 352, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11471341313}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x96xf32>, %3: tensor<240x1x1x96xf32>, %7: tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n  return %ret : tensor<128x28x28x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x96xf32>, %arg1: tensor<240x1x1x96xf32>, %arg2: tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x240xf32>\n    memref.copy %2, %alloc : memref<128x28x28x240xf32> to memref<128x28x28x240xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<240x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x240xf32>\n    return %3 : tensor<128x28x28x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x96xf32>) -> tensor<128x28x28x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<240x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<240x1x1x96xf32>) -> tensor<240x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8008428522}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<216x1x1x54xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<216x1x1x54xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x54xf32>, %3: tensor<216x1x1x54xf32>, %7: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<216x1x1x54xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n  return %ret : tensor<512x1x1x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x54xf32>, %arg1: tensor<216x1x1x54xf32>, %arg2: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<216x1x1x54xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x54xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x216xf32>\n    memref.copy %2, %alloc : memref<512x1x1x216xf32> to memref<512x1x1x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 54 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x54xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<216x1x1x54xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x216xf32>\n    return %3 : tensor<512x1x1x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x54xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x54xf32>) -> tensor<512x1x1x54xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<216x1x1x54xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<216x1x1x54xf32>) -> tensor<216x1x1x54xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x54xf32>, tensor<216x1x1x54xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 54, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17107645}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x115x115x11xf32>, %3: tensor<5x5x11x1xf32>, %7: tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n  return %ret : tensor<256x56x56x11x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x115x115x11xf32>, %arg1: tensor<5x5x11x1xf32>, %arg2: tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x11x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x115x115x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x11x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x11x1xf32>\n    memref.copy %2, %alloc : memref<256x56x56x11x1xf32> to memref<256x56x56x11x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x115x115x11xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x11x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x11x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x11x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x11x1xf32>\n    return %3 : tensor<256x56x56x11x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x11x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x115x115x11xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x115x115x11xf32>) -> tensor<256x115x115x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x11x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x11x1xf32>) -> tensor<5x5x11x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x11x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x115x115x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x11x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x11x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 632990824}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1088xf32>, tensor<272x1x1x1088xf32>) outs(%7 : tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1088xf32>, tensor<272x1x1x1088xf32>) outs(%7 : tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x1088xf32>, %3: tensor<272x1x1x1088xf32>, %7: tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1088xf32>, tensor<272x1x1x1088xf32>) outs(%7 : tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32>\n  return %ret : tensor<128x1x1x272xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x1088xf32>, %arg1: tensor<272x1x1x1088xf32>, %arg2: tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<272x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x272xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x272xf32>\n    memref.copy %2, %alloc : memref<128x1x1x272xf32> to memref<128x1x1x272xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 272 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<272x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x272xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x272xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x272xf32>\n    return %3 : tensor<128x1x1x272xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x272xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x1088xf32>) -> tensor<128x1x1x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<272x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<272x1x1x1088xf32>) -> tensor<272x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x272xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1088xf32>, tensor<272x1x1x1088xf32>) outs(%7 : tensor<128x1x1x272xf32>) -> tensor<128x1x1x272xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x272xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x272xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 272, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 141318420}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1232xf32>, tensor<308x1x1x1232xf32>) outs(%7 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1232xf32>, tensor<308x1x1x1232xf32>) outs(%7 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x1232xf32>, %3: tensor<308x1x1x1232xf32>, %7: tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1232xf32>, tensor<308x1x1x1232xf32>) outs(%7 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>\n  return %ret : tensor<128x1x1x308xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x1232xf32>, %arg1: tensor<308x1x1x1232xf32>, %arg2: tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<308x1x1x1232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x1232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x308xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x308xf32>\n    memref.copy %2, %alloc : memref<128x1x1x308xf32> to memref<128x1x1x308xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 308 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x1232xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<308x1x1x1232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x308xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x308xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x308xf32>\n    return %3 : tensor<128x1x1x308xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x308xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x1232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x1232xf32>) -> tensor<128x1x1x1232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<308x1x1x1232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<308x1x1x1232xf32>) -> tensor<308x1x1x1232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x308xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x1232xf32>, tensor<308x1x1x1232xf32>) outs(%7 : tensor<128x1x1x308xf32>) -> tensor<128x1x1x308xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x308xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x308xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 308, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 181626060}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x528xf32>, %3: tensor<88x1x1x528xf32>, %7: tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32>\n  return %ret : tensor<512x7x7x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x528xf32>, %arg1: tensor<88x1x1x528xf32>, %arg2: tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x88xf32>\n    memref.copy %2, %alloc : memref<512x7x7x88xf32> to memref<512x7x7x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x88xf32>\n    return %3 : tensor<512x7x7x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x528xf32>) -> tensor<88x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x7x7x88xf32>) -> tensor<512x7x7x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4338454698}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<96x1x1x192xf32>) outs(%7 : tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<96x1x1x192xf32>) outs(%7 : tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x35x35x192xf32>, %3: tensor<96x1x1x192xf32>, %7: tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<96x1x1x192xf32>) outs(%7 : tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32>\n  return %ret : tensor<256x35x35x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x35x35x192xf32>, %arg1: tensor<96x1x1x192xf32>, %arg2: tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x35x35x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x35x35x96xf32>\n    memref.copy %2, %alloc : memref<256x35x35x96xf32> to memref<256x35x35x96xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x35x35x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x35x35x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x35x35x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x35x35x96xf32>\n    return %3 : tensor<256x35x35x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x35x35x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x35x35x192xf32>) -> tensor<256x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x192xf32>) -> tensor<96x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x35x35x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<96x1x1x192xf32>) outs(%7 : tensor<256x35x35x96xf32>) -> tensor<256x35x35x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x35x35x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x35x35x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21024211989}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<48x1x1x48xf32>) outs(%7 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<48x1x1x48xf32>) outs(%7 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x48xf32>, %3: tensor<48x1x1x48xf32>, %7: tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<48x1x1x48xf32>) outs(%7 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n  return %ret : tensor<128x56x56x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x48xf32>, %arg1: tensor<48x1x1x48xf32>, %arg2: tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x48xf32>\n    memref.copy %2, %alloc : memref<128x56x56x48xf32> to memref<128x56x56x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x48xf32>\n    return %3 : tensor<128x56x56x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x48xf32>) -> tensor<48x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x48xf32>, tensor<48x1x1x48xf32>) outs(%7 : tensor<128x56x56x48xf32>) -> tensor<128x56x56x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2995407246}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<18x1x1x72xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<18x1x1x72xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x72xf32>, %3: tensor<18x1x1x72xf32>, %7: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<18x1x1x72xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n  return %ret : tensor<512x1x1x18xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x72xf32>, %arg1: tensor<18x1x1x72xf32>, %arg2: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<18x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x18xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x18xf32>\n    memref.copy %2, %alloc : memref<512x1x1x18xf32> to memref<512x1x1x18xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 18 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<18x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x18xf32>\n    return %3 : tensor<512x1x1x18xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x18xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<18x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<18x1x1x72xf32>) -> tensor<18x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<18x1x1x72xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x18xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x18xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 18, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2027290}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<176x1x1x704xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<176x1x1x704xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x704xf32>, %3: tensor<176x1x1x704xf32>, %7: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<176x1x1x704xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n  return %ret : tensor<512x7x7x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x704xf32>, %arg1: tensor<176x1x1x704xf32>, %arg2: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<176x1x1x704xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x704xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176xf32> to memref<512x7x7x176xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 704 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x704xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<176x1x1x704xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176xf32>\n    return %3 : tensor<512x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x704xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x704xf32>) -> tensor<512x7x7x704xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<176x1x1x704xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<176x1x1x704xf32>) -> tensor<176x1x1x704xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<176x1x1x704xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 704, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11617491292}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<512x1x1x128xf32>, %7: tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n  return %ret : tensor<512x1x1x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<512x1x1x128xf32>, %arg2: tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x512xf32>\n    memref.copy %2, %alloc : memref<512x1x1x512xf32> to memref<512x1x1x512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x512xf32>\n    return %3 : tensor<512x1x1x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 114033497}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<128x1x1x8xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<128x1x1x8xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<128x1x1x8xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<128x1x1x8xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<128x1x1x8xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<128x1x1x8xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 849095}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x240xf32>, %3: tensor<528x1x1x240xf32>, %7: tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32>\n  return %ret : tensor<256x14x14x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x240xf32>, %arg1: tensor<528x1x1x240xf32>, %arg2: tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<528x1x1x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x528xf32>\n    memref.copy %2, %alloc : memref<256x14x14x528xf32> to memref<256x14x14x528xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<528x1x1x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x528xf32>\n    return %3 : tensor<256x14x14x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x240xf32>) -> tensor<256x14x14x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<528x1x1x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<528x1x1x240xf32>) -> tensor<528x1x1x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<256x14x14x528xf32>) -> tensor<256x14x14x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23237399033}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x33x33x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x33x33x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x33x33x88xf32>, %3: tensor<7x7x88x1xf32>, %7: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x33x33x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n  return %ret : tensor<128x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x33x33x88xf32>, %arg1: tensor<7x7x88x1xf32>, %arg2: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x33x33x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<128x14x14x88x1xf32> to memref<128x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x33x33x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x88x1xf32>\n    return %3 : tensor<128x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x33x33x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x33x33x88xf32>) -> tensor<128x33x33x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x88x1xf32>) -> tensor<7x7x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x33x33x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 355587193}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x104xf32>, %3: tensor<208x1x1x104xf32>, %7: tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>\n  return %ret : tensor<512x14x14x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x104xf32>, %arg1: tensor<208x1x1x104xf32>, %arg2: tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x208xf32>\n    memref.copy %2, %alloc : memref<512x14x14x208xf32> to memref<512x14x14x208xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x208xf32>\n    return %3 : tensor<512x14x14x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x104xf32>) -> tensor<208x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x104xf32>, tensor<208x1x1x104xf32>) outs(%7 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7556535024}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x928xf32>, tensor<128x1x1x928xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x928xf32>, tensor<128x1x1x928xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x928xf32>, %3: tensor<128x1x1x928xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x928xf32>, tensor<128x1x1x928xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x928xf32>, %arg1: tensor<128x1x1x928xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x928xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x928xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 928 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x928xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x928xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x928xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x928xf32>) -> tensor<512x7x7x928xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x928xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x928xf32>) -> tensor<128x1x1x928xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x928xf32>, tensor<128x1x1x928xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 928, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11166949483}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<96x1x1x576xf32>) outs(%7 : tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<96x1x1x576xf32>) outs(%7 : tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x576xf32>, %3: tensor<96x1x1x576xf32>, %7: tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<96x1x1x576xf32>) outs(%7 : tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32>\n  return %ret : tensor<256x14x14x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x576xf32>, %arg1: tensor<96x1x1x576xf32>, %arg2: tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x96xf32>\n    memref.copy %2, %alloc : memref<256x14x14x96xf32> to memref<256x14x14x96xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x96xf32>\n    return %3 : tensor<256x14x14x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x576xf32>) -> tensor<256x14x14x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x576xf32>) -> tensor<96x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<96x1x1x576xf32>) outs(%7 : tensor<256x14x14x96xf32>) -> tensor<256x14x14x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10353271967}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x80xf32>, tensor<768x1x1x80xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x80xf32>, tensor<768x1x1x80xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x80xf32>, %3: tensor<768x1x1x80xf32>, %7: tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x80xf32>, tensor<768x1x1x80xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n  return %ret : tensor<128x1x1x768xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x80xf32>, %arg1: tensor<768x1x1x80xf32>, %arg2: tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<768x1x1x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x80xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x768xf32>\n    memref.copy %2, %alloc : memref<128x1x1x768xf32> to memref<128x1x1x768xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 768 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 80 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x80xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<768x1x1x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x768xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x768xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x768xf32>\n    return %3 : tensor<128x1x1x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x80xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x80xf32>) -> tensor<128x1x1x80xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<768x1x1x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<768x1x1x80xf32>) -> tensor<768x1x1x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x80xf32>, tensor<768x1x1x80xf32>) outs(%7 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 768, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 80, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25092932}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<48x1x1x32xf32>, %7: tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32>\n  return %ret : tensor<128x112x112x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<48x1x1x32xf32>, %arg2: tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x112x112x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x112x112x48xf32>\n    memref.copy %2, %alloc : memref<128x112x112x48xf32> to memref<128x112x112x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x112x112x48xf32>\n    return %3 : tensor<128x112x112x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x112x112x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x32xf32>) -> tensor<48x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x112x112x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<48x1x1x32xf32>) outs(%7 : tensor<128x112x112x48xf32>) -> tensor<128x112x112x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x112x112x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x112x112x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6952253378}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x672xf32>, tensor<672x1x1x672xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x672xf32>, tensor<672x1x1x672xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x672xf32>, %3: tensor<672x1x1x672xf32>, %7: tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x672xf32>, tensor<672x1x1x672xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n  return %ret : tensor<128x7x7x672xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x672xf32>, %arg1: tensor<672x1x1x672xf32>, %arg2: tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<672x1x1x672xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x672xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x672xf32>\n    memref.copy %2, %alloc : memref<128x7x7x672xf32> to memref<128x7x7x672xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 672 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x672xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<672x1x1x672xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x672xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x672xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x672xf32>\n    return %3 : tensor<128x7x7x672xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x672xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<672x1x1x672xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<672x1x1x672xf32>) -> tensor<672x1x1x672xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x672xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x672xf32>, tensor<672x1x1x672xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x672xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x672xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10576562549}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x96xf32>, tensor<24x1x1x96xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x96xf32>, tensor<24x1x1x96xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x96xf32>, %3: tensor<24x1x1x96xf32>, %7: tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x96xf32>, tensor<24x1x1x96xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n  return %ret : tensor<512x56x56x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x96xf32>, %arg1: tensor<24x1x1x96xf32>, %arg2: tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x24xf32>\n    memref.copy %2, %alloc : memref<512x56x56x24xf32> to memref<512x56x56x24xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x24xf32>\n    return %3 : tensor<512x56x56x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x96xf32>) -> tensor<24x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x96xf32>, tensor<24x1x1x96xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12902633256}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<72x1x1x32xf32>, %7: tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32>\n  return %ret : tensor<256x112x112x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<72x1x1x32xf32>, %arg2: tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x112x112x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x112x112x72xf32>\n    memref.copy %2, %alloc : memref<256x112x112x72xf32> to memref<256x112x112x72xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x112x112x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x112x112x72xf32>\n    return %3 : tensor<256x112x112x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x112x112x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x32xf32>) -> tensor<72x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x112x112x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x112x112x72xf32>) -> tensor<256x112x112x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x112x112x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x112x112x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20435891297}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<1392x1x1x174xf32>) outs(%7 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<1392x1x1x174xf32>) outs(%7 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x174xf32>, %3: tensor<1392x1x1x174xf32>, %7: tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<1392x1x1x174xf32>) outs(%7 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>\n  return %ret : tensor<256x1x1x1392xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x174xf32>, %arg1: tensor<1392x1x1x174xf32>, %arg2: tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1392x1x1x174xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x174xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x1392xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x1392xf32>\n    memref.copy %2, %alloc : memref<256x1x1x1392xf32> to memref<256x1x1x1392xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1392 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 174 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x174xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1392x1x1x174xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1392xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x1392xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x1392xf32>\n    return %3 : tensor<256x1x1x1392xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x1392xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x174xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1392x1x1x174xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1392x1x1x174xf32>) -> tensor<1392x1x1x174xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x1392xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<1392x1x1x174xf32>) outs(%7 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x1392xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x1392xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1392, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 174, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 216413265}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1280xf32>, %3: tensor<128x1x1x1280xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1280xf32>, %arg1: tensor<128x1x1x1280xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1280xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1280 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1280xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1280xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1280xf32>) -> tensor<128x14x14x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1280xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1280xf32>) -> tensor<128x1x1x1280xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15439557600}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x76x76x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x76x76x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x76x76x256xf32>, %3: tensor<3x3x256x1xf32>, %7: tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x76x76x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32>\n  return %ret : tensor<256x74x74x256x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x76x76x256xf32>, %arg1: tensor<3x3x256x1xf32>, %arg2: tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x256x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x76x76x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x74x74x256x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x74x74x256x1xf32>\n    memref.copy %2, %alloc : memref<256x74x74x256x1xf32> to memref<256x74x74x256x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 74 {\n        affine.for %arg5 = 0 to 74 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x76x76x256xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x256x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x74x74x256x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x74x74x256x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x74x74x256x1xf32>\n    return %3 : tensor<256x74x74x256x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x74x74x256x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x76x76x256xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x76x76x256xf32>) -> tensor<256x76x76x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x256x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x256x1xf32>) -> tensor<3x3x256x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x74x74x256x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x76x76x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<256x74x74x256x1xf32>) -> tensor<256x74x74x256x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x74x74x256x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x74x74x256x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 74, 1], ["%arg5", 0, 74, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 8659099974}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x128xf32>, %3: tensor<512x1x1x128xf32>, %7: tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>\n  return %ret : tensor<512x14x14x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x128xf32>, %arg1: tensor<512x1x1x128xf32>, %arg2: tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x512xf32>\n    memref.copy %2, %alloc : memref<512x14x14x512xf32> to memref<512x14x14x512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x512xf32>\n    return %3 : tensor<512x14x14x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x128xf32>, tensor<512x1x1x128xf32>) outs(%7 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23335199688}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x144xf32>, tensor<32x1x1x144xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x144xf32>, tensor<32x1x1x144xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x144xf32>, %3: tensor<32x1x1x144xf32>, %7: tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x144xf32>, tensor<32x1x1x144xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n  return %ret : tensor<128x28x28x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x144xf32>, %arg1: tensor<32x1x1x144xf32>, %arg2: tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x32xf32>\n    memref.copy %2, %alloc : memref<128x28x28x32xf32> to memref<128x28x28x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x32xf32>\n    return %3 : tensor<128x28x28x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x144xf32>) -> tensor<128x28x28x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x144xf32>) -> tensor<32x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x144xf32>, tensor<32x1x1x144xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1653156588}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<12x1x1x104xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<12x1x1x104xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x104xf32>, %3: tensor<12x1x1x104xf32>, %7: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<12x1x1x104xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n  return %ret : tensor<512x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x104xf32>, %arg1: tensor<12x1x1x104xf32>, %arg2: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x12xf32>\n    memref.copy %2, %alloc : memref<512x1x1x12xf32> to memref<512x1x1x12xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x12xf32>\n    return %3 : tensor<512x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x104xf32>) -> tensor<12x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<12x1x1x104xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2091913}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x23x23x336xf32>, tensor<3x3x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x23x23x336xf32>, tensor<3x3x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x23x23x336xf32>, %3: tensor<3x3x336x1xf32>, %7: tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x23x23x336xf32>, tensor<3x3x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n  return %ret : tensor<256x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x23x23x336xf32>, %arg1: tensor<3x3x336x1xf32>, %arg2: tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x23x23x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<256x21x21x336x1xf32> to memref<256x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x23x23x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x21x21x336x1xf32>\n    return %3 : tensor<256x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x23x23x336xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x23x23x336xf32>) -> tensor<256x23x23x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x336x1xf32>) -> tensor<3x3x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x23x23x336xf32>, tensor<3x3x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 871521450}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<224x1x1x32xf32>) outs(%7 : tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<224x1x1x32xf32>) outs(%7 : tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<224x1x1x32xf32>, %7: tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<224x1x1x32xf32>) outs(%7 : tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32>\n  return %ret : tensor<128x56x56x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<224x1x1x32xf32>, %arg2: tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x224xf32>\n    memref.copy %2, %alloc : memref<128x56x56x224xf32> to memref<128x56x56x224xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x224xf32>\n    return %3 : tensor<128x56x56x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x32xf32>) -> tensor<224x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<224x1x1x32xf32>) outs(%7 : tensor<128x56x56x224xf32>) -> tensor<128x56x56x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7996104029}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x80xf32>, tensor<240x1x1x80xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x80xf32>, tensor<240x1x1x80xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x80xf32>, %3: tensor<240x1x1x80xf32>, %7: tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x80xf32>, tensor<240x1x1x80xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n  return %ret : tensor<128x28x28x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x80xf32>, %arg1: tensor<240x1x1x80xf32>, %arg2: tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x1x1x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x80xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x240xf32>\n    memref.copy %2, %alloc : memref<128x28x28x240xf32> to memref<128x28x28x240xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 80 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x80xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<240x1x1x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x240xf32>\n    return %3 : tensor<128x28x28x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x80xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x80xf32>) -> tensor<128x56x56x80xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<240x1x1x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<240x1x1x80xf32>) -> tensor<240x1x1x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x80xf32>, tensor<240x1x1x80xf32>) outs(%7 : tensor<128x28x28x240xf32>) -> tensor<128x28x28x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 80, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6534513667}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<54x1x1x576xf32>) outs(%7 : tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<54x1x1x576xf32>) outs(%7 : tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x576xf32>, %3: tensor<54x1x1x576xf32>, %7: tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<54x1x1x576xf32>) outs(%7 : tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32>\n  return %ret : tensor<128x1x1x54xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x576xf32>, %arg1: tensor<54x1x1x576xf32>, %arg2: tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<54x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x54xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x54xf32>\n    memref.copy %2, %alloc : memref<128x1x1x54xf32> to memref<128x1x1x54xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 54 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<54x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x54xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x54xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x54xf32>\n    return %3 : tensor<128x1x1x54xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x54xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<54x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<54x1x1x576xf32>) -> tensor<54x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x54xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<54x1x1x576xf32>) outs(%7 : tensor<128x1x1x54xf32>) -> tensor<128x1x1x54xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x54xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x54xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 54, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14674818}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x384xf32>, tensor<128x1x1x384xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x384xf32>, tensor<128x1x1x384xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x384xf32>, %3: tensor<128x1x1x384xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x384xf32>, tensor<128x1x1x384xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x384xf32>, %arg1: tensor<128x1x1x384xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x384xf32>) -> tensor<128x1x1x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x384xf32>, tensor<128x1x1x384xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18271466408}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x800xf32>, %3: tensor<128x1x1x800xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x800xf32>, %arg1: tensor<128x1x1x800xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x800xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x800xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 800 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x800xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x800xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x800xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x800xf32>) -> tensor<256x14x14x800xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x800xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x800xf32>) -> tensor<128x1x1x800xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 800, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19230927769}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x392xf32>, tensor<784x1x1x392xf32>) outs(%7 : tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x392xf32>, tensor<784x1x1x392xf32>) outs(%7 : tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x392xf32>, %3: tensor<784x1x1x392xf32>, %7: tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x392xf32>, tensor<784x1x1x392xf32>) outs(%7 : tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32>\n  return %ret : tensor<128x14x14x784xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x392xf32>, %arg1: tensor<784x1x1x392xf32>, %arg2: tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<784x1x1x392xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x392xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x784xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x784xf32>\n    memref.copy %2, %alloc : memref<128x14x14x784xf32> to memref<128x14x14x784xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 784 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 392 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x392xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<784x1x1x392xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x784xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x784xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x784xf32>\n    return %3 : tensor<128x14x14x784xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x784xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x392xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x392xf32>) -> tensor<128x28x28x392xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<784x1x1x392xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<784x1x1x392xf32>) -> tensor<784x1x1x392xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x784xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x392xf32>, tensor<784x1x1x392xf32>) outs(%7 : tensor<128x14x14x784xf32>) -> tensor<128x14x14x784xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x784xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x784xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 784, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 392, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28551257708}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1248xf32>, %3: tensor<128x1x1x1248xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1248xf32>, %arg1: tensor<128x1x1x1248xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1248xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1248xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1248 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1248xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1248xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1248xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1248xf32>) -> tensor<128x14x14x1248xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1248xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1248xf32>) -> tensor<128x1x1x1248xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1248xf32>, tensor<128x1x1x1248xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1248, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15056412612}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x256xf32>, %3: tensor<128x1x1x256xf32>, %7: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n  return %ret : tensor<256x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x256xf32>, %arg1: tensor<128x1x1x256xf32>, %arg2: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x128xf32>\n    memref.copy %2, %alloc : memref<256x28x28x128xf32> to memref<256x28x28x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x128xf32>\n    return %3 : tensor<256x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x256xf32>) -> tensor<256x56x56x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24110134015}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x44xf32>, tensor<3x3x44x1xf32>) outs(%7 : tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x44xf32>, tensor<3x3x44x1xf32>) outs(%7 : tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x30x30x44xf32>, %3: tensor<3x3x44x1xf32>, %7: tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x44xf32>, tensor<3x3x44x1xf32>) outs(%7 : tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32>\n  return %ret : tensor<512x28x28x44x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x30x30x44xf32>, %arg1: tensor<3x3x44x1xf32>, %arg2: tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x44x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x30x30x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x44x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x44x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x44x1xf32> to memref<512x28x28x44x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x30x30x44xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x44x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x44x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x44x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x44x1xf32>\n    return %3 : tensor<512x28x28x44x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x44x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x30x30x44xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x30x30x44xf32>) -> tensor<512x30x30x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x44x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x44x1xf32>) -> tensor<3x3x44x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x44x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x44xf32>, tensor<3x3x44x1xf32>) outs(%7 : tensor<512x28x28x44x1xf32>) -> tensor<512x28x28x44x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x44x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x44x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 395973240}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x288xf32>, tensor<72x1x1x288xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x288xf32>, tensor<72x1x1x288xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x288xf32>, %3: tensor<72x1x1x288xf32>, %7: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x288xf32>, tensor<72x1x1x288xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n  return %ret : tensor<512x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x288xf32>, %arg1: tensor<72x1x1x288xf32>, %arg2: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x72xf32>\n    memref.copy %2, %alloc : memref<512x1x1x72xf32> to memref<512x1x1x72xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x72xf32>\n    return %3 : tensor<512x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x288xf32>) -> tensor<512x1x1x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x288xf32>) -> tensor<72x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x288xf32>, tensor<72x1x1x288xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 38147361}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<128x1x1x192xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<128x1x1x192xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x192xf32>, %3: tensor<128x1x1x192xf32>, %7: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<128x1x1x192xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n  return %ret : tensor<128x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x192xf32>, %arg1: tensor<128x1x1x192xf32>, %arg2: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x128xf32>\n    memref.copy %2, %alloc : memref<128x28x28x128xf32> to memref<128x28x28x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x128xf32>\n    return %3 : tensor<128x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x192xf32>) -> tensor<128x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<128x1x1x192xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8938088142}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%padded, %3 : tensor<512x225x225x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%padded, %3 : tensor<512x225x225x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x225x225x3xf32>, %3: tensor<32x3x3x3xf32>, %7: tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%padded, %3 : tensor<512x225x225x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n  return %ret : tensor<512x112x112x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x225x225x3xf32>, %arg1: tensor<32x3x3x3xf32>, %arg2: tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x225x225x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x112x112x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x112x112x32xf32>\n    memref.copy %2, %alloc : memref<512x112x112x32xf32> to memref<512x112x112x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x225x225x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x112x112x32xf32>\n    return %3 : tensor<512x112x112x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x112x112x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x225x225x3xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x225x225x3xf32>) -> tensor<512x225x225x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x3xf32>) -> tensor<32x3x3x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%padded, %3 : tensor<512x225x225x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x112x112x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x112x112x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16366796434}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x32xf32>, %3: tensor<192x1x1x32xf32>, %7: tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32>\n  return %ret : tensor<256x28x28x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x32xf32>, %arg1: tensor<192x1x1x32xf32>, %arg2: tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x192xf32>\n    memref.copy %2, %alloc : memref<256x28x28x192xf32> to memref<256x28x28x192xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x192xf32>\n    return %3 : tensor<256x28x28x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x32xf32>) -> tensor<192x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x28x28x192xf32>) -> tensor<256x28x28x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3472738199}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x32xf32>, %3: tensor<32x1x1x32xf32>, %7: tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>\n  return %ret : tensor<128x56x56x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x32xf32>, %arg1: tensor<32x1x1x32xf32>, %arg2: tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x32xf32>\n    memref.copy %2, %alloc : memref<128x56x56x32xf32> to memref<128x56x56x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x32xf32>\n    return %3 : tensor<128x56x56x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x32xf32>) -> tensor<32x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<128x56x56x32xf32>) -> tensor<128x56x56x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1299405040}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x56xf32>, %3: tensor<152x1x1x56xf32>, %7: tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32>\n  return %ret : tensor<128x28x28x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x56xf32>, %arg1: tensor<152x1x1x56xf32>, %arg2: tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x152xf32>\n    memref.copy %2, %alloc : memref<128x28x28x152xf32> to memref<128x28x28x152xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x152xf32>\n    return %3 : tensor<128x28x28x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x56xf32>) -> tensor<128x28x28x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x56xf32>) -> tensor<152x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x56xf32>, tensor<152x1x1x56xf32>) outs(%7 : tensor<128x28x28x152xf32>) -> tensor<128x28x28x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2755781046}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x2016xf32>, tensor<224x1x1x2016xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x2016xf32>, tensor<224x1x1x2016xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x2016xf32>, %3: tensor<224x1x1x2016xf32>, %7: tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x2016xf32>, tensor<224x1x1x2016xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n  return %ret : tensor<512x1x1x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x2016xf32>, %arg1: tensor<224x1x1x2016xf32>, %arg2: tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x2016xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x2016xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x224xf32>\n    memref.copy %2, %alloc : memref<512x1x1x224xf32> to memref<512x1x1x224xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 2016 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x2016xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x2016xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x224xf32>\n    return %3 : tensor<512x1x1x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x2016xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x2016xf32>) -> tensor<512x1x1x2016xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x2016xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x2016xf32>) -> tensor<224x1x1x2016xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x2016xf32>, tensor<224x1x1x2016xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 2016, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 867900226}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x896xf32>, tensor<112x1x1x896xf32>) outs(%7 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x896xf32>, tensor<112x1x1x896xf32>) outs(%7 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x896xf32>, %3: tensor<112x1x1x896xf32>, %7: tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x896xf32>, tensor<112x1x1x896xf32>) outs(%7 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n  return %ret : tensor<128x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x896xf32>, %arg1: tensor<112x1x1x896xf32>, %arg2: tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x896xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x896xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x112xf32>\n    memref.copy %2, %alloc : memref<128x1x1x112xf32> to memref<128x1x1x112xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 896 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x896xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x896xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x112xf32>\n    return %3 : tensor<128x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x896xf32>) -> tensor<128x1x1x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x896xf32>) -> tensor<112x1x1x896xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x896xf32>, tensor<112x1x1x896xf32>) outs(%7 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 47936741}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<120x1x1x120xf32>) outs(%7 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<120x1x1x120xf32>) outs(%7 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x120xf32>, %3: tensor<120x1x1x120xf32>, %7: tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<120x1x1x120xf32>) outs(%7 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>\n  return %ret : tensor<128x28x28x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x120xf32>, %arg1: tensor<120x1x1x120xf32>, %arg2: tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x120xf32>\n    memref.copy %2, %alloc : memref<128x28x28x120xf32> to memref<128x28x28x120xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x120xf32>\n    return %3 : tensor<128x28x28x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x120xf32>) -> tensor<120x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x120xf32>, tensor<120x1x1x120xf32>) outs(%7 : tensor<128x28x28x120xf32>) -> tensor<128x28x28x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5093816546}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<440x1x1x52xf32>) outs(%7 : tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<440x1x1x52xf32>) outs(%7 : tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x52xf32>, %3: tensor<440x1x1x52xf32>, %7: tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<440x1x1x52xf32>) outs(%7 : tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32>\n  return %ret : tensor<128x1x1x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x52xf32>, %arg1: tensor<440x1x1x52xf32>, %arg2: tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x52xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x52xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x440xf32>\n    memref.copy %2, %alloc : memref<128x1x1x440xf32> to memref<128x1x1x440xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 52 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x52xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x52xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x440xf32>\n    return %3 : tensor<128x1x1x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x52xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x52xf32>) -> tensor<128x1x1x52xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x52xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x52xf32>) -> tensor<440x1x1x52xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<440x1x1x52xf32>) outs(%7 : tensor<128x1x1x440xf32>) -> tensor<128x1x1x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 52, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8278032}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<320x1x1x128xf32>) outs(%7 : tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<320x1x1x128xf32>) outs(%7 : tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x128xf32>, %3: tensor<320x1x1x128xf32>, %7: tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<320x1x1x128xf32>) outs(%7 : tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32>\n  return %ret : tensor<512x14x14x320xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x128xf32>, %arg1: tensor<320x1x1x128xf32>, %arg2: tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<320x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x320xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x320xf32>\n    memref.copy %2, %alloc : memref<512x14x14x320xf32> to memref<512x14x14x320xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 320 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<320x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x320xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x320xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x320xf32>\n    return %3 : tensor<512x14x14x320xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x320xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<320x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<320x1x1x128xf32>) -> tensor<320x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x320xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<320x1x1x128xf32>) outs(%7 : tensor<512x14x14x320xf32>) -> tensor<512x14x14x320xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x320xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x320xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 320, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14652264699}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x176xf32>, tensor<176x1x1x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x176xf32>, tensor<176x1x1x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x176xf32>, %3: tensor<176x1x1x176xf32>, %7: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x176xf32>, tensor<176x1x1x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n  return %ret : tensor<512x7x7x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x176xf32>, %arg1: tensor<176x1x1x176xf32>, %arg2: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<176x1x1x176xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176xf32> to memref<512x7x7x176xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 176 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x176xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<176x1x1x176xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176xf32>\n    return %3 : tensor<512x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x176xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<176x1x1x176xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<176x1x1x176xf32>) -> tensor<176x1x1x176xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x176xf32>, tensor<176x1x1x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 176, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2798751769}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1024xf32>, tensor<256x1x1x1024xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1024xf32>, tensor<256x1x1x1024xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1024xf32>, %3: tensor<256x1x1x1024xf32>, %7: tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1024xf32>, tensor<256x1x1x1024xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n  return %ret : tensor<128x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1024xf32>, %arg1: tensor<256x1x1x1024xf32>, %arg2: tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x1024xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x256xf32>\n    memref.copy %2, %alloc : memref<128x14x14x256xf32> to memref<128x14x14x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1024 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1024xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x1024xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x256xf32>\n    return %3 : tensor<128x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1024xf32>) -> tensor<128x14x14x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x1024xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x1024xf32>) -> tensor<256x1x1x1024xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1024xf32>, tensor<256x1x1x1024xf32>) outs(%7 : tensor<128x14x14x256xf32>) -> tensor<128x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24662330051}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x30x30x128xf32>, %3: tensor<32x3x3x128xf32>, %7: tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>\n  return %ret : tensor<256x28x28x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x30x30x128xf32>, %arg1: tensor<32x3x3x128xf32>, %arg2: tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x30x30x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x32xf32>\n    memref.copy %2, %alloc : memref<256x28x28x32xf32> to memref<256x28x28x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x30x30x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x32xf32>\n    return %3 : tensor<256x28x28x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x30x30x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x30x30x128xf32>) -> tensor<256x30x30x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x128xf32>) -> tensor<32x3x3x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x30x30x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<256x28x28x32xf32>) -> tensor<256x28x28x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27833907495}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<36x1x1x144xf32>) outs(%7 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<36x1x1x144xf32>) outs(%7 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<36x1x1x144xf32>, %7: tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<36x1x1x144xf32>) outs(%7 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n  return %ret : tensor<512x1x1x36xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<36x1x1x144xf32>, %arg2: tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<36x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x36xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x36xf32>\n    memref.copy %2, %alloc : memref<512x1x1x36xf32> to memref<512x1x1x36xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 36 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<36x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x36xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x36xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x36xf32>\n    return %3 : tensor<512x1x1x36xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x36xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<36x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<36x1x1x144xf32>) -> tensor<36x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x36xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<36x1x1x144xf32>) outs(%7 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x36xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x36xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 36, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9056122}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<112x1x1x12xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<112x1x1x12xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x12xf32>, %3: tensor<112x1x1x12xf32>, %7: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<112x1x1x12xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n  return %ret : tensor<512x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x12xf32>, %arg1: tensor<112x1x1x12xf32>, %arg2: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x12xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x12xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x112xf32>\n    memref.copy %2, %alloc : memref<512x1x1x112xf32> to memref<512x1x1x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 12 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x12xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x12xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x112xf32>\n    return %3 : tensor<512x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x12xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x12xf32>) -> tensor<112x1x1x12xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<112x1x1x12xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 12, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1256954}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<144x1x1x24xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<144x1x1x24xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x24xf32>, %3: tensor<144x1x1x24xf32>, %7: tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<144x1x1x24xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n  return %ret : tensor<128x56x56x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x24xf32>, %arg1: tensor<144x1x1x24xf32>, %arg2: tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x144xf32>\n    memref.copy %2, %alloc : memref<128x56x56x144xf32> to memref<128x56x56x144xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x144xf32>\n    return %3 : tensor<128x56x56x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x24xf32>) -> tensor<144x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<144x1x1x24xf32>) outs(%7 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3619811071}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x21x21x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x21x21x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x21x21x336xf32>, %3: tensor<336x1x1x336xf32>, %7: tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x21x21x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>\n  return %ret : tensor<128x21x21x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x21x21x336xf32>, %arg1: tensor<336x1x1x336xf32>, %arg2: tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x21x21x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x21x21x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x21x21x336xf32>\n    memref.copy %2, %alloc : memref<128x21x21x336xf32> to memref<128x21x21x336xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x21x21x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x21x21x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x21x21x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x21x21x336xf32>\n    return %3 : tensor<128x21x21x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x21x21x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x21x21x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x336xf32>) -> tensor<336x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x21x21x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x21x21x336xf32>, tensor<336x1x1x336xf32>) outs(%7 : tensor<128x21x21x336xf32>) -> tensor<128x21x21x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x21x21x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x21x21x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23515431419}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x117x117x32xf32>, tensor<7x7x32x1xf32>) outs(%7 : tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x117x117x32xf32>, tensor<7x7x32x1xf32>) outs(%7 : tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x117x117x32xf32>, %3: tensor<7x7x32x1xf32>, %7: tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x117x117x32xf32>, tensor<7x7x32x1xf32>) outs(%7 : tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32>\n  return %ret : tensor<128x56x56x32x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x117x117x32xf32>, %arg1: tensor<7x7x32x1xf32>, %arg2: tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x32x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x117x117x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x32x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x32x1xf32>\n    memref.copy %2, %alloc : memref<128x56x56x32x1xf32> to memref<128x56x56x32x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x117x117x32xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x32x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x32x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x32x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x32x1xf32>\n    return %3 : tensor<128x56x56x32x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x32x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x117x117x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x117x117x32xf32>) -> tensor<128x117x117x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x32x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x32x1xf32>) -> tensor<7x7x32x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x32x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x117x117x32xf32>, tensor<7x7x32x1xf32>) outs(%7 : tensor<128x56x56x32x1xf32>) -> tensor<128x56x56x32x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x32x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x32x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2090579156}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x192xf32>, %3: tensor<64x1x1x192xf32>, %7: tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32>\n  return %ret : tensor<256x14x14x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x192xf32>, %arg1: tensor<64x1x1x192xf32>, %arg2: tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x64xf32>\n    memref.copy %2, %alloc : memref<256x14x14x64xf32> to memref<256x14x14x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x64xf32>\n    return %3 : tensor<256x14x14x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x192xf32>) -> tensor<256x14x14x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x192xf32>) -> tensor<64x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<256x14x14x64xf32>) -> tensor<256x14x14x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2235736346}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x3024xf32>, tensor<308x1x1x3024xf32>) outs(%7 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x3024xf32>, tensor<308x1x1x3024xf32>) outs(%7 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x3024xf32>, %3: tensor<308x1x1x3024xf32>, %7: tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x3024xf32>, tensor<308x1x1x3024xf32>) outs(%7 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n  return %ret : tensor<512x1x1x308xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x3024xf32>, %arg1: tensor<308x1x1x3024xf32>, %arg2: tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<308x1x1x3024xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x3024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x308xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x308xf32>\n    memref.copy %2, %alloc : memref<512x1x1x308xf32> to memref<512x1x1x308xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 308 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 3024 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x3024xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<308x1x1x3024xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x308xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x308xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x308xf32>\n    return %3 : tensor<512x1x1x308xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x308xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x3024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x3024xf32>) -> tensor<512x1x1x3024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<308x1x1x3024xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<308x1x1x3024xf32>) -> tensor<308x1x1x3024xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x308xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x3024xf32>, tensor<308x1x1x3024xf32>) outs(%7 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x308xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x308xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 308, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 3024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1794099589}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x960xf32>, %3: tensor<128x1x1x960xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x960xf32>, %arg1: tensor<128x1x1x960xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x960xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 960 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x960xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x960xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x960xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x960xf32>) -> tensor<256x14x14x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x960xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x960xf32>) -> tensor<128x1x1x960xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x960xf32>, tensor<128x1x1x960xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 960, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23127536215}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x256xf32>, %3: tensor<128x1x1x256xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x256xf32>, %arg1: tensor<128x1x1x256xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<128x1x1x256xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12051669187}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x576xf32>, %3: tensor<128x1x1x576xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x576xf32>, %arg1: tensor<128x1x1x576xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x576xf32>) -> tensor<256x14x14x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13788359157}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<3712x1x1x348xf32>) outs(%7 : tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<3712x1x1x348xf32>) outs(%7 : tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x348xf32>, %3: tensor<3712x1x1x348xf32>, %7: tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<3712x1x1x348xf32>) outs(%7 : tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32>\n  return %ret : tensor<512x1x1x3712xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x348xf32>, %arg1: tensor<3712x1x1x348xf32>, %arg2: tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3712x1x1x348xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x348xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x3712xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x3712xf32>\n    memref.copy %2, %alloc : memref<512x1x1x3712xf32> to memref<512x1x1x3712xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 3712 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 348 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x348xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<3712x1x1x348xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x3712xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x3712xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x3712xf32>\n    return %3 : tensor<512x1x1x3712xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x3712xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x348xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x348xf32>) -> tensor<512x1x1x348xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3712x1x1x348xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3712x1x1x348xf32>) -> tensor<3712x1x1x348xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x3712xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<3712x1x1x348xf32>) outs(%7 : tensor<512x1x1x3712xf32>) -> tensor<512x1x1x3712xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x3712xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x3712xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 3712, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 348, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2402640222}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x144xf32>, tensor<288x1x1x144xf32>) outs(%7 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x144xf32>, tensor<288x1x1x144xf32>) outs(%7 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x144xf32>, %3: tensor<288x1x1x144xf32>, %7: tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x144xf32>, tensor<288x1x1x144xf32>) outs(%7 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>\n  return %ret : tensor<128x28x28x288xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x144xf32>, %arg1: tensor<288x1x1x144xf32>, %arg2: tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x288xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x288xf32>\n    memref.copy %2, %alloc : memref<128x28x28x288xf32> to memref<128x28x28x288xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 288 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<288x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x288xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x288xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x288xf32>\n    return %3 : tensor<128x28x28x288xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x288xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x144xf32>) -> tensor<128x56x56x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<288x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<288x1x1x144xf32>) -> tensor<288x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x288xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x144xf32>, tensor<288x1x1x144xf32>) outs(%7 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x288xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x288xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 288, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14944192237}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x59x59x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x59x59x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x59x59x22xf32>, %3: tensor<5x5x22x1xf32>, %7: tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x59x59x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32>\n  return %ret : tensor<128x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x59x59x22xf32>, %arg1: tensor<5x5x22x1xf32>, %arg2: tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x59x59x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<128x28x28x22x1xf32> to memref<128x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x59x59x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x22x1xf32>\n    return %3 : tensor<128x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x59x59x22xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x59x59x22xf32>) -> tensor<128x59x59x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x22x1xf32>) -> tensor<5x5x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x59x59x22xf32>, tensor<5x5x22x1xf32>) outs(%7 : tensor<128x28x28x22x1xf32>) -> tensor<128x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 161606760}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<160x1x1x960xf32>) outs(%7 : tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<160x1x1x960xf32>) outs(%7 : tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x960xf32>, %3: tensor<160x1x1x960xf32>, %7: tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<160x1x1x960xf32>) outs(%7 : tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32>\n  return %ret : tensor<256x7x7x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x960xf32>, %arg1: tensor<160x1x1x960xf32>, %arg2: tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x960xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x160xf32>\n    memref.copy %2, %alloc : memref<256x7x7x160xf32> to memref<256x7x7x160xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 960 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x960xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x960xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x160xf32>\n    return %3 : tensor<256x7x7x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x960xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x960xf32>) -> tensor<256x7x7x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x960xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x960xf32>) -> tensor<160x1x1x960xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x960xf32>, tensor<160x1x1x960xf32>) outs(%7 : tensor<256x7x7x160xf32>) -> tensor<256x7x7x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 960, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7226149610}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x62x62x11xf32>, tensor<7x7x11x1xf32>) outs(%7 : tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x62x62x11xf32>, tensor<7x7x11x1xf32>) outs(%7 : tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x62x62x11xf32>, %3: tensor<7x7x11x1xf32>, %7: tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x62x62x11xf32>, tensor<7x7x11x1xf32>) outs(%7 : tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32>\n  return %ret : tensor<512x56x56x11x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x62x62x11xf32>, %arg1: tensor<7x7x11x1xf32>, %arg2: tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x11x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x62x62x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x11x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x11x1xf32>\n    memref.copy %2, %alloc : memref<512x56x56x11x1xf32> to memref<512x56x56x11x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x62x62x11xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x11x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x11x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x11x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x11x1xf32>\n    return %3 : tensor<512x56x56x11x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x11x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x62x62x11xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x62x62x11xf32>) -> tensor<512x62x62x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x11x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x11x1xf32>) -> tensor<7x7x11x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x11x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x62x62x11xf32>, tensor<7x7x11x1xf32>) outs(%7 : tensor<512x56x56x11x1xf32>) -> tensor<512x56x56x11x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x11x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x11x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2844236638}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1344xf32>, tensor<128x1x1x1344xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1344xf32>, tensor<128x1x1x1344xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1344xf32>, %3: tensor<128x1x1x1344xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1344xf32>, tensor<128x1x1x1344xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1344xf32>, %arg1: tensor<128x1x1x1344xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1344xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1344xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1344 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1344xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1344xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1344xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1344xf32>) -> tensor<256x7x7x1344xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1344xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1344xf32>) -> tensor<128x1x1x1344xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1344xf32>, tensor<128x1x1x1344xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1344, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8106917928}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1888xf32>, tensor<128x1x1x1888xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1888xf32>, tensor<128x1x1x1888xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1888xf32>, %3: tensor<128x1x1x1888xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1888xf32>, tensor<128x1x1x1888xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1888xf32>, %arg1: tensor<128x1x1x1888xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1888xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1888xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1888 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1888xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1888xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1888xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1888xf32>) -> tensor<128x7x7x1888xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1888xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1888xf32>) -> tensor<128x1x1x1888xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1888xf32>, tensor<128x1x1x1888xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1888, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5703945827}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x58x58x64xf32>, tensor<64x3x3x64xf32>) outs(%7 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x58x58x64xf32>, tensor<64x3x3x64xf32>) outs(%7 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x58x58x64xf32>, %3: tensor<64x3x3x64xf32>, %7: tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x58x58x64xf32>, tensor<64x3x3x64xf32>) outs(%7 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>\n  return %ret : tensor<128x28x28x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x58x58x64xf32>, %arg1: tensor<64x3x3x64xf32>, %arg2: tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x3x3x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x58x58x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x64xf32>\n    memref.copy %2, %alloc : memref<128x28x28x64xf32> to memref<128x28x28x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x58x58x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x3x3x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x64xf32>\n    return %3 : tensor<128x28x28x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x58x58x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x58x58x64xf32>) -> tensor<128x58x58x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x3x3x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x3x3x64xf32>) -> tensor<64x3x3x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x58x58x64xf32>, tensor<64x3x3x64xf32>) outs(%7 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13848681499}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x42xf32>, tensor<168x1x1x42xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x42xf32>, tensor<168x1x1x42xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x42xf32>, %3: tensor<168x1x1x42xf32>, %7: tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x42xf32>, tensor<168x1x1x42xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n  return %ret : tensor<256x1x1x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x42xf32>, %arg1: tensor<168x1x1x42xf32>, %arg2: tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x168xf32>\n    memref.copy %2, %alloc : memref<256x1x1x168xf32> to memref<256x1x1x168xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x42xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x168xf32>\n    return %3 : tensor<256x1x1x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x42xf32>) -> tensor<256x1x1x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x42xf32>) -> tensor<168x1x1x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x42xf32>, tensor<168x1x1x42xf32>) outs(%7 : tensor<256x1x1x168xf32>) -> tensor<256x1x1x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4722656}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1312xf32>, %3: tensor<128x1x1x1312xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1312xf32>, %arg1: tensor<128x1x1x1312xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1312xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1312xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1312 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1312xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1312xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1312xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1312xf32>) -> tensor<256x7x7x1312xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1312xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1312xf32>) -> tensor<128x1x1x1312xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1312xf32>, tensor<128x1x1x1312xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1312, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7913059382}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x320xf32>, tensor<1280x1x1x320xf32>) outs(%7 : tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x320xf32>, tensor<1280x1x1x320xf32>) outs(%7 : tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x320xf32>, %3: tensor<1280x1x1x320xf32>, %7: tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x320xf32>, tensor<1280x1x1x320xf32>) outs(%7 : tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32>\n  return %ret : tensor<256x7x7x1280xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x320xf32>, %arg1: tensor<1280x1x1x320xf32>, %arg2: tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1280x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x1280xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x1280xf32>\n    memref.copy %2, %alloc : memref<256x7x7x1280xf32> to memref<256x7x7x1280xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1280 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1280x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x1280xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x1280xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x1280xf32>\n    return %3 : tensor<256x7x7x1280xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x1280xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x320xf32>) -> tensor<256x7x7x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1280x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1280x1x1x320xf32>) -> tensor<1280x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x1280xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x320xf32>, tensor<1280x1x1x320xf32>) outs(%7 : tensor<256x7x7x1280xf32>) -> tensor<256x7x7x1280xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x1280xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x1280xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1280, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18969152936}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x192xf32>, %3: tensor<32x1x1x192xf32>, %7: tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n  return %ret : tensor<128x28x28x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x192xf32>, %arg1: tensor<32x1x1x192xf32>, %arg2: tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x32xf32>\n    memref.copy %2, %alloc : memref<128x28x28x32xf32> to memref<128x28x28x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x32xf32>\n    return %3 : tensor<128x28x28x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x192xf32>) -> tensor<128x28x28x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x192xf32>) -> tensor<32x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<128x28x28x32xf32>) -> tensor<128x28x28x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2235127099}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x3712xf32>, tensor<348x1x1x3712xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x3712xf32>, tensor<348x1x1x3712xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x3712xf32>, %3: tensor<348x1x1x3712xf32>, %7: tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x3712xf32>, tensor<348x1x1x3712xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n  return %ret : tensor<256x1x1x348xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x3712xf32>, %arg1: tensor<348x1x1x3712xf32>, %arg2: tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<348x1x1x3712xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x3712xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x348xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x348xf32>\n    memref.copy %2, %alloc : memref<256x1x1x348xf32> to memref<256x1x1x348xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 348 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 3712 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x3712xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<348x1x1x3712xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x348xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x348xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x348xf32>\n    return %3 : tensor<256x1x1x348xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x348xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x3712xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x3712xf32>) -> tensor<256x1x1x3712xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<348x1x1x3712xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<348x1x1x3712xf32>) -> tensor<348x1x1x3712xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x348xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x3712xf32>, tensor<348x1x1x3712xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x348xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x348xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 348, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 3712, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1245303586}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x44xf32>, %3: tensor<22x1x1x44xf32>, %7: tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32>\n  return %ret : tensor<256x28x28x22xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x44xf32>, %arg1: tensor<22x1x1x44xf32>, %arg2: tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<22x1x1x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x22xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x22xf32>\n    memref.copy %2, %alloc : memref<256x28x28x22xf32> to memref<256x28x28x22xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 44 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x44xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<22x1x1x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x22xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x22xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x22xf32>\n    return %3 : tensor<256x28x28x22xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x22xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x44xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x44xf32>) -> tensor<256x28x28x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<22x1x1x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<22x1x1x44xf32>) -> tensor<22x1x1x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x22xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x28x28x22xf32>) -> tensor<256x28x28x22xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x22xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x22xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 44, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 633101860}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x512xf32>, %3: tensor<128x1x1x512xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x512xf32>, %arg1: tensor<128x1x1x512xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1529782056}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<1392x1x1x348xf32>) outs(%7 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<1392x1x1x348xf32>) outs(%7 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x348xf32>, %3: tensor<1392x1x1x348xf32>, %7: tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<1392x1x1x348xf32>) outs(%7 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>\n  return %ret : tensor<512x1x1x1392xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x348xf32>, %arg1: tensor<1392x1x1x348xf32>, %arg2: tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1392x1x1x348xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x348xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1392xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1392xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1392xf32> to memref<512x1x1x1392xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1392 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 348 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x348xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1392x1x1x348xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1392xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1392xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1392xf32>\n    return %3 : tensor<512x1x1x1392xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1392xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x348xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x348xf32>) -> tensor<512x1x1x348xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1392x1x1x348xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1392x1x1x348xf32>) -> tensor<1392x1x1x348xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1392xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x348xf32>, tensor<1392x1x1x348xf32>) outs(%7 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1392xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1392xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1392, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 348, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 900900564}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x576xf32>, %3: tensor<128x1x1x576xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x576xf32>, %arg1: tensor<128x1x1x576xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x576xf32>) -> tensor<512x7x7x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<128x1x1x576xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6896036700}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x144xf32>, tensor<24x1x1x144xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x144xf32>, tensor<24x1x1x144xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x144xf32>, %3: tensor<24x1x1x144xf32>, %7: tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x144xf32>, tensor<24x1x1x144xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n  return %ret : tensor<512x56x56x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x144xf32>, %arg1: tensor<24x1x1x144xf32>, %arg2: tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x24xf32>\n    memref.copy %2, %alloc : memref<512x56x56x24xf32> to memref<512x56x56x24xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x24xf32>\n    return %3 : tensor<512x56x56x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x144xf32>) -> tensor<512x56x56x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x144xf32>) -> tensor<24x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x144xf32>, tensor<24x1x1x144xf32>) outs(%7 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19901544306}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x16x16x576xf32>, %3: tensor<3x3x576x1xf32>, %7: tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32>\n  return %ret : tensor<128x14x14x576x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x16x16x576xf32>, %arg1: tensor<3x3x576x1xf32>, %arg2: tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x576x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x16x16x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x576x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x576x1xf32>\n    memref.copy %2, %alloc : memref<128x14x14x576x1xf32> to memref<128x14x14x576x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x16x16x576xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x576x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x576x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x576x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x576x1xf32>\n    return %3 : tensor<128x14x14x576x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x576x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x16x16x576xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x16x16x576xf32>) -> tensor<128x16x16x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x576x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x576x1xf32>) -> tensor<3x3x576x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x576x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<128x14x14x576x1xf32>) -> tensor<128x14x14x576x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x576x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x576x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 340268719}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<192x7x1x160xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<192x7x1x160xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x23x17x160xf32>, %3: tensor<192x7x1x160xf32>, %7: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<192x7x1x160xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n  return %ret : tensor<128x17x17x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x23x17x160xf32>, %arg1: tensor<192x7x1x160xf32>, %arg2: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x7x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x23x17x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x17x17x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x17x17x192xf32>\n    memref.copy %2, %alloc : memref<128x17x17x192xf32> to memref<128x17x17x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x23x17x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x7x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x17x17x192xf32>\n    return %3 : tensor<128x17x17x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x17x17x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x23x17x160xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x23x17x160xf32>) -> tensor<128x23x17x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x7x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x7x1x160xf32>) -> tensor<192x7x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x17x17x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<192x7x1x160xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x17x17x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x17x17x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 29927628813}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x408xf32>, tensor<408x1x1x408xf32>) outs(%7 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x408xf32>, tensor<408x1x1x408xf32>) outs(%7 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x408xf32>, %3: tensor<408x1x1x408xf32>, %7: tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x408xf32>, tensor<408x1x1x408xf32>) outs(%7 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>\n  return %ret : tensor<128x14x14x408xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x408xf32>, %arg1: tensor<408x1x1x408xf32>, %arg2: tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<408x1x1x408xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x408xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x408xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x408xf32>\n    memref.copy %2, %alloc : memref<128x14x14x408xf32> to memref<128x14x14x408xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 408 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 408 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x408xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<408x1x1x408xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x408xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x408xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x408xf32>\n    return %3 : tensor<128x14x14x408xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x408xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x408xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<408x1x1x408xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<408x1x1x408xf32>) -> tensor<408x1x1x408xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x408xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x408xf32>, tensor<408x1x1x408xf32>) outs(%7 : tensor<128x14x14x408xf32>) -> tensor<128x14x14x408xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x408xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x408xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 408, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 408, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15474764112}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x44xf32>, %3: tensor<22x1x1x44xf32>, %7: tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32>\n  return %ret : tensor<256x56x56x22xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x44xf32>, %arg1: tensor<22x1x1x44xf32>, %arg2: tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<22x1x1x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x22xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x22xf32>\n    memref.copy %2, %alloc : memref<256x56x56x22xf32> to memref<256x56x56x22xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 44 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x44xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<22x1x1x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x22xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x22xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x22xf32>\n    return %3 : tensor<256x56x56x22xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x22xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x44xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x44xf32>) -> tensor<256x56x56x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<22x1x1x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<22x1x1x44xf32>) -> tensor<22x1x1x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x22xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x44xf32>, tensor<22x1x1x44xf32>) outs(%7 : tensor<256x56x56x22xf32>) -> tensor<256x56x56x22xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x22xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x22xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 44, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2531848676}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x512xf32>, %3: tensor<512x1x1x512xf32>, %7: tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n  return %ret : tensor<128x14x14x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x512xf32>, %arg1: tensor<512x1x1x512xf32>, %arg2: tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x512xf32>\n    memref.copy %2, %alloc : memref<128x14x14x512xf32> to memref<128x14x14x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x512xf32>\n    return %3 : tensor<128x14x14x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x512xf32>, tensor<512x1x1x512xf32>) outs(%7 : tensor<128x14x14x512xf32>) -> tensor<128x14x14x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24465418979}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<168x1x1x168xf32>) outs(%7 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<168x1x1x168xf32>) outs(%7 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x168xf32>, %3: tensor<168x1x1x168xf32>, %7: tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<168x1x1x168xf32>) outs(%7 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>\n  return %ret : tensor<128x28x28x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x168xf32>, %arg1: tensor<168x1x1x168xf32>, %arg2: tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x168xf32>\n    memref.copy %2, %alloc : memref<128x28x28x168xf32> to memref<128x28x28x168xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x168xf32>\n    return %3 : tensor<128x28x28x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x168xf32>) -> tensor<168x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x168xf32>, tensor<168x1x1x168xf32>) outs(%7 : tensor<128x28x28x168xf32>) -> tensor<128x28x28x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10198913677}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x2048xf32>, tensor<512x1x1x2048xf32>) outs(%7 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x2048xf32>, tensor<512x1x1x2048xf32>) outs(%7 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x2048xf32>, %3: tensor<512x1x1x2048xf32>, %7: tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x2048xf32>, tensor<512x1x1x2048xf32>) outs(%7 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>\n  return %ret : tensor<128x7x7x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x2048xf32>, %arg1: tensor<512x1x1x2048xf32>, %arg2: tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x2048xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x2048xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x512xf32>\n    memref.copy %2, %alloc : memref<128x7x7x512xf32> to memref<128x7x7x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 2048 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x2048xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x2048xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x512xf32>\n    return %3 : tensor<128x7x7x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x2048xf32>) -> tensor<128x7x7x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x2048xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x2048xf32>) -> tensor<512x1x1x2048xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x2048xf32>, tensor<512x1x1x2048xf32>) outs(%7 : tensor<128x7x7x512xf32>) -> tensor<128x7x7x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 2048, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24766986155}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x48xf32>, tensor<512x1x1x48xf32>) outs(%7 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x48xf32>, tensor<512x1x1x48xf32>) outs(%7 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x48xf32>, %3: tensor<512x1x1x48xf32>, %7: tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x48xf32>, tensor<512x1x1x48xf32>) outs(%7 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n  return %ret : tensor<128x1x1x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x48xf32>, %arg1: tensor<512x1x1x48xf32>, %arg2: tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x512xf32>\n    memref.copy %2, %alloc : memref<128x1x1x512xf32> to memref<128x1x1x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x512xf32>\n    return %3 : tensor<128x1x1x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x48xf32>) -> tensor<128x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x48xf32>, tensor<512x1x1x48xf32>) outs(%7 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8686697}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x32xf32>, %3: tensor<64x1x1x32xf32>, %7: tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n  return %ret : tensor<512x28x28x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x32xf32>, %arg1: tensor<64x1x1x32xf32>, %arg2: tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x64xf32>\n    memref.copy %2, %alloc : memref<512x28x28x64xf32> to memref<512x28x28x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x64xf32>\n    return %3 : tensor<512x28x28x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x32xf32>) -> tensor<512x56x56x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x32xf32>) -> tensor<64x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2280676817}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x47x47x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x47x47x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x47x47x336xf32>, %3: tensor<7x7x336x1xf32>, %7: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x47x47x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n  return %ret : tensor<512x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x47x47x336xf32>, %arg1: tensor<7x7x336x1xf32>, %arg2: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x47x47x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<512x21x21x336x1xf32> to memref<512x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x47x47x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x21x21x336x1xf32>\n    return %3 : tensor<512x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x47x47x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x47x47x336xf32>) -> tensor<512x47x47x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x336x1xf32>) -> tensor<7x7x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x47x47x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 12363504749}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x736xf32>, %3: tensor<128x1x1x736xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x736xf32>, %arg1: tensor<128x1x1x736xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x736xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x736xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 736 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x736xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x736xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x736xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x736xf32>) -> tensor<128x14x14x736xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x736xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x736xf32>) -> tensor<128x1x1x736xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 736, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8837575635}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<32x1x1x32xf32>, %7: tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n  return %ret : tensor<512x112x112x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<32x1x1x32xf32>, %arg2: tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x112x112x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x112x112x32xf32>\n    memref.copy %2, %alloc : memref<512x112x112x32xf32> to memref<512x112x112x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x112x112x32xf32>\n    return %3 : tensor<512x112x112x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x112x112x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x32xf32>) -> tensor<32x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x112x112x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x112x112x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18984187568}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x416xf32>, tensor<128x1x1x416xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x416xf32>, tensor<128x1x1x416xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x416xf32>, %3: tensor<128x1x1x416xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x416xf32>, tensor<128x1x1x416xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x416xf32>, %arg1: tensor<128x1x1x416xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x416xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x416xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 416 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x416xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x416xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x416xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x416xf32>) -> tensor<128x14x14x416xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x416xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x416xf32>) -> tensor<128x1x1x416xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x416xf32>, tensor<128x1x1x416xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 416, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4953040244}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<448x1x1x112xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<448x1x1x112xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x112xf32>, %3: tensor<448x1x1x112xf32>, %7: tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<448x1x1x112xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n  return %ret : tensor<128x1x1x448xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x112xf32>, %arg1: tensor<448x1x1x112xf32>, %arg2: tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<448x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x448xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x448xf32>\n    memref.copy %2, %alloc : memref<128x1x1x448xf32> to memref<128x1x1x448xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 448 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<448x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x448xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x448xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x448xf32>\n    return %3 : tensor<128x1x1x448xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x448xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<448x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<448x1x1x112xf32>) -> tensor<448x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x448xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<448x1x1x112xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x448xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x448xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 448, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21392282}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x92xf32>, tensor<368x1x1x92xf32>) outs(%7 : tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x92xf32>, tensor<368x1x1x92xf32>) outs(%7 : tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x92xf32>, %3: tensor<368x1x1x92xf32>, %7: tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x92xf32>, tensor<368x1x1x92xf32>) outs(%7 : tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32>\n  return %ret : tensor<128x1x1x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x92xf32>, %arg1: tensor<368x1x1x92xf32>, %arg2: tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x92xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x92xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x368xf32>\n    memref.copy %2, %alloc : memref<128x1x1x368xf32> to memref<128x1x1x368xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 92 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x92xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x92xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x368xf32>\n    return %3 : tensor<128x1x1x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x92xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x92xf32>) -> tensor<128x1x1x92xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x92xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x92xf32>) -> tensor<368x1x1x92xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x92xf32>, tensor<368x1x1x92xf32>) outs(%7 : tensor<128x1x1x368xf32>) -> tensor<128x1x1x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 92, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14023322}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x45x45x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x45x45x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x45x45x336xf32>, %3: tensor<5x5x336x1xf32>, %7: tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x45x45x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n  return %ret : tensor<256x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x45x45x336xf32>, %arg1: tensor<5x5x336x1xf32>, %arg2: tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x45x45x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<256x21x21x336x1xf32> to memref<256x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x45x45x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x21x21x336x1xf32>\n    return %3 : tensor<256x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x45x45x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x45x45x336xf32>) -> tensor<256x45x45x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x336x1xf32>) -> tensor<5x5x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x45x45x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<256x21x21x336x1xf32>) -> tensor<256x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2801957613}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1024xf32>, tensor<128x1x1x1024xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1024xf32>, tensor<128x1x1x1024xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x1024xf32>, %3: tensor<128x1x1x1024xf32>, %7: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1024xf32>, tensor<128x1x1x1024xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n  return %ret : tensor<256x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x1024xf32>, %arg1: tensor<128x1x1x1024xf32>, %arg2: tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1024xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x128xf32>\n    memref.copy %2, %alloc : memref<256x7x7x128xf32> to memref<256x7x7x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1024 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x1024xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1024xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x128xf32>\n    return %3 : tensor<256x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x1024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1024xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1024xf32>) -> tensor<128x1x1x1024xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x1024xf32>, tensor<128x1x1x1024xf32>) outs(%7 : tensor<256x7x7x128xf32>) -> tensor<256x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6171773805}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x16x16x128xf32>, %3: tensor<32x3x3x128xf32>, %7: tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32>\n  return %ret : tensor<128x14x14x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x16x16x128xf32>, %arg1: tensor<32x3x3x128xf32>, %arg2: tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x16x16x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x32xf32>\n    memref.copy %2, %alloc : memref<128x14x14x32xf32> to memref<128x14x14x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x16x16x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x32xf32>\n    return %3 : tensor<128x14x14x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x16x16x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x16x16x128xf32>) -> tensor<128x16x16x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x128xf32>) -> tensor<32x3x3x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x16x16x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<128x14x14x32xf32>) -> tensor<128x14x14x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3478858356}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x9x9x128xf32>, %3: tensor<32x3x3x128xf32>, %7: tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32>\n  return %ret : tensor<512x7x7x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x9x9x128xf32>, %arg1: tensor<32x3x3x128xf32>, %arg2: tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x9x9x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x32xf32>\n    memref.copy %2, %alloc : memref<512x7x7x32xf32> to memref<512x7x7x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x9x9x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x32xf32>\n    return %3 : tensor<512x7x7x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x9x9x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x9x9x128xf32>) -> tensor<512x9x9x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x128xf32>) -> tensor<32x3x3x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x128xf32>, tensor<32x3x3x128xf32>) outs(%7 : tensor<512x7x7x32xf32>) -> tensor<512x7x7x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3479336418}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x17x17x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x17x17x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x17x17x672xf32>, %3: tensor<7x7x672x1xf32>, %7: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x17x17x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n  return %ret : tensor<256x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x17x17x672xf32>, %arg1: tensor<7x7x672x1xf32>, %arg2: tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x17x17x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<256x11x11x672x1xf32> to memref<256x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x17x17x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x11x11x672x1xf32>\n    return %3 : tensor<256x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x17x17x672xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x17x17x672xf32>) -> tensor<256x17x17x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x672x1xf32>) -> tensor<7x7x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x17x17x672xf32>, tensor<7x7x672x1xf32>) outs(%7 : tensor<256x11x11x672x1xf32>) -> tensor<256x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3556474230}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<96x1x1x48xf32>, %7: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n  return %ret : tensor<512x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<96x1x1x48xf32>, %arg2: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x96xf32>\n    memref.copy %2, %alloc : memref<512x56x56x96xf32> to memref<512x56x56x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x96xf32>\n    return %3 : tensor<512x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x48xf32>) -> tensor<96x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23793338240}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x96xf32>, tensor<42x1x1x96xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x96xf32>, tensor<42x1x1x96xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x83x83x96xf32>, %3: tensor<42x1x1x96xf32>, %7: tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x96xf32>, tensor<42x1x1x96xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n  return %ret : tensor<256x83x83x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x83x83x96xf32>, %arg1: tensor<42x1x1x96xf32>, %arg2: tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x83x83x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x83x83x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x83x83x42xf32>\n    memref.copy %2, %alloc : memref<256x83x83x42xf32> to memref<256x83x83x42xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x83x83x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x83x83x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x83x83x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x83x83x42xf32>\n    return %3 : tensor<256x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x83x83x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x83x83x96xf32>) -> tensor<256x83x83x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x96xf32>) -> tensor<42x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x83x83x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x83x83x96xf32>, tensor<42x1x1x96xf32>) outs(%7 : tensor<256x83x83x42xf32>) -> tensor<256x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24574244594}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<64x1x1x384xf32>) outs(%7 : tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<64x1x1x384xf32>) outs(%7 : tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x384xf32>, %3: tensor<64x1x1x384xf32>, %7: tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<64x1x1x384xf32>) outs(%7 : tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32>\n  return %ret : tensor<128x14x14x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x384xf32>, %arg1: tensor<64x1x1x384xf32>, %arg2: tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x64xf32>\n    memref.copy %2, %alloc : memref<128x14x14x64xf32> to memref<128x14x14x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 384 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x384xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x64xf32>\n    return %3 : tensor<128x14x14x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x384xf32>) -> tensor<128x14x14x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x384xf32>) -> tensor<64x1x1x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x384xf32>, tensor<64x1x1x384xf32>) outs(%7 : tensor<128x14x14x64xf32>) -> tensor<128x14x14x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2281997801}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x512xf32>, tensor<48x1x1x512xf32>) outs(%7 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x512xf32>, tensor<48x1x1x512xf32>) outs(%7 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x512xf32>, %3: tensor<48x1x1x512xf32>, %7: tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x512xf32>, tensor<48x1x1x512xf32>) outs(%7 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>\n  return %ret : tensor<256x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x512xf32>, %arg1: tensor<48x1x1x512xf32>, %arg2: tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x48xf32>\n    memref.copy %2, %alloc : memref<256x1x1x48xf32> to memref<256x1x1x48xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x48xf32>\n    return %3 : tensor<256x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x512xf32>) -> tensor<256x1x1x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x512xf32>) -> tensor<48x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x512xf32>, tensor<48x1x1x512xf32>) outs(%7 : tensor<256x1x1x48xf32>) -> tensor<256x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23165144}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x58x58x11xf32>, tensor<3x3x11x1xf32>) outs(%7 : tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x58x58x11xf32>, tensor<3x3x11x1xf32>) outs(%7 : tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x58x58x11xf32>, %3: tensor<3x3x11x1xf32>, %7: tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x58x58x11xf32>, tensor<3x3x11x1xf32>) outs(%7 : tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32>\n  return %ret : tensor<128x56x56x11x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x58x58x11xf32>, %arg1: tensor<3x3x11x1xf32>, %arg2: tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x11x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x58x58x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x11x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x11x1xf32>\n    memref.copy %2, %alloc : memref<128x56x56x11x1xf32> to memref<128x56x56x11x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x58x58x11xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x11x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x11x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x56x56x11x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x11x1xf32>\n    return %3 : tensor<128x56x56x11x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x11x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x58x58x11xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x58x58x11xf32>) -> tensor<128x58x58x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x11x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x11x1xf32>) -> tensor<3x3x11x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x11x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x58x58x11xf32>, tensor<3x3x11x1xf32>) outs(%7 : tensor<128x56x56x11x1xf32>) -> tensor<128x56x56x11x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x11x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x11x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 102226413}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x32xf32>, %3: tensor<11x1x1x32xf32>, %7: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n  return %ret : tensor<512x56x56x11xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x32xf32>, %arg1: tensor<11x1x1x32xf32>, %arg2: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<11x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x11xf32>\n    memref.copy %2, %alloc : memref<512x56x56x11xf32> to memref<512x56x56x11xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<11x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x11xf32>\n    return %3 : tensor<512x56x56x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x32xf32>) -> tensor<512x56x56x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<11x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<11x1x1x32xf32>) -> tensor<11x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x11xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x32xf32>, tensor<11x1x1x32xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1605911794}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x320xf32>, %3: tensor<32x1x1x320xf32>, %7: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n  return %ret : tensor<256x1x1x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x320xf32>, %arg1: tensor<32x1x1x320xf32>, %arg2: tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x32xf32>\n    memref.copy %2, %alloc : memref<256x1x1x32xf32> to memref<256x1x1x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x32xf32>\n    return %3 : tensor<256x1x1x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x320xf32>) -> tensor<256x1x1x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x320xf32>) -> tensor<32x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9506436}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x17x17x176xf32>, %3: tensor<5x5x176x1xf32>, %7: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n  return %ret : tensor<256x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x17x17x176xf32>, %arg1: tensor<5x5x176x1xf32>, %arg2: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x17x17x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<256x7x7x176x1xf32> to memref<256x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x17x17x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x176x1xf32>\n    return %3 : tensor<256x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x17x17x176xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x17x17x176xf32>) -> tensor<256x17x17x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x176x1xf32>) -> tensor<5x5x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x17x17x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 162107187}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x32xf32>, %3: tensor<192x1x1x32xf32>, %7: tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n  return %ret : tensor<256x1x1x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x32xf32>, %arg1: tensor<192x1x1x32xf32>, %arg2: tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x192xf32>\n    memref.copy %2, %alloc : memref<256x1x1x192xf32> to memref<256x1x1x192xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x192xf32>\n    return %3 : tensor<256x1x1x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x32xf32>) -> tensor<256x1x1x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x32xf32>) -> tensor<192x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x32xf32>, tensor<192x1x1x32xf32>) outs(%7 : tensor<256x1x1x192xf32>) -> tensor<256x1x1x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3752690}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<224x1x1x8xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<224x1x1x8xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x8xf32>, %3: tensor<224x1x1x8xf32>, %7: tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<224x1x1x8xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n  return %ret : tensor<256x1x1x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x8xf32>, %arg1: tensor<224x1x1x8xf32>, %arg2: tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x224xf32>\n    memref.copy %2, %alloc : memref<256x1x1x224xf32> to memref<256x1x1x224xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x224xf32>\n    return %3 : tensor<256x1x1x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x8xf32>) -> tensor<224x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x8xf32>, tensor<224x1x1x8xf32>) outs(%7 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 753326}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<48x1x1x256xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<48x1x1x256xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x35x35x256xf32>, %3: tensor<48x1x1x256xf32>, %7: tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<48x1x1x256xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n  return %ret : tensor<128x35x35x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x35x35x256xf32>, %arg1: tensor<48x1x1x256xf32>, %arg2: tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x35x35x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x48xf32>\n    memref.copy %2, %alloc : memref<128x35x35x48xf32> to memref<128x35x35x48xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x35x35x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x48xf32>\n    return %3 : tensor<128x35x35x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x35x35x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x35x35x256xf32>) -> tensor<128x35x35x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x256xf32>) -> tensor<48x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x256xf32>, tensor<48x1x1x256xf32>) outs(%7 : tensor<128x35x35x48xf32>) -> tensor<128x35x35x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7076544627}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x288xf32>, %3: tensor<128x1x1x288xf32>, %7: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n  return %ret : tensor<128x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x288xf32>, %arg1: tensor<128x1x1x288xf32>, %arg2: tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x128xf32>\n    memref.copy %2, %alloc : memref<128x28x28x128xf32> to memref<128x28x28x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x128xf32>\n    return %3 : tensor<128x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x288xf32>) -> tensor<128x28x28x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<128x28x28x128xf32>) -> tensor<128x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13594384389}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x18x18x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x18x18x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x18x18x88xf32>, %3: tensor<5x5x88x1xf32>, %7: tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x18x18x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n  return %ret : tensor<256x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x18x18x88xf32>, %arg1: tensor<5x5x88x1xf32>, %arg2: tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x18x18x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<256x14x14x88x1xf32> to memref<256x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x18x18x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x88x1xf32>\n    return %3 : tensor<256x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x18x18x88xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x18x18x88xf32>) -> tensor<256x18x18x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x88x1xf32>) -> tensor<5x5x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x18x18x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<256x14x14x88x1xf32>) -> tensor<256x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 316784468}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<224x1x1x56xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<224x1x1x56xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x56xf32>, %3: tensor<224x1x1x56xf32>, %7: tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<224x1x1x56xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n  return %ret : tensor<512x1x1x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x56xf32>, %arg1: tensor<224x1x1x56xf32>, %arg2: tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x224xf32>\n    memref.copy %2, %alloc : memref<512x1x1x224xf32> to memref<512x1x1x224xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x224xf32>\n    return %3 : tensor<512x1x1x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x1x56xf32>) -> tensor<224x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<224x1x1x56xf32>) outs(%7 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18579752}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<208x1x1x52xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<208x1x1x52xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x52xf32>, %3: tensor<208x1x1x52xf32>, %7: tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<208x1x1x52xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n  return %ret : tensor<128x1x1x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x52xf32>, %arg1: tensor<208x1x1x52xf32>, %arg2: tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x52xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x52xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x208xf32>\n    memref.copy %2, %alloc : memref<128x1x1x208xf32> to memref<128x1x1x208xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 52 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x52xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x52xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x208xf32>\n    return %3 : tensor<128x1x1x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x52xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x52xf32>) -> tensor<128x1x1x52xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x52xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x52xf32>) -> tensor<208x1x1x52xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x52xf32>, tensor<208x1x1x52xf32>) outs(%7 : tensor<128x1x1x208xf32>) -> tensor<128x1x1x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 52, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3916521}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x56xf32>, tensor<448x1x1x56xf32>) outs(%7 : tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x56xf32>, tensor<448x1x1x56xf32>) outs(%7 : tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x56xf32>, %3: tensor<448x1x1x56xf32>, %7: tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x56xf32>, tensor<448x1x1x56xf32>) outs(%7 : tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32>\n  return %ret : tensor<256x1x1x448xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x56xf32>, %arg1: tensor<448x1x1x56xf32>, %arg2: tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<448x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x448xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x448xf32>\n    memref.copy %2, %alloc : memref<256x1x1x448xf32> to memref<256x1x1x448xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 448 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<448x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x448xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x448xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x448xf32>\n    return %3 : tensor<256x1x1x448xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x448xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x56xf32>) -> tensor<256x1x1x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<448x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<448x1x1x56xf32>) -> tensor<448x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x448xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x56xf32>, tensor<448x1x1x56xf32>) outs(%7 : tensor<256x1x1x448xf32>) -> tensor<256x1x1x448xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x448xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x448xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 448, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18747190}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<32x3x3x32xf32>) outs(%7 : tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<32x3x3x32xf32>) outs(%7 : tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x37x37x32xf32>, %3: tensor<32x3x3x32xf32>, %7: tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<32x3x3x32xf32>) outs(%7 : tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32>\n  return %ret : tensor<128x35x35x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x37x37x32xf32>, %arg1: tensor<32x3x3x32xf32>, %arg2: tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x37x37x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x32xf32>\n    memref.copy %2, %alloc : memref<128x35x35x32xf32> to memref<128x35x35x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x37x37x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x32xf32>\n    return %3 : tensor<128x35x35x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x37x37x32xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x37x37x32xf32>) -> tensor<128x37x37x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x32xf32>) -> tensor<32x3x3x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x37x37x32xf32>, tensor<32x3x3x32xf32>) outs(%7 : tensor<128x35x35x32xf32>) -> tensor<128x35x35x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5327628993}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<1008x1x1x432xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<1008x1x1x432xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x432xf32>, %3: tensor<1008x1x1x432xf32>, %7: tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<1008x1x1x432xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n  return %ret : tensor<128x7x7x1008xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x432xf32>, %arg1: tensor<1008x1x1x432xf32>, %arg2: tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1008x1x1x432xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x432xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x1008xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x1008xf32>\n    memref.copy %2, %alloc : memref<128x7x7x1008xf32> to memref<128x7x7x1008xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1008 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 432 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x432xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1008x1x1x432xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1008xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x1008xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x1008xf32>\n    return %3 : tensor<128x7x7x1008xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x1008xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x432xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x432xf32>) -> tensor<128x14x14x432xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1008x1x1x432xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1008x1x1x432xf32>) -> tensor<1008x1x1x432xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x1008xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x432xf32>, tensor<1008x1x1x432xf32>) outs(%7 : tensor<128x7x7x1008xf32>) -> tensor<128x7x7x1008xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x1008xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x1008xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1008, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 432, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10133125072}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x27x27x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x27x27x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x27x27x336xf32>, %3: tensor<7x7x336x1xf32>, %7: tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x27x27x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32>\n  return %ret : tensor<128x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x27x27x336xf32>, %arg1: tensor<7x7x336x1xf32>, %arg2: tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x27x27x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<128x21x21x336x1xf32> to memref<128x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x27x27x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x21x21x336x1xf32>\n    return %3 : tensor<128x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x27x27x336xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x27x27x336xf32>) -> tensor<128x27x27x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x336x1xf32>) -> tensor<7x7x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x27x27x336xf32>, tensor<7x7x336x1xf32>) outs(%7 : tensor<128x21x21x336x1xf32>) -> tensor<128x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3060834958}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x28xf32>, tensor<256x1x1x28xf32>) outs(%7 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x28xf32>, tensor<256x1x1x28xf32>) outs(%7 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x28xf32>, %3: tensor<256x1x1x28xf32>, %7: tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x28xf32>, tensor<256x1x1x28xf32>) outs(%7 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n  return %ret : tensor<128x1x1x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x28xf32>, %arg1: tensor<256x1x1x28xf32>, %arg2: tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x28xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x28xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x256xf32>\n    memref.copy %2, %alloc : memref<128x1x1x256xf32> to memref<128x1x1x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 28 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x28xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x28xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x256xf32>\n    return %3 : tensor<128x1x1x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x28xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x28xf32>) -> tensor<128x1x1x28xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x28xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x28xf32>) -> tensor<256x1x1x28xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x28xf32>, tensor<256x1x1x28xf32>) outs(%7 : tensor<128x1x1x256xf32>) -> tensor<128x1x1x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 28, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2114686}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x16x16x256xf32>, tensor<256x3x3x256xf32>) outs(%7 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x16x16x256xf32>, tensor<256x3x3x256xf32>) outs(%7 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x16x16x256xf32>, %3: tensor<256x3x3x256xf32>, %7: tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x16x16x256xf32>, tensor<256x3x3x256xf32>) outs(%7 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>\n  return %ret : tensor<256x7x7x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x16x16x256xf32>, %arg1: tensor<256x3x3x256xf32>, %arg2: tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x3x3x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x16x16x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x256xf32>\n    memref.copy %2, %alloc : memref<256x7x7x256xf32> to memref<256x7x7x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x16x16x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x3x3x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x256xf32>\n    return %3 : tensor<256x7x7x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x16x16x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x16x16x256xf32>) -> tensor<256x16x16x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x3x3x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x3x3x256xf32>) -> tensor<256x3x3x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x16x16x256xf32>, tensor<256x3x3x256xf32>) outs(%7 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27898765660}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x640xf32>, %3: tensor<128x1x1x640xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x640xf32>, %arg1: tensor<128x1x1x640xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x640xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x640xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 640 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x640xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x640xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x640xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x640xf32>) -> tensor<128x14x14x640xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x640xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x640xf32>) -> tensor<128x1x1x640xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x640xf32>, tensor<128x1x1x640xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 640, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7672366274}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x152xf32>, tensor<38x1x1x152xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x152xf32>, tensor<38x1x1x152xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x152xf32>, %3: tensor<38x1x1x152xf32>, %7: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x152xf32>, tensor<38x1x1x152xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n  return %ret : tensor<512x1x1x38xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x152xf32>, %arg1: tensor<38x1x1x152xf32>, %arg2: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<38x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x38xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x38xf32>\n    memref.copy %2, %alloc : memref<512x1x1x38xf32> to memref<512x1x1x38xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 38 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<38x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x38xf32>\n    return %3 : tensor<512x1x1x38xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x38xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<38x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<38x1x1x152xf32>) -> tensor<38x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x152xf32>, tensor<38x1x1x152xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x38xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x38xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 38, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10198845}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x19x19x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x19x19x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x19x19x176xf32>, %3: tensor<7x7x176x1xf32>, %7: tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x19x19x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32>\n  return %ret : tensor<128x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x19x19x176xf32>, %arg1: tensor<7x7x176x1xf32>, %arg2: tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x19x19x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<128x7x7x176x1xf32> to memref<128x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x19x19x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x176x1xf32>\n    return %3 : tensor<128x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x19x19x176xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x19x19x176xf32>) -> tensor<128x19x19x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x176x1xf32>) -> tensor<7x7x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x19x19x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<128x7x7x176x1xf32>) -> tensor<128x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 180606938}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x35x35x320xf32>, %3: tensor<32x1x1x320xf32>, %7: tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32>\n  return %ret : tensor<512x35x35x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x35x35x320xf32>, %arg1: tensor<32x1x1x320xf32>, %arg2: tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x35x35x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x35x35x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x35x35x32xf32>\n    memref.copy %2, %alloc : memref<512x35x35x32xf32> to memref<512x35x35x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x35x35x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x35x35x32xf32>\n    return %3 : tensor<512x35x35x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x35x35x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x35x35x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x35x35x320xf32>) -> tensor<512x35x35x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x320xf32>) -> tensor<32x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x35x35x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x320xf32>, tensor<32x1x1x320xf32>) outs(%7 : tensor<512x35x35x32xf32>) -> tensor<512x35x35x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x35x35x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x35x35x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23675152857}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<12x1x1x112xf32>) outs(%7 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<12x1x1x112xf32>) outs(%7 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x112xf32>, %3: tensor<12x1x1x112xf32>, %7: tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<12x1x1x112xf32>) outs(%7 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>\n  return %ret : tensor<128x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x112xf32>, %arg1: tensor<12x1x1x112xf32>, %arg2: tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x12xf32>\n    memref.copy %2, %alloc : memref<128x1x1x12xf32> to memref<128x1x1x12xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x12xf32>\n    return %3 : tensor<128x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x112xf32>) -> tensor<128x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x112xf32>) -> tensor<12x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x112xf32>, tensor<12x1x1x112xf32>) outs(%7 : tensor<128x1x1x12xf32>) -> tensor<128x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 566774}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<128x1x1x96xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<128x1x1x96xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x96xf32>, %3: tensor<128x1x1x96xf32>, %7: tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<128x1x1x96xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n  return %ret : tensor<128x56x56x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x96xf32>, %arg1: tensor<128x1x1x96xf32>, %arg2: tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x128xf32>\n    memref.copy %2, %alloc : memref<128x56x56x128xf32> to memref<128x56x56x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x128xf32>\n    return %3 : tensor<128x56x56x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x96xf32>) -> tensor<128x56x56x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x96xf32>) -> tensor<128x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x96xf32>, tensor<128x1x1x96xf32>) outs(%7 : tensor<128x56x56x128xf32>) -> tensor<128x56x56x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17108615385}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x256xf32>, tensor<1024x1x1x256xf32>) outs(%7 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x256xf32>, tensor<1024x1x1x256xf32>) outs(%7 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x7x7x256xf32>, %3: tensor<1024x1x1x256xf32>, %7: tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x256xf32>, tensor<1024x1x1x256xf32>) outs(%7 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>\n  return %ret : tensor<256x7x7x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x7x7x256xf32>, %arg1: tensor<1024x1x1x256xf32>, %arg2: tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1024x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x7x7x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x1024xf32>\n    memref.copy %2, %alloc : memref<256x7x7x1024xf32> to memref<256x7x7x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 1024 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x7x7x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1024x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x1024xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x1024xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x1024xf32>\n    return %3 : tensor<256x7x7x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x7x7x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x7x7x256xf32>) -> tensor<256x7x7x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1024x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1024x1x1x256xf32>) -> tensor<1024x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x1024xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x7x7x256xf32>, tensor<1024x1x1x256xf32>) outs(%7 : tensor<256x7x7x1024xf32>) -> tensor<256x7x7x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 1024, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12046396215}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x336xf32>, tensor<888x1x1x336xf32>) outs(%7 : tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x336xf32>, tensor<888x1x1x336xf32>) outs(%7 : tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x336xf32>, %3: tensor<888x1x1x336xf32>, %7: tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x336xf32>, tensor<888x1x1x336xf32>) outs(%7 : tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32>\n  return %ret : tensor<256x7x7x888xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x336xf32>, %arg1: tensor<888x1x1x336xf32>, %arg2: tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<888x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x888xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x888xf32>\n    memref.copy %2, %alloc : memref<256x7x7x888xf32> to memref<256x7x7x888xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 888 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<888x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x888xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x888xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x888xf32>\n    return %3 : tensor<256x7x7x888xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x888xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x336xf32>) -> tensor<256x14x14x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<888x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<888x1x1x336xf32>) -> tensor<888x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x888xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x336xf32>, tensor<888x1x1x336xf32>) outs(%7 : tensor<256x7x7x888xf32>) -> tensor<256x7x7x888xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x888xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x888xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 888, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13812986694}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x7x7x1088xf32>, %3: tensor<128x1x1x1088xf32>, %7: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n  return %ret : tensor<128x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x7x7x1088xf32>, %arg1: tensor<128x1x1x1088xf32>, %arg2: tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x7x7x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x128xf32>\n    memref.copy %2, %alloc : memref<128x7x7x128xf32> to memref<128x7x7x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x7x7x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x128xf32>\n    return %3 : tensor<128x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x7x7x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x7x7x1088xf32>) -> tensor<128x7x7x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1088xf32>) -> tensor<128x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x7x7x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<128x7x7x128xf32>) -> tensor<128x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3277505371}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<896x1x1x112xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<896x1x1x112xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x112xf32>, %3: tensor<896x1x1x112xf32>, %7: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<896x1x1x112xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n  return %ret : tensor<512x1x1x896xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x112xf32>, %arg1: tensor<896x1x1x112xf32>, %arg2: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<896x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x896xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x896xf32>\n    memref.copy %2, %alloc : memref<512x1x1x896xf32> to memref<512x1x1x896xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 896 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<896x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x896xf32>\n    return %3 : tensor<512x1x1x896xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x896xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<896x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<896x1x1x112xf32>) -> tensor<896x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x896xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<896x1x1x112xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x896xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x896xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 896, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 171118532}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1440xf32>, %3: tensor<128x1x1x1440xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1440xf32>, %arg1: tensor<128x1x1x1440xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1440xf32>) -> tensor<128x14x14x1440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1440xf32>) -> tensor<128x1x1x1440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17381420171}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x58x58x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x58x58x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x58x58x144xf32>, %3: tensor<3x3x144x1xf32>, %7: tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x58x58x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32>\n  return %ret : tensor<512x56x56x144x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x58x58x144xf32>, %arg1: tensor<3x3x144x1xf32>, %arg2: tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x144x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x58x58x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x144x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x144x1xf32>\n    memref.copy %2, %alloc : memref<512x56x56x144x1xf32> to memref<512x56x56x144x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x58x58x144xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x144x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x144x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x144x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x144x1xf32>\n    return %3 : tensor<512x56x56x144x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x144x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x58x58x144xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x58x58x144xf32>) -> tensor<512x58x58x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x144x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x144x1xf32>) -> tensor<3x3x144x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x144x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x58x58x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x56x56x144x1xf32>) -> tensor<512x56x56x144x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x144x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x144x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5301272861}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1056xf32>, %3: tensor<128x1x1x1056xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1056xf32>, %arg1: tensor<128x1x1x1056xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1056xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1056 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1056xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1056xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1056xf32>) -> tensor<128x14x14x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1056xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1056xf32>) -> tensor<128x1x1x1056xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1056xf32>, tensor<128x1x1x1056xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12721425011}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x46x46x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x46x46x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x46x46x84xf32>, %3: tensor<5x5x84x1xf32>, %7: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x46x46x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n  return %ret : tensor<128x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x46x46x84xf32>, %arg1: tensor<5x5x84x1xf32>, %arg2: tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x46x46x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<128x42x42x84x1xf32> to memref<128x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x46x46x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x42x42x84x1xf32>\n    return %3 : tensor<128x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x46x46x84xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x46x46x84xf32>) -> tensor<128x46x46x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x84x1xf32>) -> tensor<5x5x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x46x46x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<128x42x42x84x1xf32>) -> tensor<128x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1342376490}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x44x44x168xf32>, tensor<3x3x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x44x44x168xf32>, tensor<3x3x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x44x44x168xf32>, %3: tensor<3x3x168x1xf32>, %7: tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x44x44x168xf32>, tensor<3x3x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n  return %ret : tensor<512x42x42x168x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x44x44x168xf32>, %arg1: tensor<3x3x168x1xf32>, %arg2: tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x168x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x44x44x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x168x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x168x1xf32>\n    memref.copy %2, %alloc : memref<512x42x42x168x1xf32> to memref<512x42x42x168x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x44x44x168xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x168x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x168x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x168x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x168x1xf32>\n    return %3 : tensor<512x42x42x168x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x168x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x44x44x168xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x44x44x168xf32>) -> tensor<512x44x44x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x168x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x168x1xf32>) -> tensor<3x3x168x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x168x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x44x44x168xf32>, tensor<3x3x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x168x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x168x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3422203963}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<696x1x1x174xf32>) outs(%7 : tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<696x1x1x174xf32>) outs(%7 : tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x174xf32>, %3: tensor<696x1x1x174xf32>, %7: tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<696x1x1x174xf32>) outs(%7 : tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32>\n  return %ret : tensor<256x1x1x696xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x174xf32>, %arg1: tensor<696x1x1x174xf32>, %arg2: tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<696x1x1x174xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x174xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x696xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x696xf32>\n    memref.copy %2, %alloc : memref<256x1x1x696xf32> to memref<256x1x1x696xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 696 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 174 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x174xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<696x1x1x174xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x696xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x696xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x696xf32>\n    return %3 : tensor<256x1x1x696xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x696xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x174xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x174xf32>) -> tensor<256x1x1x174xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<696x1x1x174xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<696x1x1x174xf32>) -> tensor<696x1x1x174xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x696xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x174xf32>, tensor<696x1x1x174xf32>) outs(%7 : tensor<256x1x1x696xf32>) -> tensor<256x1x1x696xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x696xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x696xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 696, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 174, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 108187411}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<324x1x1x1296xf32>) outs(%7 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<324x1x1x1296xf32>) outs(%7 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1296xf32>, %3: tensor<324x1x1x1296xf32>, %7: tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<324x1x1x1296xf32>) outs(%7 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>\n  return %ret : tensor<512x1x1x324xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1296xf32>, %arg1: tensor<324x1x1x1296xf32>, %arg2: tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<324x1x1x1296xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1296xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x324xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x324xf32>\n    memref.copy %2, %alloc : memref<512x1x1x324xf32> to memref<512x1x1x324xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 324 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1296 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1296xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<324x1x1x1296xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x324xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x324xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x324xf32>\n    return %3 : tensor<512x1x1x324xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x324xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<324x1x1x1296xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<324x1x1x1296xf32>) -> tensor<324x1x1x1296xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x324xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<324x1x1x1296xf32>) outs(%7 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x324xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x324xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 324, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1296, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 804052844}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<28x1x1x256xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<28x1x1x256xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x256xf32>, %3: tensor<28x1x1x256xf32>, %7: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<28x1x1x256xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n  return %ret : tensor<512x1x1x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x256xf32>, %arg1: tensor<28x1x1x256xf32>, %arg2: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<28x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x28xf32>\n    memref.copy %2, %alloc : memref<512x1x1x28xf32> to memref<512x1x1x28xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<28x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x28xf32>\n    return %3 : tensor<512x1x1x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<28x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<28x1x1x256xf32>) -> tensor<28x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x28xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<28x1x1x256xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13274618}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x736xf32>, %3: tensor<128x1x1x736xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x736xf32>, %arg1: tensor<128x1x1x736xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x736xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x736xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 736 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x736xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x736xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x736xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x736xf32>) -> tensor<512x7x7x736xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x736xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x736xf32>) -> tensor<128x1x1x736xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x736xf32>, tensor<128x1x1x736xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 736, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8837690781}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x1280xf32>, tensor<192x1x1x1280xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x1280xf32>, tensor<192x1x1x1280xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x8x8x1280xf32>, %3: tensor<192x1x1x1280xf32>, %7: tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x1280xf32>, tensor<192x1x1x1280xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n  return %ret : tensor<128x8x8x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x8x8x1280xf32>, %arg1: tensor<192x1x1x1280xf32>, %arg2: tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x1280xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x8x8x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x8x8x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x8x8x192xf32>\n    memref.copy %2, %alloc : memref<128x8x8x192xf32> to memref<128x8x8x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1280 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x8x8x1280xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x1280xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x8x8x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x8x8x192xf32>\n    return %3 : tensor<128x8x8x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x8x8x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x8x8x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x8x8x1280xf32>) -> tensor<128x8x8x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x1280xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x1280xf32>) -> tensor<192x1x1x1280xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x8x8x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x8x8x1280xf32>, tensor<192x1x1x1280xf32>) outs(%7 : tensor<128x8x8x192xf32>) -> tensor<128x8x8x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x8x8x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x8x8x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7573659311}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x60x60x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x60x60x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x60x60x11xf32>, %3: tensor<5x5x11x1xf32>, %7: tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x60x60x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n  return %ret : tensor<256x56x56x11x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x60x60x11xf32>, %arg1: tensor<5x5x11x1xf32>, %arg2: tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x11x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x60x60x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x11x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x11x1xf32>\n    memref.copy %2, %alloc : memref<256x56x56x11x1xf32> to memref<256x56x56x11x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x60x60x11xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x11x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x11x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x56x56x11x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x11x1xf32>\n    return %3 : tensor<256x56x56x11x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x11x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x60x60x11xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x60x60x11xf32>) -> tensor<256x60x60x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x11x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x11x1xf32>) -> tensor<5x5x11x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x11x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x60x60x11xf32>, tensor<5x5x11x1xf32>) outs(%7 : tensor<256x56x56x11x1xf32>) -> tensor<256x56x56x11x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x11x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x11x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 630053915}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x22xf32>, tensor<22x1x1x22xf32>) outs(%7 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x22xf32>, tensor<22x1x1x22xf32>) outs(%7 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x22xf32>, %3: tensor<22x1x1x22xf32>, %7: tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x22xf32>, tensor<22x1x1x22xf32>) outs(%7 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>\n  return %ret : tensor<128x28x28x22xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x22xf32>, %arg1: tensor<22x1x1x22xf32>, %arg2: tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<22x1x1x22xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x22xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x22xf32>\n    memref.copy %2, %alloc : memref<128x28x28x22xf32> to memref<128x28x28x22xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 22 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x22xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<22x1x1x22xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x22xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x22xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x22xf32>\n    return %3 : tensor<128x28x28x22xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x22xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x22xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<22x1x1x22xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<22x1x1x22xf32>) -> tensor<22x1x1x22xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x22xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x22xf32>, tensor<22x1x1x22xf32>) outs(%7 : tensor<128x28x28x22xf32>) -> tensor<128x28x28x22xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x22xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x22xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 22, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 127983902}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x112xf32>, tensor<112x1x1x112xf32>) outs(%7 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x112xf32>, tensor<112x1x1x112xf32>) outs(%7 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x112xf32>, %3: tensor<112x1x1x112xf32>, %7: tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x112xf32>, tensor<112x1x1x112xf32>) outs(%7 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>\n  return %ret : tensor<256x28x28x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x112xf32>, %arg1: tensor<112x1x1x112xf32>, %arg2: tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x112xf32>\n    memref.copy %2, %alloc : memref<256x28x28x112xf32> to memref<256x28x28x112xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x112xf32>\n    return %3 : tensor<256x28x28x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x112xf32>) -> tensor<112x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x112xf32>, tensor<112x1x1x112xf32>) outs(%7 : tensor<256x28x28x112xf32>) -> tensor<256x28x28x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8826206333}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x20x20x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x20x20x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x20x20x88xf32>, %3: tensor<7x7x88x1xf32>, %7: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x20x20x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n  return %ret : tensor<128x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x20x20x88xf32>, %arg1: tensor<7x7x88x1xf32>, %arg2: tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x20x20x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<128x14x14x88x1xf32> to memref<128x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x20x20x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x88x1xf32>\n    return %3 : tensor<128x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x20x20x88xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x20x20x88xf32>) -> tensor<128x20x20x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x88x1xf32>) -> tensor<7x7x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x20x20x88xf32>, tensor<7x7x88x1xf32>) outs(%7 : tensor<128x14x14x88x1xf32>) -> tensor<128x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 351878864}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x120xf32>, tensor<30x1x1x120xf32>) outs(%7 : tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x120xf32>, tensor<30x1x1x120xf32>) outs(%7 : tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x120xf32>, %3: tensor<30x1x1x120xf32>, %7: tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x120xf32>, tensor<30x1x1x120xf32>) outs(%7 : tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32>\n  return %ret : tensor<256x1x1x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x120xf32>, %arg1: tensor<30x1x1x120xf32>, %arg2: tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<30x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x30xf32>\n    memref.copy %2, %alloc : memref<256x1x1x30xf32> to memref<256x1x1x30xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<30x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x30xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x30xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x30xf32>\n    return %3 : tensor<256x1x1x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x120xf32>) -> tensor<256x1x1x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<30x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<30x1x1x120xf32>) -> tensor<30x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x30xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x120xf32>, tensor<30x1x1x120xf32>) outs(%7 : tensor<256x1x1x30xf32>) -> tensor<256x1x1x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x30xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3187819}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x64xf32>, %3: tensor<64x1x1x64xf32>, %7: tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n  return %ret : tensor<512x28x28x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x64xf32>, %arg1: tensor<64x1x1x64xf32>, %arg2: tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x64xf32>\n    memref.copy %2, %alloc : memref<512x28x28x64xf32> to memref<512x28x28x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x64xf32>\n    return %3 : tensor<512x28x28x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x64xf32>) -> tensor<64x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5479957416}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x13x13x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x13x13x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x13x13x176xf32>, %3: tensor<7x7x176x1xf32>, %7: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x13x13x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n  return %ret : tensor<256x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x13x13x176xf32>, %arg1: tensor<7x7x176x1xf32>, %arg2: tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x13x13x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<256x7x7x176x1xf32> to memref<256x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x13x13x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x176x1xf32>\n    return %3 : tensor<256x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x13x13x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x13x13x176xf32>) -> tensor<256x13x13x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x176x1xf32>) -> tensor<7x7x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x13x13x176xf32>, tensor<7x7x176x1xf32>) outs(%7 : tensor<256x7x7x176x1xf32>) -> tensor<256x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 357153323}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<56x1x1x448xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<56x1x1x448xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x448xf32>, %3: tensor<56x1x1x448xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<56x1x1x448xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x448xf32>, %arg1: tensor<56x1x1x448xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x448xf32>) -> tensor<512x1x1x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x448xf32>) -> tensor<56x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<56x1x1x448xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 47000710}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<16x1x1x32xf32>) outs(%7 : tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<16x1x1x32xf32>) outs(%7 : tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x112x112x32xf32>, %3: tensor<16x1x1x32xf32>, %7: tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<16x1x1x32xf32>) outs(%7 : tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32>\n  return %ret : tensor<128x112x112x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x112x112x32xf32>, %arg1: tensor<16x1x1x32xf32>, %arg2: tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x112x112x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x112x112x16xf32>\n    memref.copy %2, %alloc : memref<128x112x112x16xf32> to memref<128x112x112x16xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<16x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x16xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x112x112x16xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x112x112x16xf32>\n    return %3 : tensor<128x112x112x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x112x112x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x112x112x32xf32>) -> tensor<128x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<16x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<16x1x1x32xf32>) -> tensor<16x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x112x112x16xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x112x112x32xf32>, tensor<16x1x1x32xf32>) outs(%7 : tensor<128x112x112x16xf32>) -> tensor<128x112x112x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x112x112x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x112x112x16xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2271336723}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<24x1x1x24xf32>) outs(%7 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<24x1x1x24xf32>) outs(%7 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x56x56x24xf32>, %3: tensor<24x1x1x24xf32>, %7: tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<24x1x1x24xf32>) outs(%7 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n  return %ret : tensor<128x56x56x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x56x56x24xf32>, %arg1: tensor<24x1x1x24xf32>, %arg2: tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x56x56x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x56x56x24xf32>\n    memref.copy %2, %alloc : memref<128x56x56x24xf32> to memref<128x56x56x24xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x56x56x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x56x56x24xf32>\n    return %3 : tensor<128x56x56x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x56x56x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x24xf32>) -> tensor<24x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x56x56x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x56x56x24xf32>, tensor<24x1x1x24xf32>) outs(%7 : tensor<128x56x56x24xf32>) -> tensor<128x56x56x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x56x56x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x56x56x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 626969941}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x160xf32>, tensor<128x1x1x160xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x160xf32>, tensor<128x1x1x160xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x28x28x160xf32>, %3: tensor<128x1x1x160xf32>, %7: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x160xf32>, tensor<128x1x1x160xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n  return %ret : tensor<256x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x28x28x160xf32>, %arg1: tensor<128x1x1x160xf32>, %arg2: tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x28x28x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x128xf32>\n    memref.copy %2, %alloc : memref<256x28x28x128xf32> to memref<256x28x28x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x28x28x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x128xf32>\n    return %3 : tensor<256x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x28x28x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x28x28x160xf32>) -> tensor<256x28x28x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x160xf32>) -> tensor<128x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x28x28x160xf32>, tensor<128x1x1x160xf32>) outs(%7 : tensor<256x28x28x128xf32>) -> tensor<256x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14766239380}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x35x35x192xf32>, %3: tensor<32x1x1x192xf32>, %7: tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32>\n  return %ret : tensor<256x35x35x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x35x35x192xf32>, %arg1: tensor<32x1x1x192xf32>, %arg2: tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x35x35x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x35x35x32xf32>\n    memref.copy %2, %alloc : memref<256x35x35x32xf32> to memref<256x35x35x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x35x35x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x35x35x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x35x35x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x35x35x32xf32>\n    return %3 : tensor<256x35x35x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x35x35x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x35x35x192xf32>) -> tensor<256x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x192xf32>) -> tensor<32x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x35x35x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x35x35x192xf32>, tensor<32x1x1x192xf32>) outs(%7 : tensor<256x35x35x32xf32>) -> tensor<256x35x35x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x35x35x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x35x35x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6984918970}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x288xf32>, tensor<64x1x1x288xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x288xf32>, tensor<64x1x1x288xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x35x35x288xf32>, %3: tensor<64x1x1x288xf32>, %7: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x288xf32>, tensor<64x1x1x288xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n  return %ret : tensor<128x35x35x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x35x35x288xf32>, %arg1: tensor<64x1x1x288xf32>, %arg2: tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x35x35x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x35x35x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x35x35x64xf32>\n    memref.copy %2, %alloc : memref<128x35x35x64xf32> to memref<128x35x35x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x35x35x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x35x35x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x35x35x64xf32>\n    return %3 : tensor<128x35x35x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x35x35x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x35x35x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x35x35x288xf32>) -> tensor<128x35x35x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x288xf32>) -> tensor<64x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x35x35x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x35x35x288xf32>, tensor<64x1x1x288xf32>) outs(%7 : tensor<128x35x35x64xf32>) -> tensor<128x35x35x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x35x35x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x35x35x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10622720904}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x120xf32>, tensor<12x1x1x120xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x120xf32>, tensor<12x1x1x120xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x120xf32>, %3: tensor<12x1x1x120xf32>, %7: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x120xf32>, tensor<12x1x1x120xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n  return %ret : tensor<512x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x120xf32>, %arg1: tensor<12x1x1x120xf32>, %arg2: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x12xf32>\n    memref.copy %2, %alloc : memref<512x1x1x12xf32> to memref<512x1x1x12xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x12xf32>\n    return %3 : tensor<512x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x120xf32>) -> tensor<12x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x120xf32>, tensor<12x1x1x120xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2468449}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1024xf32>, tensor<512x1x1x1024xf32>) outs(%7 : tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1024xf32>, tensor<512x1x1x1024xf32>) outs(%7 : tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x1024xf32>, %3: tensor<512x1x1x1024xf32>, %7: tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1024xf32>, tensor<512x1x1x1024xf32>) outs(%7 : tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32>\n  return %ret : tensor<256x7x7x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x1024xf32>, %arg1: tensor<512x1x1x1024xf32>, %arg2: tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<512x1x1x1024xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x512xf32>\n    memref.copy %2, %alloc : memref<256x7x7x512xf32> to memref<256x7x7x512xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1024 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x1024xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<512x1x1x1024xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x512xf32>\n    return %3 : tensor<256x7x7x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x1024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x1024xf32>) -> tensor<256x14x14x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<512x1x1x1024xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<512x1x1x1024xf32>) -> tensor<512x1x1x1024xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x1024xf32>, tensor<512x1x1x1024xf32>) outs(%7 : tensor<256x7x7x512xf32>) -> tensor<256x7x7x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24665693050}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x36xf32>, tensor<288x1x1x36xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x36xf32>, tensor<288x1x1x36xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x36xf32>, %3: tensor<288x1x1x36xf32>, %7: tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x36xf32>, tensor<288x1x1x36xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n  return %ret : tensor<128x1x1x288xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x36xf32>, %arg1: tensor<288x1x1x36xf32>, %arg2: tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<288x1x1x36xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x36xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x288xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x288xf32>\n    memref.copy %2, %alloc : memref<128x1x1x288xf32> to memref<128x1x1x288xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 288 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 36 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x36xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<288x1x1x36xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x288xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x288xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x288xf32>\n    return %3 : tensor<128x1x1x288xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x288xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x36xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x36xf32>) -> tensor<128x1x1x36xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<288x1x1x36xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<288x1x1x36xf32>) -> tensor<288x1x1x36xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x288xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x36xf32>, tensor<288x1x1x36xf32>) outs(%7 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x288xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x288xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 288, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 36, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3365061}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<120x1x1x12xf32>) outs(%7 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<120x1x1x12xf32>) outs(%7 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x12xf32>, %3: tensor<120x1x1x12xf32>, %7: tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<120x1x1x12xf32>) outs(%7 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>\n  return %ret : tensor<512x1x1x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x12xf32>, %arg1: tensor<120x1x1x12xf32>, %arg2: tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x12xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x12xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x120xf32>\n    memref.copy %2, %alloc : memref<512x1x1x120xf32> to memref<512x1x1x120xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 12 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x12xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x12xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x120xf32>\n    return %3 : tensor<512x1x1x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x12xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x12xf32>) -> tensor<120x1x1x12xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<120x1x1x12xf32>) outs(%7 : tensor<512x1x1x120xf32>) -> tensor<512x1x1x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 12, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1353671}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x169x169x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x169x169x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x169x169x42xf32>, %3: tensor<5x5x42x1xf32>, %7: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x169x169x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n  return %ret : tensor<128x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x169x169x42xf32>, %arg1: tensor<5x5x42x1xf32>, %arg2: tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x169x169x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<128x83x83x42x1xf32> to memref<128x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<128x169x169x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<128x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x83x83x42x1xf32>\n    return %3 : tensor<128x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x169x169x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x169x169x42xf32>) -> tensor<128x169x169x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x42x1xf32>) -> tensor<5x5x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x169x169x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<128x83x83x42x1xf32>) -> tensor<128x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 2621001095}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x288xf32>, tensor<672x1x1x288xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x288xf32>, tensor<672x1x1x288xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x288xf32>, %3: tensor<672x1x1x288xf32>, %7: tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x288xf32>, tensor<672x1x1x288xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n  return %ret : tensor<128x7x7x672xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x288xf32>, %arg1: tensor<672x1x1x288xf32>, %arg2: tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<672x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x7x7x672xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x7x7x672xf32>\n    memref.copy %2, %alloc : memref<128x7x7x672xf32> to memref<128x7x7x672xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<672x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x672xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x7x7x672xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x7x7x672xf32>\n    return %3 : tensor<128x7x7x672xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x7x7x672xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x288xf32>) -> tensor<128x14x14x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<672x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<672x1x1x288xf32>) -> tensor<672x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x7x7x672xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x288xf32>, tensor<672x1x1x288xf32>) outs(%7 : tensor<128x7x7x672xf32>) -> tensor<128x7x7x672xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x7x7x672xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x7x7x672xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4461684506}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<144x1x1x1296xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<144x1x1x1296xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1296xf32>, %3: tensor<144x1x1x1296xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<144x1x1x1296xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1296xf32>, %arg1: tensor<144x1x1x1296xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x1296xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1296xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1296 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1296xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x1296xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x1296xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x1296xf32>) -> tensor<144x1x1x1296xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1296xf32>, tensor<144x1x1x1296xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1296, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 357936718}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<104x1x1x12xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<104x1x1x12xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x12xf32>, %3: tensor<104x1x1x12xf32>, %7: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<104x1x1x12xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n  return %ret : tensor<512x1x1x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x12xf32>, %arg1: tensor<104x1x1x12xf32>, %arg2: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x12xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x12xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x104xf32>\n    memref.copy %2, %alloc : memref<512x1x1x104xf32> to memref<512x1x1x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 12 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x12xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x12xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x104xf32>\n    return %3 : tensor<512x1x1x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x12xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x12xf32>) -> tensor<104x1x1x12xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x12xf32>, tensor<104x1x1x12xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 12, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1176533}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<768x1x1x320xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<768x1x1x320xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x320xf32>, %3: tensor<768x1x1x320xf32>, %7: tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<768x1x1x320xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n  return %ret : tensor<256x7x7x768xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x320xf32>, %arg1: tensor<768x1x1x320xf32>, %arg2: tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<768x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x7x7x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x7x7x768xf32>\n    memref.copy %2, %alloc : memref<256x7x7x768xf32> to memref<256x7x7x768xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 768 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<768x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x768xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x7x7x768xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x7x7x768xf32>\n    return %3 : tensor<256x7x7x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x7x7x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x320xf32>) -> tensor<256x14x14x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<768x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<768x1x1x320xf32>) -> tensor<768x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x7x7x768xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x320xf32>, tensor<768x1x1x320xf32>) outs(%7 : tensor<256x7x7x768xf32>) -> tensor<256x7x7x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x7x7x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x7x7x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 768, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11384880291}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x152xf32>, %3: tensor<368x1x1x152xf32>, %7: tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32>\n  return %ret : tensor<256x14x14x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x152xf32>, %arg1: tensor<368x1x1x152xf32>, %arg2: tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x368xf32>\n    memref.copy %2, %alloc : memref<256x14x14x368xf32> to memref<256x14x14x368xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x368xf32>\n    return %3 : tensor<256x14x14x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x152xf32>) -> tensor<256x14x14x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x152xf32>) -> tensor<368x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x152xf32>, tensor<368x1x1x152xf32>) outs(%7 : tensor<256x14x14x368xf32>) -> tensor<256x14x14x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10035481248}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<24x1x1x8xf32>) outs(%7 : tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<24x1x1x8xf32>) outs(%7 : tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<24x1x1x8xf32>, %7: tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<24x1x1x8xf32>) outs(%7 : tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32>\n  return %ret : tensor<512x1x1x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<24x1x1x8xf32>, %arg2: tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x24xf32>\n    memref.copy %2, %alloc : memref<512x1x1x24xf32> to memref<512x1x1x24xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x24xf32>\n    return %3 : tensor<512x1x1x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x8xf32>) -> tensor<24x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<24x1x1x8xf32>) outs(%7 : tensor<512x1x1x24xf32>) -> tensor<512x1x1x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 150021}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<96x1x1x32xf32>, %7: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n  return %ret : tensor<512x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<96x1x1x32xf32>, %arg2: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x96xf32>\n    memref.copy %2, %alloc : memref<512x56x56x96xf32> to memref<512x56x56x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x96xf32>\n    return %3 : tensor<512x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x32xf32>) -> tensor<96x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<96x1x1x32xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13903880275}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1600xf32>, %3: tensor<128x1x1x1600xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1600xf32>, %arg1: tensor<128x1x1x1600xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1600xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1600xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1600 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1600xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1600xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1600xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1600xf32>) -> tensor<512x7x7x1600xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1600xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1600xf32>) -> tensor<128x1x1x1600xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1600xf32>, tensor<128x1x1x1600xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1600, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19322328598}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<348x1x1x1392xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<348x1x1x1392xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x1392xf32>, %3: tensor<348x1x1x1392xf32>, %7: tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<348x1x1x1392xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n  return %ret : tensor<256x1x1x348xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x1392xf32>, %arg1: tensor<348x1x1x1392xf32>, %arg2: tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<348x1x1x1392xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x1392xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x348xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x348xf32>\n    memref.copy %2, %alloc : memref<256x1x1x348xf32> to memref<256x1x1x348xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 348 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1392 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x1392xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<348x1x1x1392xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x348xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x348xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x348xf32>\n    return %3 : tensor<256x1x1x348xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x348xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x1392xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x1392xf32>) -> tensor<256x1x1x1392xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<348x1x1x1392xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<348x1x1x1392xf32>) -> tensor<348x1x1x1392xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x348xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x1392xf32>, tensor<348x1x1x1392xf32>) outs(%7 : tensor<256x1x1x348xf32>) -> tensor<256x1x1x348xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x348xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x348xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 348, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1392, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 464106012}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<160x7x1x160xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<160x7x1x160xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x23x17x160xf32>, %3: tensor<160x7x1x160xf32>, %7: tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<160x7x1x160xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n  return %ret : tensor<128x17x17x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x23x17x160xf32>, %arg1: tensor<160x7x1x160xf32>, %arg2: tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x7x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x23x17x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x17x17x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x17x17x160xf32>\n    memref.copy %2, %alloc : memref<128x17x17x160xf32> to memref<128x17x17x160xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x23x17x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x7x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x17x17x160xf32>\n    return %3 : tensor<128x17x17x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x17x17x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x23x17x160xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x23x17x160xf32>) -> tensor<128x23x17x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x7x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x7x1x160xf32>) -> tensor<160x7x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x17x17x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x160xf32>, tensor<160x7x1x160xf32>) outs(%7 : tensor<128x17x17x160xf32>) -> tensor<128x17x17x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x17x17x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x17x17x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24946896551}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<72x1x1x32xf32>, %7: tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>\n  return %ret : tensor<256x56x56x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<72x1x1x32xf32>, %arg2: tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x72xf32>\n    memref.copy %2, %alloc : memref<256x56x56x72xf32> to memref<256x56x56x72xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x72xf32>\n    return %3 : tensor<256x56x56x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x32xf32>) -> tensor<72x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<72x1x1x32xf32>) outs(%7 : tensor<256x56x56x72xf32>) -> tensor<256x56x56x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5119353460}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x32x32x44xf32>, tensor<5x5x44x1xf32>) outs(%7 : tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x32x32x44xf32>, tensor<5x5x44x1xf32>) outs(%7 : tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<256x32x32x44xf32>, %3: tensor<5x5x44x1xf32>, %7: tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x32x32x44xf32>, tensor<5x5x44x1xf32>) outs(%7 : tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32>\n  return %ret : tensor<256x28x28x44x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x32x44xf32>, %arg1: tensor<5x5x44x1xf32>, %arg2: tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x44x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x32x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x28x28x44x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x28x28x44x1xf32>\n    memref.copy %2, %alloc : memref<256x28x28x44x1xf32> to memref<256x28x28x44x1xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<256x32x32x44xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x44x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x44x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<256x28x28x44x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x28x28x44x1xf32>\n    return %3 : tensor<256x28x28x44x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x28x28x44x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<256x32x32x44xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<256x32x32x44xf32>) -> tensor<256x32x32x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x44x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x44x1xf32>) -> tensor<5x5x44x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x28x28x44x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<256x32x32x44xf32>, tensor<5x5x44x1xf32>) outs(%7 : tensor<256x28x28x44x1xf32>) -> tensor<256x28x28x44x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x28x28x44x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x28x28x44x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 623593414}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x14x14x1568xf32>, %3: tensor<128x1x1x1568xf32>, %7: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n  return %ret : tensor<128x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x14x14x1568xf32>, %arg1: tensor<128x1x1x1568xf32>, %arg2: tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1568xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x14x14x1568xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x14x14x128xf32>\n    memref.copy %2, %alloc : memref<128x14x14x128xf32> to memref<128x14x14x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1568 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x14x14x1568xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1568xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x14x14x128xf32>\n    return %3 : tensor<128x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x14x14x1568xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x14x14x1568xf32>) -> tensor<128x14x14x1568xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1568xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1568xf32>) -> tensor<128x1x1x1568xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x14x14x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<128x14x14x128xf32>) -> tensor<128x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1568, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18946184050}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x888xf32>, tensor<222x1x1x888xf32>) outs(%7 : tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x888xf32>, tensor<222x1x1x888xf32>) outs(%7 : tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x888xf32>, %3: tensor<222x1x1x888xf32>, %7: tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x888xf32>, tensor<222x1x1x888xf32>) outs(%7 : tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32>\n  return %ret : tensor<256x1x1x222xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x888xf32>, %arg1: tensor<222x1x1x888xf32>, %arg2: tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<222x1x1x888xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x888xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x222xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x222xf32>\n    memref.copy %2, %alloc : memref<256x1x1x222xf32> to memref<256x1x1x222xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 222 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 888 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x888xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<222x1x1x888xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x222xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x222xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x222xf32>\n    return %3 : tensor<256x1x1x222xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x222xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x888xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x888xf32>) -> tensor<256x1x1x888xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<222x1x1x888xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<222x1x1x888xf32>) -> tensor<222x1x1x888xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x222xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x888xf32>, tensor<222x1x1x888xf32>) outs(%7 : tensor<256x1x1x222xf32>) -> tensor<256x1x1x222xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x222xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x222xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 222, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 888, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 187820091}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x128xf32>, tensor<192x7x1x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x128xf32>, tensor<192x7x1x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x23x17x128xf32>, %3: tensor<192x7x1x128xf32>, %7: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x128xf32>, tensor<192x7x1x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n  return %ret : tensor<128x17x17x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x23x17x128xf32>, %arg1: tensor<192x7x1x128xf32>, %arg2: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x7x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x23x17x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x17x17x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x17x17x192xf32>\n    memref.copy %2, %alloc : memref<128x17x17x192xf32> to memref<128x17x17x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x23x17x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x7x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x17x17x192xf32>\n    return %3 : tensor<128x17x17x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x17x17x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x23x17x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x23x17x128xf32>) -> tensor<128x23x17x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x7x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x7x1x128xf32>) -> tensor<192x7x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x17x17x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x23x17x128xf32>, tensor<192x7x1x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x17x17x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x17x17x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23930454951}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x112x112x32xf32>, %3: tensor<32x1x1x32xf32>, %7: tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>\n  return %ret : tensor<256x56x56x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x112x112x32xf32>, %arg1: tensor<32x1x1x32xf32>, %arg2: tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x32xf32>\n    memref.copy %2, %alloc : memref<256x56x56x32xf32> to memref<256x56x56x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x32xf32>\n    return %3 : tensor<256x56x56x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x112x112x32xf32>) -> tensor<256x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x32xf32>) -> tensor<32x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<256x112x112x32xf32>, tensor<32x1x1x32xf32>) outs(%7 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2275408507}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x608xf32>, %3: tensor<128x1x1x608xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x608xf32>, %arg1: tensor<128x1x1x608xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x608xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x608xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x608xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x608xf32>) -> tensor<512x14x14x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x608xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x608xf32>) -> tensor<128x1x1x608xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x608xf32>, tensor<128x1x1x608xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 29146326676}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x256xf32>, %3: tensor<64x1x1x256xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x256xf32>, %arg1: tensor<64x1x1x256xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x256xf32>) -> tensor<64x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x256xf32>, tensor<64x1x1x256xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 30074382}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x160xf32>, tensor<960x1x1x160xf32>) outs(%7 : tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x160xf32>, tensor<960x1x1x160xf32>) outs(%7 : tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x160xf32>, %3: tensor<960x1x1x160xf32>, %7: tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x160xf32>, tensor<960x1x1x160xf32>) outs(%7 : tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32>\n  return %ret : tensor<512x7x7x960xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x160xf32>, %arg1: tensor<960x1x1x160xf32>, %arg2: tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<960x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x960xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x960xf32>\n    memref.copy %2, %alloc : memref<512x7x7x960xf32> to memref<512x7x7x960xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 960 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<960x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x960xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x960xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x960xf32>\n    return %3 : tensor<512x7x7x960xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x960xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<960x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<960x1x1x160xf32>) -> tensor<960x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x960xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x160xf32>, tensor<960x1x1x160xf32>) outs(%7 : tensor<512x7x7x960xf32>) -> tensor<512x7x7x960xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x960xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x960xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 960, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13831506866}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<72x1x1x576xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<72x1x1x576xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x576xf32>, %3: tensor<72x1x1x576xf32>, %7: tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<72x1x1x576xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n  return %ret : tensor<128x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x576xf32>, %arg1: tensor<72x1x1x576xf32>, %arg2: tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x72xf32>\n    memref.copy %2, %alloc : memref<128x1x1x72xf32> to memref<128x1x1x72xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x72xf32>\n    return %3 : tensor<128x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x576xf32>) -> tensor<128x1x1x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x576xf32>) -> tensor<72x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x576xf32>, tensor<72x1x1x576xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19562536}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<30x1x1x336xf32>) outs(%7 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<30x1x1x336xf32>) outs(%7 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x336xf32>, %3: tensor<30x1x1x336xf32>, %7: tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<30x1x1x336xf32>) outs(%7 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>\n  return %ret : tensor<128x1x1x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x336xf32>, %arg1: tensor<30x1x1x336xf32>, %arg2: tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<30x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x30xf32>\n    memref.copy %2, %alloc : memref<128x1x1x30xf32> to memref<128x1x1x30xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<30x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x30xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x30xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x30xf32>\n    return %3 : tensor<128x1x1x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x336xf32>) -> tensor<128x1x1x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<30x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<30x1x1x336xf32>) -> tensor<30x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x30xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x336xf32>, tensor<30x1x1x336xf32>) outs(%7 : tensor<128x1x1x30xf32>) -> tensor<128x1x1x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x30xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4909085}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x38xf32>, tensor<152x1x1x38xf32>) outs(%7 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x38xf32>, tensor<152x1x1x38xf32>) outs(%7 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x38xf32>, %3: tensor<152x1x1x38xf32>, %7: tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x38xf32>, tensor<152x1x1x38xf32>) outs(%7 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>\n  return %ret : tensor<128x1x1x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x38xf32>, %arg1: tensor<152x1x1x38xf32>, %arg2: tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x38xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x38xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x152xf32>\n    memref.copy %2, %alloc : memref<128x1x1x152xf32> to memref<128x1x1x152xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 38 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x38xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x38xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x152xf32>\n    return %3 : tensor<128x1x1x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x38xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x38xf32>) -> tensor<128x1x1x38xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x38xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x38xf32>) -> tensor<152x1x1x38xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x38xf32>, tensor<152x1x1x38xf32>) outs(%7 : tensor<128x1x1x152xf32>) -> tensor<128x1x1x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 38, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1849603}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<8x1x1x224xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<8x1x1x224xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x1x1x224xf32>, %3: tensor<8x1x1x224xf32>, %7: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<8x1x1x224xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n  return %ret : tensor<256x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1x1x224xf32>, %arg1: tensor<8x1x1x224xf32>, %arg2: tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1x1x8xf32>\n    memref.copy %2, %alloc : memref<256x1x1x8xf32> to memref<256x1x1x8xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1x1x8xf32>\n    return %3 : tensor<256x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x1x1x224xf32>) -> tensor<256x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x224xf32>) -> tensor<8x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x1x1x224xf32>, tensor<8x1x1x224xf32>) outs(%7 : tensor<256x1x1x8xf32>) -> tensor<256x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1609618}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x56x56x32xf32>, %3: tensor<64x1x1x32xf32>, %7: tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n  return %ret : tensor<256x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x56x56x32xf32>, %arg1: tensor<64x1x1x32xf32>, %arg2: tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x56x56x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x56x56x64xf32>\n    memref.copy %2, %alloc : memref<256x56x56x64xf32> to memref<256x56x56x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x56x56x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x56x56x64xf32>\n    return %3 : tensor<256x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x56x56x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x56x56x32xf32>) -> tensor<256x56x56x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x32xf32>) -> tensor<64x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x56x56x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<256x56x56x64xf32>) -> tensor<256x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4536614179}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x28x28x64xf32>, %3: tensor<256x1x1x64xf32>, %7: tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n  return %ret : tensor<128x28x28x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x28x28x64xf32>, %arg1: tensor<256x1x1x64xf32>, %arg2: tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x28x28x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x28x28x256xf32>\n    memref.copy %2, %alloc : memref<128x28x28x256xf32> to memref<128x28x28x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x28x28x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x28x28x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x28x28x256xf32>\n    return %3 : tensor<128x28x28x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x28x28x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x28x28x64xf32>) -> tensor<128x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x28x28x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x28x28x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<128x28x28x256xf32>) -> tensor<128x28x28x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x28x28x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x28x28x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5453916504}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x42xf32>, tensor<448x1x1x42xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x42xf32>, tensor<448x1x1x42xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x42xf32>, %3: tensor<448x1x1x42xf32>, %7: tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x42xf32>, tensor<448x1x1x42xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n  return %ret : tensor<128x1x1x448xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x42xf32>, %arg1: tensor<448x1x1x42xf32>, %arg2: tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<448x1x1x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x448xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x448xf32>\n    memref.copy %2, %alloc : memref<128x1x1x448xf32> to memref<128x1x1x448xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 448 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x42xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<448x1x1x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x448xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x448xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x448xf32>\n    return %3 : tensor<128x1x1x448xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x448xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x42xf32>) -> tensor<128x1x1x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<448x1x1x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<448x1x1x42xf32>) -> tensor<448x1x1x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x448xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x42xf32>, tensor<448x1x1x42xf32>) outs(%7 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x448xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x448xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 448, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6272373}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<256x14x14x288xf32>, %3: tensor<128x1x1x288xf32>, %7: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n  return %ret : tensor<256x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x14x14x288xf32>, %arg1: tensor<128x1x1x288xf32>, %arg2: tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x288xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x14x14x288xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x14x14x128xf32>\n    memref.copy %2, %alloc : memref<256x14x14x128xf32> to memref<256x14x14x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 288 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<256x14x14x288xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x288xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x14x14x128xf32>\n    return %3 : tensor<256x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<256x14x14x288xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<256x14x14x288xf32>) -> tensor<256x14x14x288xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x288xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x288xf32>) -> tensor<128x1x1x288xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<256x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<256x14x14x288xf32>, tensor<128x1x1x288xf32>) outs(%7 : tensor<256x14x14x128xf32>) -> tensor<256x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 288, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6798940934}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x17x23x128xf32>, tensor<192x1x7x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x17x23x128xf32>, tensor<192x1x7x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<128x17x23x128xf32>, %3: tensor<192x1x7x128xf32>, %7: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x17x23x128xf32>, tensor<192x1x7x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n  return %ret : tensor<128x17x17x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x17x23x128xf32>, %arg1: tensor<192x1x7x128xf32>, %arg2: tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x7x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x17x23x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x17x17x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x17x17x192xf32>\n    memref.copy %2, %alloc : memref<128x17x17x192xf32> to memref<128x17x17x192xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 17 {\n        affine.for %arg5 = 0 to 17 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x17x23x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x7x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x17x17x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x17x17x192xf32>\n    return %3 : tensor<128x17x17x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x17x17x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<128x17x23x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<128x17x23x128xf32>) -> tensor<128x17x23x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x7x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x7x128xf32>) -> tensor<192x1x7x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x17x17x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<128x17x23x128xf32>, tensor<192x1x7x128xf32>) outs(%7 : tensor<128x17x17x192xf32>) -> tensor<128x17x17x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x17x17x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x17x17x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 17, 1], ["%arg5", 0, 17, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23926241840}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<72x1x1x8xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<72x1x1x8xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<128x1x1x8xf32>, %3: tensor<72x1x1x8xf32>, %7: tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<72x1x1x8xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n  return %ret : tensor<128x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1x1x8xf32>, %arg1: tensor<72x1x1x8xf32>, %arg2: tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1x1x72xf32>\n    memref.copy %2, %alloc : memref<128x1x1x72xf32> to memref<128x1x1x72xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<128x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<128x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1x1x72xf32>\n    return %3 : tensor<128x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<128x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<128x1x1x8xf32>) -> tensor<128x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x8xf32>) -> tensor<72x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<128x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<128x1x1x8xf32>, tensor<72x1x1x8xf32>) outs(%7 : tensor<128x1x1x72xf32>) -> tensor<128x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 118566}]]