{
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x130x130xf32>) outs(%25 : tensor<128x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x130x130xf32>) outs(%25 : tensor<128x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x130x130xf32>, %25: tensor<128x192x130x130xf32>) -> tensor<128x192x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x130x130xf32>) outs(%25 : tensor<128x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x130x130xf32>\n  return %ret : tensor<128x192x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x130x130xf32>, %arg1: tensor<128x192x130x130xf32>) -> tensor<128x192x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x130x130xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x130x130xf32>\n    return %1 : tensor<128x192x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x130x130xf32>) -> tensor<128x192x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x130x130xf32>) -> tensor<128x192x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x130x130xf32>) outs(%25 : tensor<128x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 345882557
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x14x14xf32>) outs(%25 : tensor<128x384x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x14x14xf32>) outs(%25 : tensor<128x384x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x384x14x14xf32>, %25: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x14x14xf32>) outs(%25 : tensor<128x384x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x14x14xf32>\n  return %ret : tensor<128x384x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x384x14x14xf32>, %arg1: tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x384x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x384x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x384x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x384x14x14xf32>\n    return %1 : tensor<128x384x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x384x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x384x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x384x14x14xf32>) -> tensor<128x384x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x14x14xf32>) outs(%25 : tensor<128x384x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 7840215
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x14x14xf32>) outs(%25 : tensor<128x192x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x14x14xf32>) outs(%25 : tensor<128x192x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x14x14xf32>, %25: tensor<128x192x14x14xf32>) -> tensor<128x192x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x14x14xf32>) outs(%25 : tensor<128x192x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x14x14xf32>\n  return %ret : tensor<128x192x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x14x14xf32>, %arg1: tensor<128x192x14x14xf32>) -> tensor<128x192x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x14x14xf32>\n    return %1 : tensor<128x192x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x14x14xf32>) -> tensor<128x192x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x14x14xf32>) -> tensor<128x192x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x14x14xf32>) outs(%25 : tensor<128x192x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3675221
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x130x130xf32>) outs(%25 : tensor<128x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x130x130xf32>) outs(%25 : tensor<128x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x130x130xf32>, %25: tensor<128x256x130x130xf32>) -> tensor<128x256x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x130x130xf32>) outs(%25 : tensor<128x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x130x130xf32>\n  return %ret : tensor<128x256x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x130x130xf32>, %arg1: tensor<128x256x130x130xf32>) -> tensor<128x256x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x130x130xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x130x130xf32>\n    return %1 : tensor<128x256x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x130x130xf32>) -> tensor<128x256x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x130x130xf32>) -> tensor<128x256x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x130x130xf32>) outs(%25 : tensor<128x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 485539139
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x56x56xf32>) outs(%25 : tensor<128x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x56x56xf32>) outs(%25 : tensor<128x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x48x56x56xf32>, %25: tensor<128x48x56x56xf32>) -> tensor<128x48x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x56x56xf32>) outs(%25 : tensor<128x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x56x56xf32>\n  return %ret : tensor<128x48x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x48x56x56xf32>, %arg1: tensor<128x48x56x56xf32>) -> tensor<128x48x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x48x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x56x56xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x48x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x48x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x48x56x56xf32>\n    return %1 : tensor<128x48x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x48x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x48x56x56xf32>) -> tensor<128x48x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x48x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x48x56x56xf32>) -> tensor<128x48x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x56x56xf32>) outs(%25 : tensor<128x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 16730524
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1024xf32>) outs(%35 : tensor<128x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1024xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1024xf32>) outs(%35 : tensor<128x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1024xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x1024xf32>, %35: tensor<128x1024xf32>) -> tensor<128x1024xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1024xf32>) outs(%35 : tensor<128x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1024xf32>\n  return %ret : tensor<128x1024xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x1024xf32>, %arg1: tensor<128x1024xf32>) -> tensor<128x1024xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 1024 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x1024xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x1024xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x1024xf32>\n    return %1 : tensor<128x1024xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x1024xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x1024xf32>) -> tensor<128x1024xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x1024xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x1024xf32>) -> tensor<128x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1024xf32>) outs(%35 : tensor<128x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1024xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          1024,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 100476
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x14x14xf32>) outs(%25 : tensor<256x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x14x14xf32>) outs(%25 : tensor<256x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x512x14x14xf32>, %25: tensor<256x512x14x14xf32>) -> tensor<256x512x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x14x14xf32>) outs(%25 : tensor<256x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x14x14xf32>\n  return %ret : tensor<256x512x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x512x14x14xf32>, %arg1: tensor<256x512x14x14xf32>) -> tensor<256x512x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x512x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x512x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x512x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x512x14x14xf32>\n    return %1 : tensor<256x512x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x512x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x512x14x14xf32>) -> tensor<256x512x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x512x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x512x14x14xf32>) -> tensor<256x512x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x14x14xf32>) outs(%25 : tensor<256x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 21310216
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x3072xf32>) outs(%35 : tensor<256x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x3072xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x3072xf32>) outs(%35 : tensor<256x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x3072xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x3072xf32>, %35: tensor<256x3072xf32>) -> tensor<256x3072xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x3072xf32>) outs(%35 : tensor<256x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x3072xf32>\n  return %ret : tensor<256x3072xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x3072xf32>, %arg1: tensor<256x3072xf32>) -> tensor<256x3072xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x3072xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x3072xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 3072 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x3072xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x3072xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x3072xf32>\n    return %1 : tensor<256x3072xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x3072xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x3072xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x3072xf32>) -> tensor<256x3072xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x3072xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x3072xf32>) -> tensor<256x3072xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x3072xf32>) outs(%35 : tensor<256x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x3072xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x3072xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x3072xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          3072,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 602550
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x240x240xf32>) outs(%25 : tensor<128x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x240x240xf32>) outs(%25 : tensor<128x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x384x240x240xf32>, %25: tensor<128x384x240x240xf32>) -> tensor<128x384x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x240x240xf32>) outs(%25 : tensor<128x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x240x240xf32>\n  return %ret : tensor<128x384x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x384x240x240xf32>, %arg1: tensor<128x384x240x240xf32>) -> tensor<128x384x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x384x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x240x240xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x384x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x384x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x384x240x240xf32>\n    return %1 : tensor<128x384x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x384x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x384x240x240xf32>) -> tensor<128x384x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x384x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x384x240x240xf32>) -> tensor<128x384x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x240x240xf32>) outs(%25 : tensor<128x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2591579652
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x224x224xf32>) outs(%25 : tensor<128x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x224x224xf32>) outs(%25 : tensor<128x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x224x224xf32>, %25: tensor<128x128x224x224xf32>) -> tensor<128x128x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x224x224xf32>) outs(%25 : tensor<128x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x224x224xf32>\n  return %ret : tensor<128x128x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x224x224xf32>, %arg1: tensor<128x128x224x224xf32>) -> tensor<128x128x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x224x224xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x224x224xf32>\n    return %1 : tensor<128x128x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x224x224xf32>) -> tensor<128x128x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x224x224xf32>) -> tensor<128x128x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x224x224xf32>) outs(%25 : tensor<128x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 744803265
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x28x28xf32>) outs(%25 : tensor<256x240x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x28x28xf32>) outs(%25 : tensor<256x240x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x28x28xf32>, %25: tensor<256x240x28x28xf32>) -> tensor<256x240x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x28x28xf32>) outs(%25 : tensor<256x240x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x28x28xf32>\n  return %ret : tensor<256x240x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x28x28xf32>, %arg1: tensor<256x240x28x28xf32>) -> tensor<256x240x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x28x28xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x28x28xf32>\n    return %1 : tensor<256x240x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x28x28xf32>) -> tensor<256x240x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x28x28xf32>) -> tensor<256x240x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x28x28xf32>) outs(%25 : tensor<256x240x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 37527599
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x768xf32>) outs(%35 : tensor<128x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x768xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x768xf32>) outs(%35 : tensor<128x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x768xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x768xf32>, %35: tensor<128x768xf32>) -> tensor<128x768xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x768xf32>) outs(%35 : tensor<128x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x768xf32>\n  return %ret : tensor<128x768xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x768xf32>, %arg1: tensor<128x768xf32>) -> tensor<128x768xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x768xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 768 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x768xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x768xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x768xf32>\n    return %1 : tensor<128x768xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x768xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x768xf32>) -> tensor<128x768xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x768xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x768xf32>) -> tensor<128x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x768xf32>) outs(%35 : tensor<128x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x768xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          768,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 75732.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x120x120xf32>) outs(%25 : tensor<256x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x120x120xf32>) outs(%25 : tensor<256x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x192x120x120xf32>, %25: tensor<256x192x120x120xf32>) -> tensor<256x192x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x120x120xf32>) outs(%25 : tensor<256x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x120x120xf32>\n  return %ret : tensor<256x192x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x192x120x120xf32>, %arg1: tensor<256x192x120x120xf32>) -> tensor<256x192x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x192x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x120x120xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x192x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x192x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x192x120x120xf32>\n    return %1 : tensor<256x192x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x192x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x192x120x120xf32>) -> tensor<256x192x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x192x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x192x120x120xf32>) -> tensor<256x192x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x120x120xf32>) outs(%25 : tensor<256x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 582537039
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x120x120xf32>) outs(%25 : tensor<128x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x120x120xf32>) outs(%25 : tensor<128x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x120x120xf32>, %25: tensor<128x192x120x120xf32>) -> tensor<128x192x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x120x120xf32>) outs(%25 : tensor<128x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x120x120xf32>\n  return %ret : tensor<128x192x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x120x120xf32>, %arg1: tensor<128x192x120x120xf32>) -> tensor<128x192x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x120x120xf32>\n    return %1 : tensor<128x192x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x120x120xf32>) -> tensor<128x192x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x120x120xf32>) -> tensor<128x192x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x120x120xf32>) outs(%25 : tensor<128x192x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 293397103
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x3072xf32>) outs(%35 : tensor<128x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x3072xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x3072xf32>) outs(%35 : tensor<128x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x3072xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x3072xf32>, %35: tensor<128x3072xf32>) -> tensor<128x3072xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x3072xf32>) outs(%35 : tensor<128x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x3072xf32>\n  return %ret : tensor<128x3072xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x3072xf32>, %arg1: tensor<128x3072xf32>) -> tensor<128x3072xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x3072xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x3072xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 3072 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x3072xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x3072xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x3072xf32>\n    return %1 : tensor<128x3072xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x3072xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x3072xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x3072xf32>) -> tensor<128x3072xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x3072xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x3072xf32>) -> tensor<128x3072xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x3072xf32>) outs(%35 : tensor<128x3072xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x3072xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x3072xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x3072xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          3072,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 299133
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1536xf32>) outs(%35 : tensor<256x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1536xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1536xf32>) outs(%35 : tensor<256x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1536xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x1536xf32>, %35: tensor<256x1536xf32>) -> tensor<256x1536xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1536xf32>) outs(%35 : tensor<256x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1536xf32>\n  return %ret : tensor<256x1536xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x1536xf32>, %arg1: tensor<256x1536xf32>) -> tensor<256x1536xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x1536xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1536xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 1536 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x1536xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x1536xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x1536xf32>\n    return %1 : tensor<256x1536xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x1536xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x1536xf32>) -> tensor<256x1536xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x1536xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x1536xf32>) -> tensor<256x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1536xf32>) outs(%35 : tensor<256x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1536xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          1536,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 299736
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x224x224xf32>) outs(%25 : tensor<256x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x224x224xf32>) outs(%25 : tensor<256x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x224x224xf32>, %25: tensor<256x384x224x224xf32>) -> tensor<256x384x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x224x224xf32>) outs(%25 : tensor<256x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x224x224xf32>\n  return %ret : tensor<256x384x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x224x224xf32>, %arg1: tensor<256x384x224x224xf32>) -> tensor<256x384x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x224x224xf32>\n    return %1 : tensor<256x384x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x224x224xf32>) -> tensor<256x384x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x224x224xf32>) -> tensor<256x384x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x224x224xf32>) outs(%25 : tensor<256x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4513269683
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x112x112xf32>) outs(%25 : tensor<256x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x112x112xf32>) outs(%25 : tensor<256x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x64x112x112xf32>, %25: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x112x112xf32>) outs(%25 : tensor<256x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x112x112xf32>\n  return %ret : tensor<256x64x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x64x112x112xf32>, %arg1: tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x64x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x64x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x64x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x64x112x112xf32>\n    return %1 : tensor<256x64x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x64x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x64x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x64x112x112xf32>) -> tensor<256x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x112x112xf32>) outs(%25 : tensor<256x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 170164201
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x512xf32>) outs(%35 : tensor<128x512xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x512xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x512xf32>) outs(%35 : tensor<128x512xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x512xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x512xf32>, %35: tensor<128x512xf32>) -> tensor<128x512xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x512xf32>) outs(%35 : tensor<128x512xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x512xf32>\n  return %ret : tensor<128x512xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x512xf32>, %arg1: tensor<128x512xf32>) -> tensor<128x512xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 512 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x512xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x512xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x512xf32>\n    return %1 : tensor<128x512xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x512xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x512xf32>) -> tensor<128x512xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x512xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x512xf32>) -> tensor<128x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x512xf32>) outs(%35 : tensor<128x512xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 51028.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x224x224xf32>) outs(%25 : tensor<128x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x224x224xf32>) outs(%25 : tensor<128x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x224x224xf32>, %25: tensor<128x96x224x224xf32>) -> tensor<128x96x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x224x224xf32>) outs(%25 : tensor<128x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x224x224xf32>\n  return %ret : tensor<128x96x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x224x224xf32>, %arg1: tensor<128x96x224x224xf32>) -> tensor<128x96x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x224x224xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x224x224xf32>\n    return %1 : tensor<128x96x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x224x224xf32>) -> tensor<128x96x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x224x224xf32>) -> tensor<128x96x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x224x224xf32>) outs(%25 : tensor<128x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 536121876
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x768xf32>) outs(%35 : tensor<256x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x768xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x768xf32>) outs(%35 : tensor<256x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x768xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x768xf32>, %35: tensor<256x768xf32>) -> tensor<256x768xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x768xf32>) outs(%35 : tensor<256x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x768xf32>\n  return %ret : tensor<256x768xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x768xf32>, %arg1: tensor<256x768xf32>) -> tensor<256x768xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x768xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x768xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 768 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x768xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x768xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x768xf32>\n    return %1 : tensor<256x768xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x768xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x768xf32>) -> tensor<256x768xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x768xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x768xf32>) -> tensor<256x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x768xf32>) outs(%35 : tensor<256x768xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x768xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          768,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 150688
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x112x112xf32>) outs(%25 : tensor<128x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x112x112xf32>) outs(%25 : tensor<128x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x112x112xf32>, %25: tensor<128x96x112x112xf32>) -> tensor<128x96x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x112x112xf32>) outs(%25 : tensor<128x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x112x112xf32>\n  return %ret : tensor<128x96x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x112x112xf32>, %arg1: tensor<128x96x112x112xf32>) -> tensor<128x96x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x112x112xf32>\n    return %1 : tensor<128x96x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x112x112xf32>) -> tensor<128x96x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x112x112xf32>) -> tensor<128x96x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x112x112xf32>) outs(%25 : tensor<128x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 127778868
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x14x14xf32>) outs(%25 : tensor<256x240x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x14x14xf32>) outs(%25 : tensor<256x240x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x14x14xf32>, %25: tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x14x14xf32>) outs(%25 : tensor<256x240x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x14x14xf32>\n  return %ret : tensor<256x240x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x14x14xf32>, %arg1: tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x14x14xf32>\n    return %1 : tensor<256x240x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x14x14xf32>) -> tensor<256x240x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x14x14xf32>) outs(%25 : tensor<256x240x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 9901954
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x28x28xf32>) outs(%25 : tensor<128x96x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x28x28xf32>) outs(%25 : tensor<128x96x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x28x28xf32>, %25: tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x28x28xf32>) outs(%25 : tensor<128x96x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x28x28xf32>\n  return %ret : tensor<128x96x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x28x28xf32>, %arg1: tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x28x28xf32>\n    return %1 : tensor<128x96x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x28x28xf32>) -> tensor<128x96x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x28x28xf32>) outs(%25 : tensor<128x96x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 7330281
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1024xf32>) outs(%35 : tensor<256x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1024xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1024xf32>) outs(%35 : tensor<256x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1024xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x1024xf32>, %35: tensor<256x1024xf32>) -> tensor<256x1024xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1024xf32>) outs(%35 : tensor<256x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1024xf32>\n  return %ret : tensor<256x1024xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x1024xf32>, %arg1: tensor<256x1024xf32>) -> tensor<256x1024xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 1024 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x1024xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x1024xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x1024xf32>\n    return %1 : tensor<256x1024xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x1024xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x1024xf32>) -> tensor<256x1024xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x1024xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x1024xf32>) -> tensor<256x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x1024xf32>) outs(%35 : tensor<256x1024xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1024xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          1024,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 185323
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x128xf32>) outs(%35 : tensor<128x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x128xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x128xf32>) outs(%35 : tensor<128x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x128xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x128xf32>, %35: tensor<128x128xf32>) -> tensor<128x128xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x128xf32>) outs(%35 : tensor<128x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x128xf32>\n  return %ret : tensor<128x128xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128xf32>, %arg1: tensor<128x128xf32>) -> tensor<128x128xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x128xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x128xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128xf32>\n    return %1 : tensor<128x128xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x128xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x128xf32>) -> tensor<128x128xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x128xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x128xf32>) -> tensor<128x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x128xf32>) outs(%35 : tensor<128x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 12564
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x120x120xf32>) outs(%25 : tensor<128x96x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x120x120xf32>) outs(%25 : tensor<128x96x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x120x120xf32>, %25: tensor<128x96x120x120xf32>) -> tensor<128x96x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x120x120xf32>) outs(%25 : tensor<128x96x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x120x120xf32>\n  return %ret : tensor<128x96x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x120x120xf32>, %arg1: tensor<128x96x120x120xf32>) -> tensor<128x96x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x120x120xf32>\n    return %1 : tensor<128x96x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x120x120xf32>) -> tensor<128x96x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x120x120xf32>) -> tensor<128x96x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x120x120xf32>) outs(%25 : tensor<128x96x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 145907879
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x7x7xf32>) outs(%25 : tensor<256x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x7x7xf32>) outs(%25 : tensor<256x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x7x7xf32>, %25: tensor<256x96x7x7xf32>) -> tensor<256x96x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x7x7xf32>) outs(%25 : tensor<256x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x7x7xf32>\n  return %ret : tensor<256x96x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x7x7xf32>, %arg1: tensor<256x96x7x7xf32>) -> tensor<256x96x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x7x7xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x7x7xf32>\n    return %1 : tensor<256x96x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x7x7xf32>) -> tensor<256x96x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x7x7xf32>) -> tensor<256x96x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x7x7xf32>) outs(%25 : tensor<256x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 962626
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x130x130xf32>) outs(%25 : tensor<256x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x130x130xf32>) outs(%25 : tensor<256x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x256x130x130xf32>, %25: tensor<256x256x130x130xf32>) -> tensor<256x256x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x130x130xf32>) outs(%25 : tensor<256x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x130x130xf32>\n  return %ret : tensor<256x256x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256x130x130xf32>, %arg1: tensor<256x256x130x130xf32>) -> tensor<256x256x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x130x130xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x256x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x256x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256x130x130xf32>\n    return %1 : tensor<256x256x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x256x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x256x130x130xf32>) -> tensor<256x256x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x256x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x256x130x130xf32>) -> tensor<256x256x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x130x130xf32>) outs(%25 : tensor<256x256x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 965598149
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1536xf32>) outs(%35 : tensor<128x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1536xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1536xf32>) outs(%35 : tensor<128x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1536xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x1536xf32>, %35: tensor<128x1536xf32>) -> tensor<128x1536xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1536xf32>) outs(%35 : tensor<128x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1536xf32>\n  return %ret : tensor<128x1536xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x1536xf32>, %arg1: tensor<128x1536xf32>) -> tensor<128x1536xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x1536xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1536xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 1536 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x1536xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x1536xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x1536xf32>\n    return %1 : tensor<128x1536xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x1536xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x1536xf32>) -> tensor<128x1536xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x1536xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x1536xf32>) -> tensor<128x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x1536xf32>) outs(%35 : tensor<128x1536xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x1536xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          1536,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 150214.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x130x130xf32>) outs(%25 : tensor<256x384x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x130x130xf32>) outs(%25 : tensor<256x384x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x130x130xf32>, %25: tensor<256x384x130x130xf32>) -> tensor<256x384x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x130x130xf32>) outs(%25 : tensor<256x384x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x130x130xf32>\n  return %ret : tensor<256x384x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x130x130xf32>, %arg1: tensor<256x384x130x130xf32>) -> tensor<256x384x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x130x130xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x130x130xf32>\n    return %1 : tensor<256x384x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x130x130xf32>) -> tensor<256x384x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x130x130xf32>) -> tensor<256x384x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x130x130xf32>) outs(%25 : tensor<256x384x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1521373797
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x112x112xf32>) outs(%25 : tensor<128x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x112x112xf32>) outs(%25 : tensor<128x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x112x112xf32>, %25: tensor<128x32x112x112xf32>) -> tensor<128x32x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x112x112xf32>) outs(%25 : tensor<128x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x112x112xf32>\n  return %ret : tensor<128x32x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x112x112xf32>, %arg1: tensor<128x32x112x112xf32>) -> tensor<128x32x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x112x112xf32>\n    return %1 : tensor<128x32x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x112x112xf32>) -> tensor<128x32x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x112x112xf32>) -> tensor<128x32x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x112x112xf32>) outs(%25 : tensor<128x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 42604458
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x128xf32>) outs(%35 : tensor<256x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x128xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x128xf32>) outs(%35 : tensor<256x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x128xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x128xf32>, %35: tensor<256x128xf32>) -> tensor<256x128xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x128xf32>) outs(%35 : tensor<256x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x128xf32>\n  return %ret : tensor<256x128xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x128xf32>, %arg1: tensor<256x128xf32>) -> tensor<256x128xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 128 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x128xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x128xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x128xf32>\n    return %1 : tensor<256x128xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x128xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x128xf32>) -> tensor<256x128xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x128xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x128xf32>) -> tensor<256x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x128xf32>) outs(%35 : tensor<256x128xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 26563
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x224x224xf32>) outs(%25 : tensor<256x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x224x224xf32>) outs(%25 : tensor<256x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x224x224xf32>, %25: tensor<256x96x224x224xf32>) -> tensor<256x96x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x224x224xf32>) outs(%25 : tensor<256x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x224x224xf32>\n  return %ret : tensor<256x96x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x224x224xf32>, %arg1: tensor<256x96x224x224xf32>) -> tensor<256x96x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x224x224xf32>\n    return %1 : tensor<256x96x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x224x224xf32>) -> tensor<256x96x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x224x224xf32>) -> tensor<256x96x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x224x224xf32>) outs(%25 : tensor<256x96x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1068224496
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x56x56xf32>) outs(%25 : tensor<128x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x56x56xf32>) outs(%25 : tensor<128x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x56x56xf32>, %25: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x56x56xf32>) outs(%25 : tensor<128x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x56x56xf32>\n  return %ret : tensor<128x256x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x56x56xf32>, %arg1: tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x56x56xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x56x56xf32>\n    return %1 : tensor<128x256x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x56x56xf32>) -> tensor<128x256x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x56x56xf32>) outs(%25 : tensor<128x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 86849445
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x7x7xf32>) outs(%25 : tensor<128x512x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x7x7xf32>) outs(%25 : tensor<128x512x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x512x7x7xf32>, %25: tensor<128x512x7x7xf32>) -> tensor<128x512x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x7x7xf32>) outs(%25 : tensor<128x512x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x7x7xf32>\n  return %ret : tensor<128x512x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x512x7x7xf32>, %arg1: tensor<128x512x7x7xf32>) -> tensor<128x512x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x512x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x512x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x512x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x512x7x7xf32>\n    return %1 : tensor<128x512x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x512x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x512x7x7xf32>) -> tensor<128x512x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x512x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x512x7x7xf32>) -> tensor<128x512x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x7x7xf32>) outs(%25 : tensor<128x512x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2568027
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x15x15xf32>) outs(%25 : tensor<256x512x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x15x15xf32>) outs(%25 : tensor<256x512x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x512x15x15xf32>, %25: tensor<256x512x15x15xf32>) -> tensor<256x512x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x15x15xf32>) outs(%25 : tensor<256x512x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x15x15xf32>\n  return %ret : tensor<256x512x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x512x15x15xf32>, %arg1: tensor<256x512x15x15xf32>) -> tensor<256x512x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x512x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x15x15xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x512x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x512x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x512x15x15xf32>\n    return %1 : tensor<256x512x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x512x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x512x15x15xf32>) -> tensor<256x512x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x512x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x512x15x15xf32>) -> tensor<256x512x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x15x15xf32>) outs(%25 : tensor<256x512x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 24277748
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x130x130xf32>) outs(%25 : tensor<256x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x130x130xf32>) outs(%25 : tensor<256x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x192x130x130xf32>, %25: tensor<256x192x130x130xf32>) -> tensor<256x192x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x130x130xf32>) outs(%25 : tensor<256x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x130x130xf32>\n  return %ret : tensor<256x192x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x192x130x130xf32>, %arg1: tensor<256x192x130x130xf32>) -> tensor<256x192x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x192x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x130x130xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x192x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x192x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x192x130x130xf32>\n    return %1 : tensor<256x192x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x192x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x192x130x130xf32>) -> tensor<256x192x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x192x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x192x130x130xf32>) -> tensor<256x192x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x130x130xf32>) outs(%25 : tensor<256x192x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 687209972
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x256xf32>) outs(%35 : tensor<128x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x256xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x256xf32>) outs(%35 : tensor<128x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x256xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x256xf32>, %35: tensor<128x256xf32>) -> tensor<128x256xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x256xf32>) outs(%35 : tensor<128x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x256xf32>\n  return %ret : tensor<128x256xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256xf32>, %arg1: tensor<128x256xf32>) -> tensor<128x256xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x256xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x256xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256xf32>\n    return %1 : tensor<128x256xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x256xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x256xf32>) -> tensor<128x256xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x256xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x256xf32>) -> tensor<128x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x256xf32>) outs(%35 : tensor<128x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 26072
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x130x130xf32>) outs(%25 : tensor<256x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x130x130xf32>) outs(%25 : tensor<256x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x288x130x130xf32>, %25: tensor<256x288x130x130xf32>) -> tensor<256x288x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x130x130xf32>) outs(%25 : tensor<256x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x130x130xf32>\n  return %ret : tensor<256x288x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x288x130x130xf32>, %arg1: tensor<256x288x130x130xf32>) -> tensor<256x288x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x288x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x130x130xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x288x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x288x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x288x130x130xf32>\n    return %1 : tensor<256x288x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x288x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x288x130x130xf32>) -> tensor<256x288x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x288x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x288x130x130xf32>) -> tensor<256x288x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x130x130xf32>) outs(%25 : tensor<256x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1100057066
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x228x228xf32>) outs(%25 : tensor<256x192x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x228x228xf32>) outs(%25 : tensor<256x192x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x192x228x228xf32>, %25: tensor<256x192x228x228xf32>) -> tensor<256x192x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x228x228xf32>) outs(%25 : tensor<256x192x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x228x228xf32>\n  return %ret : tensor<256x192x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x192x228x228xf32>, %arg1: tensor<256x192x228x228xf32>) -> tensor<256x192x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x192x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x228x228xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x192x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x192x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x192x228x228xf32>\n    return %1 : tensor<256x192x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x192x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x192x228x228xf32>) -> tensor<256x192x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x192x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x192x228x228xf32>) -> tensor<256x192x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x228x228xf32>) outs(%25 : tensor<256x192x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2325240857
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x56x56xf32>) outs(%25 : tensor<128x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x56x56xf32>) outs(%25 : tensor<128x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x56x56xf32>, %25: tensor<128x192x56x56xf32>) -> tensor<128x192x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x56x56xf32>) outs(%25 : tensor<128x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x56x56xf32>\n  return %ret : tensor<128x192x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x56x56xf32>, %arg1: tensor<128x192x56x56xf32>) -> tensor<128x192x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x56x56xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x56x56xf32>\n    return %1 : tensor<128x192x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x56x56xf32>) -> tensor<128x192x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x56x56xf32>) -> tensor<128x192x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x56x56xf32>) outs(%25 : tensor<128x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 66436716
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x240x240xf32>) outs(%25 : tensor<128x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x240x240xf32>) outs(%25 : tensor<128x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x288x240x240xf32>, %25: tensor<128x288x240x240xf32>) -> tensor<128x288x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x240x240xf32>) outs(%25 : tensor<128x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x240x240xf32>\n  return %ret : tensor<128x288x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x288x240x240xf32>, %arg1: tensor<128x288x240x240xf32>) -> tensor<128x288x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x288x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x240x240xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x288x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x288x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x288x240x240xf32>\n    return %1 : tensor<128x288x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x288x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x288x240x240xf32>) -> tensor<128x288x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x288x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x288x240x240xf32>) -> tensor<128x288x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x240x240xf32>) outs(%25 : tensor<128x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1932699906
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x7x7xf32>) outs(%25 : tensor<128x64x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x7x7xf32>) outs(%25 : tensor<128x64x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x64x7x7xf32>, %25: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x7x7xf32>) outs(%25 : tensor<128x64x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x7x7xf32>\n  return %ret : tensor<128x64x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x64x7x7xf32>, %arg1: tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x64x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x64x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x64x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x64x7x7xf32>\n    return %1 : tensor<128x64x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x64x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x64x7x7xf32>) -> tensor<128x64x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x7x7xf32>) outs(%25 : tensor<128x64x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 320030
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x14x14xf32>) outs(%25 : tensor<128x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x14x14xf32>) outs(%25 : tensor<128x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x14x14xf32>, %25: tensor<128x256x14x14xf32>) -> tensor<128x256x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x14x14xf32>) outs(%25 : tensor<128x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x14x14xf32>\n  return %ret : tensor<128x256x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x14x14xf32>, %arg1: tensor<128x256x14x14xf32>) -> tensor<128x256x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x14x14xf32>\n    return %1 : tensor<128x256x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x14x14xf32>) -> tensor<128x256x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x14x14xf32>) -> tensor<128x256x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x14x14xf32>) outs(%25 : tensor<128x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5123443
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x150x150xf32>) outs(%25 : tensor<128x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x150x150xf32>) outs(%25 : tensor<128x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x240x150x150xf32>, %25: tensor<128x240x150x150xf32>) -> tensor<128x240x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x150x150xf32>) outs(%25 : tensor<128x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x150x150xf32>\n  return %ret : tensor<128x240x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x240x150x150xf32>, %arg1: tensor<128x240x150x150xf32>) -> tensor<128x240x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x240x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x150x150xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x240x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x240x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x240x150x150xf32>\n    return %1 : tensor<128x240x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x240x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x240x150x150xf32>) -> tensor<128x240x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x240x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x240x150x150xf32>) -> tensor<128x240x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x150x150xf32>) outs(%25 : tensor<128x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 616710357
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x224x224xf32>) outs(%25 : tensor<128x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x224x224xf32>) outs(%25 : tensor<128x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x224x224xf32>, %25: tensor<128x32x224x224xf32>) -> tensor<128x32x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x224x224xf32>) outs(%25 : tensor<128x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x224x224xf32>\n  return %ret : tensor<128x32x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x224x224xf32>, %arg1: tensor<128x32x224x224xf32>) -> tensor<128x32x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x224x224xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x224x224xf32>\n    return %1 : tensor<128x32x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x224x224xf32>) -> tensor<128x32x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x224x224xf32>) -> tensor<128x32x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x224x224xf32>) outs(%25 : tensor<128x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 164012581
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x28x28xf32>) outs(%25 : tensor<128x512x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x28x28xf32>) outs(%25 : tensor<128x512x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x512x28x28xf32>, %25: tensor<128x512x28x28xf32>) -> tensor<128x512x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x28x28xf32>) outs(%25 : tensor<128x512x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x28x28xf32>\n  return %ret : tensor<128x512x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x512x28x28xf32>, %arg1: tensor<128x512x28x28xf32>) -> tensor<128x512x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x512x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x512x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x512x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x512x28x28xf32>\n    return %1 : tensor<128x512x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x512x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x512x28x28xf32>) -> tensor<128x512x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x512x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x512x28x28xf32>) -> tensor<128x512x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x28x28xf32>) outs(%25 : tensor<128x512x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 40506088
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x224x224xf32>) outs(%25 : tensor<128x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x224x224xf32>) outs(%25 : tensor<128x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x224x224xf32>, %25: tensor<128x256x224x224xf32>) -> tensor<128x256x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x224x224xf32>) outs(%25 : tensor<128x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x224x224xf32>\n  return %ret : tensor<128x256x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x224x224xf32>, %arg1: tensor<128x256x224x224xf32>) -> tensor<128x256x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x224x224xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x224x224xf32>\n    return %1 : tensor<128x256x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x224x224xf32>) -> tensor<128x256x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x224x224xf32>) -> tensor<128x256x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x224x224xf32>) outs(%25 : tensor<128x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1501003811
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x150x150xf32>) outs(%25 : tensor<128x48x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x150x150xf32>) outs(%25 : tensor<128x48x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x48x150x150xf32>, %25: tensor<128x48x150x150xf32>) -> tensor<128x48x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x150x150xf32>) outs(%25 : tensor<128x48x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x150x150xf32>\n  return %ret : tensor<128x48x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x48x150x150xf32>, %arg1: tensor<128x48x150x150xf32>) -> tensor<128x48x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x48x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x150x150xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x48x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x48x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x48x150x150xf32>\n    return %1 : tensor<128x48x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x48x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x48x150x150xf32>) -> tensor<128x48x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x48x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x48x150x150xf32>) -> tensor<128x48x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x150x150xf32>) outs(%25 : tensor<128x48x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 113142340
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x7x7xf32>) outs(%25 : tensor<128x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x7x7xf32>) outs(%25 : tensor<128x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x384x7x7xf32>, %25: tensor<128x384x7x7xf32>) -> tensor<128x384x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x7x7xf32>) outs(%25 : tensor<128x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x7x7xf32>\n  return %ret : tensor<128x384x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x384x7x7xf32>, %arg1: tensor<128x384x7x7xf32>) -> tensor<128x384x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x384x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x384x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x384x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x384x7x7xf32>\n    return %1 : tensor<128x384x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x384x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x384x7x7xf32>) -> tensor<128x384x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x384x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x384x7x7xf32>) -> tensor<128x384x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x7x7xf32>) outs(%25 : tensor<128x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1921986
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x112x112xf32>) outs(%25 : tensor<256x240x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x112x112xf32>) outs(%25 : tensor<256x240x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x112x112xf32>, %25: tensor<256x240x112x112xf32>) -> tensor<256x240x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x112x112xf32>) outs(%25 : tensor<256x240x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x112x112xf32>\n  return %ret : tensor<256x240x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x112x112xf32>, %arg1: tensor<256x240x112x112xf32>) -> tensor<256x240x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x112x112xf32>\n    return %1 : tensor<256x240x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x112x112xf32>) -> tensor<256x240x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x112x112xf32>) -> tensor<256x240x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x112x112xf32>) outs(%25 : tensor<256x240x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 635330672
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x14x14xf32>) outs(%25 : tensor<256x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x14x14xf32>) outs(%25 : tensor<256x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x14x14xf32>, %25: tensor<256x32x14x14xf32>) -> tensor<256x32x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x14x14xf32>) outs(%25 : tensor<256x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x14x14xf32>\n  return %ret : tensor<256x32x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x14x14xf32>, %arg1: tensor<256x32x14x14xf32>) -> tensor<256x32x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x14x14xf32>\n    return %1 : tensor<256x32x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x14x14xf32>) -> tensor<256x32x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x14x14xf32>) -> tensor<256x32x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x14x14xf32>) outs(%25 : tensor<256x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1214370
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x56x56xf32>) outs(%25 : tensor<256x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x56x56xf32>) outs(%25 : tensor<256x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x192x56x56xf32>, %25: tensor<256x192x56x56xf32>) -> tensor<256x192x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x56x56xf32>) outs(%25 : tensor<256x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x56x56xf32>\n  return %ret : tensor<256x192x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x192x56x56xf32>, %arg1: tensor<256x192x56x56xf32>) -> tensor<256x192x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x192x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x192x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x192x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x192x56x56xf32>\n    return %1 : tensor<256x192x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x192x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x192x56x56xf32>) -> tensor<256x192x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x192x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x192x56x56xf32>) -> tensor<256x192x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x56x56xf32>) outs(%25 : tensor<256x192x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 132410627
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x120x120xf32>) outs(%25 : tensor<128x32x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x120x120xf32>) outs(%25 : tensor<128x32x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x120x120xf32>, %25: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x120x120xf32>) outs(%25 : tensor<128x32x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x120x120xf32>\n  return %ret : tensor<128x32x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x120x120xf32>, %arg1: tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x120x120xf32>\n    return %1 : tensor<128x32x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x120x120xf32>) -> tensor<128x32x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x120x120xf32>) outs(%25 : tensor<128x32x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 48879650
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x240x240xf32>) outs(%25 : tensor<256x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x240x240xf32>) outs(%25 : tensor<256x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x288x240x240xf32>, %25: tensor<256x288x240x240xf32>) -> tensor<256x288x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x240x240xf32>) outs(%25 : tensor<256x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x240x240xf32>\n  return %ret : tensor<256x288x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x288x240x240xf32>, %arg1: tensor<256x288x240x240xf32>) -> tensor<256x288x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x288x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x288x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x288x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x288x240x240xf32>\n    return %1 : tensor<256x288x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x288x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x288x240x240xf32>) -> tensor<256x288x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x288x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x288x240x240xf32>) -> tensor<256x288x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x240x240xf32>) outs(%25 : tensor<256x288x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3882374854
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x150x150xf32>) outs(%25 : tensor<128x96x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x150x150xf32>) outs(%25 : tensor<128x96x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x150x150xf32>, %25: tensor<128x96x150x150xf32>) -> tensor<128x96x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x150x150xf32>) outs(%25 : tensor<128x96x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x150x150xf32>\n  return %ret : tensor<128x96x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x150x150xf32>, %arg1: tensor<128x96x150x150xf32>) -> tensor<128x96x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x150x150xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x150x150xf32>\n    return %1 : tensor<128x96x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x150x150xf32>) -> tensor<128x96x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x150x150xf32>) -> tensor<128x96x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x150x150xf32>) outs(%25 : tensor<128x96x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 226001670
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x240x240xf32>) outs(%25 : tensor<256x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x240x240xf32>) outs(%25 : tensor<256x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x256x240x240xf32>, %25: tensor<256x256x240x240xf32>) -> tensor<256x256x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x240x240xf32>) outs(%25 : tensor<256x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x240x240xf32>\n  return %ret : tensor<256x256x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256x240x240xf32>, %arg1: tensor<256x256x240x240xf32>) -> tensor<256x256x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x256x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x256x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256x240x240xf32>\n    return %1 : tensor<256x256x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x256x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x256x240x240xf32>) -> tensor<256x256x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x256x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x256x240x240xf32>) -> tensor<256x256x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x240x240xf32>) outs(%25 : tensor<256x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3447506114
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x28x28xf32>) outs(%25 : tensor<256x384x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x28x28xf32>) outs(%25 : tensor<256x384x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x28x28xf32>, %25: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x28x28xf32>) outs(%25 : tensor<256x384x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x28x28xf32>\n  return %ret : tensor<256x384x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x28x28xf32>, %arg1: tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x28x28xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x28x28xf32>\n    return %1 : tensor<256x384x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x28x28xf32>) -> tensor<256x384x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x28x28xf32>) outs(%25 : tensor<256x384x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 60541331
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x112x112xf32>) outs(%25 : tensor<128x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x112x112xf32>) outs(%25 : tensor<128x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x512x112x112xf32>, %25: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x112x112xf32>) outs(%25 : tensor<128x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x112x112xf32>\n  return %ret : tensor<128x512x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x512x112x112xf32>, %arg1: tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x512x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x512x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x512x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x512x112x112xf32>\n    return %1 : tensor<128x512x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x512x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x512x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x512x112x112xf32>) -> tensor<128x512x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x112x112xf32>) outs(%25 : tensor<128x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 762031439
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x15x15xf32>) outs(%25 : tensor<128x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x15x15xf32>) outs(%25 : tensor<128x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x384x15x15xf32>, %25: tensor<128x384x15x15xf32>) -> tensor<128x384x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x15x15xf32>) outs(%25 : tensor<128x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x15x15xf32>\n  return %ret : tensor<128x384x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x384x15x15xf32>, %arg1: tensor<128x384x15x15xf32>) -> tensor<128x384x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x384x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x384x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x384x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x384x15x15xf32>\n    return %1 : tensor<128x384x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x384x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x384x15x15xf32>) -> tensor<128x384x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x384x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x384x15x15xf32>) -> tensor<128x384x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x15x15xf32>) outs(%25 : tensor<128x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 9151274
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x228x228xf32>) outs(%25 : tensor<128x32x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x228x228xf32>) outs(%25 : tensor<128x32x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x228x228xf32>, %25: tensor<128x32x228x228xf32>) -> tensor<128x32x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x228x228xf32>) outs(%25 : tensor<128x32x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x228x228xf32>\n  return %ret : tensor<128x32x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x228x228xf32>, %arg1: tensor<128x32x228x228xf32>) -> tensor<128x32x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x228x228xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x228x228xf32>\n    return %1 : tensor<128x32x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x228x228xf32>) -> tensor<128x32x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x228x228xf32>) -> tensor<128x32x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x228x228xf32>) outs(%25 : tensor<128x32x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 169827227
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x112x112xf32>) outs(%25 : tensor<256x384x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x112x112xf32>) outs(%25 : tensor<256x384x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x112x112xf32>, %25: tensor<256x384x112x112xf32>) -> tensor<256x384x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x112x112xf32>) outs(%25 : tensor<256x384x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x112x112xf32>\n  return %ret : tensor<256x384x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x112x112xf32>, %arg1: tensor<256x384x112x112xf32>) -> tensor<256x384x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x112x112xf32>\n    return %1 : tensor<256x384x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x112x112xf32>) -> tensor<256x384x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x112x112xf32>) -> tensor<256x384x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x112x112xf32>) outs(%25 : tensor<256x384x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1121010997
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x15x15xf32>) outs(%25 : tensor<128x48x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x15x15xf32>) outs(%25 : tensor<128x48x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x48x15x15xf32>, %25: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x15x15xf32>) outs(%25 : tensor<128x48x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x15x15xf32>\n  return %ret : tensor<128x48x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x48x15x15xf32>, %arg1: tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x48x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x48x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x48x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x48x15x15xf32>\n    return %1 : tensor<128x48x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x48x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x48x15x15xf32>) -> tensor<128x48x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x15x15xf32>) outs(%25 : tensor<128x48x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1047841
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x112x112xf32>) outs(%25 : tensor<128x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x112x112xf32>) outs(%25 : tensor<128x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x64x112x112xf32>, %25: tensor<128x64x112x112xf32>) -> tensor<128x64x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x112x112xf32>) outs(%25 : tensor<128x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x112x112xf32>\n  return %ret : tensor<128x64x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x64x112x112xf32>, %arg1: tensor<128x64x112x112xf32>) -> tensor<128x64x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x64x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x64x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x64x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x64x112x112xf32>\n    return %1 : tensor<128x64x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x64x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x64x112x112xf32>) -> tensor<128x64x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x64x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x64x112x112xf32>) -> tensor<128x64x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x112x112xf32>) outs(%25 : tensor<128x64x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 84653524
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x150x150xf32>) outs(%25 : tensor<256x512x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x150x150xf32>) outs(%25 : tensor<256x512x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x512x150x150xf32>, %25: tensor<256x512x150x150xf32>) -> tensor<256x512x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x150x150xf32>) outs(%25 : tensor<256x512x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x150x150xf32>\n  return %ret : tensor<256x512x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x512x150x150xf32>, %arg1: tensor<256x512x150x150xf32>) -> tensor<256x512x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x512x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x150x150xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x512x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x512x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x512x150x150xf32>\n    return %1 : tensor<256x512x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x512x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x512x150x150xf32>) -> tensor<256x512x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x512x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x512x150x150xf32>) -> tensor<256x512x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x150x150xf32>) outs(%25 : tensor<256x512x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2728905844
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x224x224xf32>) outs(%25 : tensor<128x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x224x224xf32>) outs(%25 : tensor<128x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x384x224x224xf32>, %25: tensor<128x384x224x224xf32>) -> tensor<128x384x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x224x224xf32>) outs(%25 : tensor<128x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x224x224xf32>\n  return %ret : tensor<128x384x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x384x224x224xf32>, %arg1: tensor<128x384x224x224xf32>) -> tensor<128x384x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x384x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x384x224x224xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x384x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x384x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x384x224x224xf32>\n    return %1 : tensor<128x384x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x384x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x384x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x384x224x224xf32>) -> tensor<128x384x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x384x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x384x224x224xf32>) -> tensor<128x384x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x384x224x224xf32>) outs(%25 : tensor<128x384x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x384x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x384x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x384x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2250439989
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x14x14xf32>) outs(%25 : tensor<128x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x14x14xf32>) outs(%25 : tensor<128x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x512x14x14xf32>, %25: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x14x14xf32>) outs(%25 : tensor<128x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x14x14xf32>\n  return %ret : tensor<128x512x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x512x14x14xf32>, %arg1: tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x512x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x512x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x512x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x512x14x14xf32>\n    return %1 : tensor<128x512x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x512x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x512x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x512x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x512x14x14xf32>) -> tensor<128x512x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x512x14x14xf32>) outs(%25 : tensor<128x512x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x512x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x512x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x512x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 10638798
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x15x15xf32>) outs(%25 : tensor<256x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x15x15xf32>) outs(%25 : tensor<256x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x64x15x15xf32>, %25: tensor<256x64x15x15xf32>) -> tensor<256x64x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x15x15xf32>) outs(%25 : tensor<256x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x15x15xf32>\n  return %ret : tensor<256x64x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x64x15x15xf32>, %arg1: tensor<256x64x15x15xf32>) -> tensor<256x64x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x64x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x15x15xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x64x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x64x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x64x15x15xf32>\n    return %1 : tensor<256x64x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x64x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x64x15x15xf32>) -> tensor<256x64x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x64x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x64x15x15xf32>) -> tensor<256x64x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x15x15xf32>) outs(%25 : tensor<256x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2803846
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x28x28xf32>) outs(%25 : tensor<128x64x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x28x28xf32>) outs(%25 : tensor<128x64x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x64x28x28xf32>, %25: tensor<128x64x28x28xf32>) -> tensor<128x64x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x28x28xf32>) outs(%25 : tensor<128x64x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x28x28xf32>\n  return %ret : tensor<128x64x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x64x28x28xf32>, %arg1: tensor<128x64x28x28xf32>) -> tensor<128x64x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x64x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x64x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x64x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x64x28x28xf32>\n    return %1 : tensor<128x64x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x64x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x64x28x28xf32>) -> tensor<128x64x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x64x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x64x28x28xf32>) -> tensor<128x64x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x28x28xf32>) outs(%25 : tensor<128x64x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5005262
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x256xf32>) outs(%35 : tensor<256x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x256xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x256xf32>) outs(%35 : tensor<256x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x256xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x256xf32>, %35: tensor<256x256xf32>) -> tensor<256x256xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x256xf32>) outs(%35 : tensor<256x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x256xf32>\n  return %ret : tensor<256x256xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256xf32>, %arg1: tensor<256x256xf32>) -> tensor<256x256xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x256xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x256xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256xf32>\n    return %1 : tensor<256x256xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x256xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x256xf32>) -> tensor<256x256xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x256xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x256xf32>) -> tensor<256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x256xf32>) outs(%35 : tensor<256x256xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 51589.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x112x112xf32>) outs(%25 : tensor<256x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x112x112xf32>) outs(%25 : tensor<256x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x112x112xf32>, %25: tensor<256x96x112x112xf32>) -> tensor<256x96x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x112x112xf32>) outs(%25 : tensor<256x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x112x112xf32>\n  return %ret : tensor<256x96x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x112x112xf32>, %arg1: tensor<256x96x112x112xf32>) -> tensor<256x96x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x112x112xf32>\n    return %1 : tensor<256x96x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x112x112xf32>) -> tensor<256x96x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x112x112xf32>) -> tensor<256x96x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x112x112xf32>) outs(%25 : tensor<256x96x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 255614830
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x112x112xf32>) outs(%25 : tensor<128x192x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x112x112xf32>) outs(%25 : tensor<128x192x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x112x112xf32>, %25: tensor<128x192x112x112xf32>) -> tensor<128x192x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x112x112xf32>) outs(%25 : tensor<128x192x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x112x112xf32>\n  return %ret : tensor<128x192x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x112x112xf32>, %arg1: tensor<128x192x112x112xf32>) -> tensor<128x192x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x112x112xf32>\n    return %1 : tensor<128x192x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x112x112xf32>) -> tensor<128x192x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x112x112xf32>) -> tensor<128x192x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x112x112xf32>) outs(%25 : tensor<128x192x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 254372981
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x14x14xf32>) outs(%25 : tensor<256x64x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x14x14xf32>) outs(%25 : tensor<256x64x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x64x14x14xf32>, %25: tensor<256x64x14x14xf32>) -> tensor<256x64x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x14x14xf32>) outs(%25 : tensor<256x64x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x14x14xf32>\n  return %ret : tensor<256x64x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x64x14x14xf32>, %arg1: tensor<256x64x14x14xf32>) -> tensor<256x64x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x64x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x64x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x64x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x64x14x14xf32>\n    return %1 : tensor<256x64x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x64x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x64x14x14xf32>) -> tensor<256x64x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x64x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x64x14x14xf32>) -> tensor<256x64x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x14x14xf32>) outs(%25 : tensor<256x64x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2441446
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x28x28xf32>) outs(%25 : tensor<128x192x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x28x28xf32>) outs(%25 : tensor<128x192x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x28x28xf32>, %25: tensor<128x192x28x28xf32>) -> tensor<128x192x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x28x28xf32>) outs(%25 : tensor<128x192x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x28x28xf32>\n  return %ret : tensor<128x192x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x28x28xf32>, %arg1: tensor<128x192x28x28xf32>) -> tensor<128x192x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x28x28xf32>\n    return %1 : tensor<128x192x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x28x28xf32>) -> tensor<128x192x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x28x28xf32>) -> tensor<128x192x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x28x28xf32>) outs(%25 : tensor<128x192x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 15145287
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x150x150xf32>) outs(%25 : tensor<256x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x150x150xf32>) outs(%25 : tensor<256x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x150x150xf32>, %25: tensor<256x240x150x150xf32>) -> tensor<256x240x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x150x150xf32>) outs(%25 : tensor<256x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x150x150xf32>\n  return %ret : tensor<256x240x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x150x150xf32>, %arg1: tensor<256x240x150x150xf32>) -> tensor<256x240x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x150x150xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x150x150xf32>\n    return %1 : tensor<256x240x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x150x150xf32>) -> tensor<256x240x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x150x150xf32>) -> tensor<256x240x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x150x150xf32>) outs(%25 : tensor<256x240x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1235398459
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x7x7xf32>) outs(%25 : tensor<256x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x7x7xf32>) outs(%25 : tensor<256x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x7x7xf32>, %25: tensor<256x384x7x7xf32>) -> tensor<256x384x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x7x7xf32>) outs(%25 : tensor<256x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x7x7xf32>\n  return %ret : tensor<256x384x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x7x7xf32>, %arg1: tensor<256x384x7x7xf32>) -> tensor<256x384x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x7x7xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x7x7xf32>\n    return %1 : tensor<256x384x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x7x7xf32>) -> tensor<256x384x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x7x7xf32>) -> tensor<256x384x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x7x7xf32>) outs(%25 : tensor<256x384x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3847032
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x15x15xf32>) outs(%25 : tensor<128x240x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x15x15xf32>) outs(%25 : tensor<128x240x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x240x15x15xf32>, %25: tensor<128x240x15x15xf32>) -> tensor<128x240x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x15x15xf32>) outs(%25 : tensor<128x240x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x15x15xf32>\n  return %ret : tensor<128x240x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x240x15x15xf32>, %arg1: tensor<128x240x15x15xf32>) -> tensor<128x240x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x240x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x240x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x240x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x240x15x15xf32>\n    return %1 : tensor<128x240x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x240x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x240x15x15xf32>) -> tensor<128x240x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x240x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x240x15x15xf32>) -> tensor<128x240x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x15x15xf32>) outs(%25 : tensor<128x240x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5244748
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x240x240xf32>) outs(%25 : tensor<256x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x240x240xf32>) outs(%25 : tensor<256x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x240x240xf32>, %25: tensor<256x32x240x240xf32>) -> tensor<256x32x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x240x240xf32>) outs(%25 : tensor<256x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x240x240xf32>\n  return %ret : tensor<256x32x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x240x240xf32>, %arg1: tensor<256x32x240x240xf32>) -> tensor<256x32x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x240x240xf32>\n    return %1 : tensor<256x32x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x240x240xf32>) -> tensor<256x32x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x240x240xf32>) -> tensor<256x32x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x240x240xf32>) outs(%25 : tensor<256x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 376465645
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x56x56xf32>) outs(%25 : tensor<256x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x56x56xf32>) outs(%25 : tensor<256x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x256x56x56xf32>, %25: tensor<256x256x56x56xf32>) -> tensor<256x256x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x56x56xf32>) outs(%25 : tensor<256x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x56x56xf32>\n  return %ret : tensor<256x256x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256x56x56xf32>, %arg1: tensor<256x256x56x56xf32>) -> tensor<256x256x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x256x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x256x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256x56x56xf32>\n    return %1 : tensor<256x256x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x256x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x256x56x56xf32>) -> tensor<256x256x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x256x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x256x56x56xf32>) -> tensor<256x256x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x56x56xf32>) outs(%25 : tensor<256x256x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 175056997
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x120x120xf32>) outs(%25 : tensor<128x240x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x120x120xf32>) outs(%25 : tensor<128x240x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x240x120x120xf32>, %25: tensor<128x240x120x120xf32>) -> tensor<128x240x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x120x120xf32>) outs(%25 : tensor<128x240x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x120x120xf32>\n  return %ret : tensor<128x240x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x240x120x120xf32>, %arg1: tensor<128x240x120x120xf32>) -> tensor<128x240x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x240x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x240x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x240x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x240x120x120xf32>\n    return %1 : tensor<128x240x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x240x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x240x120x120xf32>) -> tensor<128x240x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x240x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x240x120x120xf32>) -> tensor<128x240x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x120x120xf32>) outs(%25 : tensor<128x240x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 371252290
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x56x56xf32>) outs(%25 : tensor<256x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x56x56xf32>) outs(%25 : tensor<256x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x48x56x56xf32>, %25: tensor<256x48x56x56xf32>) -> tensor<256x48x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x56x56xf32>) outs(%25 : tensor<256x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x56x56xf32>\n  return %ret : tensor<256x48x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x48x56x56xf32>, %arg1: tensor<256x48x56x56xf32>) -> tensor<256x48x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x48x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x48x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x48x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x48x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x48x56x56xf32>\n    return %1 : tensor<256x48x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x48x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x48x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x48x56x56xf32>) -> tensor<256x48x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x48x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x48x56x56xf32>) -> tensor<256x48x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x56x56xf32>) outs(%25 : tensor<256x48x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x48x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x48x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 33298880
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x14x14xf32>) outs(%25 : tensor<256x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x14x14xf32>) outs(%25 : tensor<256x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x256x14x14xf32>, %25: tensor<256x256x14x14xf32>) -> tensor<256x256x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x14x14xf32>) outs(%25 : tensor<256x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x14x14xf32>\n  return %ret : tensor<256x256x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256x14x14xf32>, %arg1: tensor<256x256x14x14xf32>) -> tensor<256x256x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x256x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x256x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256x14x14xf32>\n    return %1 : tensor<256x256x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x256x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x256x14x14xf32>) -> tensor<256x256x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x256x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x256x14x14xf32>) -> tensor<256x256x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x14x14xf32>) outs(%25 : tensor<256x256x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 10668267
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x240x240xf32>) outs(%25 : tensor<256x240x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x240x240xf32>) outs(%25 : tensor<256x240x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x240x240xf32>, %25: tensor<256x240x240x240xf32>) -> tensor<256x240x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x240x240xf32>) outs(%25 : tensor<256x240x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x240x240xf32>\n  return %ret : tensor<256x240x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x240x240xf32>, %arg1: tensor<256x240x240x240xf32>) -> tensor<256x240x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x240x240xf32>\n    return %1 : tensor<256x240x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x240x240xf32>) -> tensor<256x240x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x240x240xf32>) -> tensor<256x240x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x240x240xf32>) outs(%25 : tensor<256x240x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3239669423
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x28x28xf32>) outs(%25 : tensor<256x128x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x28x28xf32>) outs(%25 : tensor<256x128x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x128x28x28xf32>, %25: tensor<256x128x28x28xf32>) -> tensor<256x128x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x28x28xf32>) outs(%25 : tensor<256x128x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x28x28xf32>\n  return %ret : tensor<256x128x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x128x28x28xf32>, %arg1: tensor<256x128x28x28xf32>) -> tensor<256x128x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x128x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x28x28xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x128x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x128x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x128x28x28xf32>\n    return %1 : tensor<256x128x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x128x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x128x28x28xf32>) -> tensor<256x128x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x128x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x128x28x28xf32>) -> tensor<256x128x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x28x28xf32>) outs(%25 : tensor<256x128x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 20169996
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x7x7xf32>) outs(%25 : tensor<128x192x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x7x7xf32>) outs(%25 : tensor<128x192x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x7x7xf32>, %25: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x7x7xf32>) outs(%25 : tensor<128x192x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x7x7xf32>\n  return %ret : tensor<128x192x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x7x7xf32>, %arg1: tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x7x7xf32>\n    return %1 : tensor<128x192x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x7x7xf32>) -> tensor<128x192x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x7x7xf32>) outs(%25 : tensor<128x192x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 961926.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x56x56xf32>) outs(%25 : tensor<256x96x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x56x56xf32>) outs(%25 : tensor<256x96x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x56x56xf32>, %25: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x56x56xf32>) outs(%25 : tensor<256x96x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x56x56xf32>\n  return %ret : tensor<256x96x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x56x56xf32>, %arg1: tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x56x56xf32>\n    return %1 : tensor<256x96x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x56x56xf32>) -> tensor<256x96x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x56x56xf32>) outs(%25 : tensor<256x96x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 66788779
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x130x130xf32>) outs(%25 : tensor<128x96x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x130x130xf32>) outs(%25 : tensor<128x96x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x130x130xf32>, %25: tensor<128x96x130x130xf32>) -> tensor<128x96x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x130x130xf32>) outs(%25 : tensor<128x96x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x130x130xf32>\n  return %ret : tensor<128x96x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x130x130xf32>, %arg1: tensor<128x96x130x130xf32>) -> tensor<128x96x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x130x130xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x130x130xf32>\n    return %1 : tensor<128x96x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x130x130xf32>) -> tensor<128x96x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x130x130xf32>) -> tensor<128x96x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x130x130xf32>) outs(%25 : tensor<128x96x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 171716926
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x228x228xf32>) outs(%25 : tensor<128x240x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x228x228xf32>) outs(%25 : tensor<128x240x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x240x228x228xf32>, %25: tensor<128x240x228x228xf32>) -> tensor<128x240x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x228x228xf32>) outs(%25 : tensor<128x240x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x228x228xf32>\n  return %ret : tensor<128x240x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x240x228x228xf32>, %arg1: tensor<128x240x228x228xf32>) -> tensor<128x240x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x240x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x240x228x228xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x240x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x240x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x240x228x228xf32>\n    return %1 : tensor<128x240x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x240x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x240x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x240x228x228xf32>) -> tensor<128x240x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x240x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x240x228x228xf32>) -> tensor<128x240x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x240x228x228xf32>) outs(%25 : tensor<128x240x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x240x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x240x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x240x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1439067163
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x7x7xf32>) outs(%25 : tensor<128x128x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x7x7xf32>) outs(%25 : tensor<128x128x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x7x7xf32>, %25: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x7x7xf32>) outs(%25 : tensor<128x128x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x7x7xf32>\n  return %ret : tensor<128x128x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x7x7xf32>, %arg1: tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x7x7xf32>\n    return %1 : tensor<128x128x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x7x7xf32>) -> tensor<128x128x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x7x7xf32>) outs(%25 : tensor<128x128x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 641391.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x120x120xf32>) outs(%25 : tensor<128x128x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x120x120xf32>) outs(%25 : tensor<128x128x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x120x120xf32>, %25: tensor<128x128x120x120xf32>) -> tensor<128x128x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x120x120xf32>) outs(%25 : tensor<128x128x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x120x120xf32>\n  return %ret : tensor<128x128x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x120x120xf32>, %arg1: tensor<128x128x120x120xf32>) -> tensor<128x128x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x120x120xf32>\n    return %1 : tensor<128x128x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x120x120xf32>) -> tensor<128x128x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x120x120xf32>) -> tensor<128x128x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x120x120xf32>) outs(%25 : tensor<128x128x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 195336258
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x120x120xf32>) outs(%25 : tensor<128x288x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x120x120xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x120x120xf32>) outs(%25 : tensor<128x288x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x120x120xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x288x120x120xf32>, %25: tensor<128x288x120x120xf32>) -> tensor<128x288x120x120xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x120x120xf32>) outs(%25 : tensor<128x288x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x120x120xf32>\n  return %ret : tensor<128x288x120x120xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x288x120x120xf32>, %arg1: tensor<128x288x120x120xf32>) -> tensor<128x288x120x120xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x288x120x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x120x120xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 120 {\n          affine.for %arg5 = 0 to 120 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x288x120x120xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x288x120x120xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x288x120x120xf32>\n    return %1 : tensor<128x288x120x120xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x120x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x288x120x120xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x288x120x120xf32>) -> tensor<128x288x120x120xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x288x120x120xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x288x120x120xf32>) -> tensor<128x288x120x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x120x120xf32>) outs(%25 : tensor<128x288x120x120xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x120x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x120x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x120x120xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          120,
          1
        ],
        [
          "%arg5",
          0,
          120,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 459495910
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x240x240xf32>) outs(%25 : tensor<128x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x240x240xf32>) outs(%25 : tensor<128x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x64x240x240xf32>, %25: tensor<128x64x240x240xf32>) -> tensor<128x64x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x240x240xf32>) outs(%25 : tensor<128x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x240x240xf32>\n  return %ret : tensor<128x64x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x64x240x240xf32>, %arg1: tensor<128x64x240x240xf32>) -> tensor<128x64x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x64x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x240x240xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x64x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x64x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x64x240x240xf32>\n    return %1 : tensor<128x64x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x64x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x64x240x240xf32>) -> tensor<128x64x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x64x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x64x240x240xf32>) -> tensor<128x64x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x240x240xf32>) outs(%25 : tensor<128x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 388450427
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x112x112xf32>) outs(%25 : tensor<256x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x112x112xf32>) outs(%25 : tensor<256x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x512x112x112xf32>, %25: tensor<256x512x112x112xf32>) -> tensor<256x512x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x112x112xf32>) outs(%25 : tensor<256x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x112x112xf32>\n  return %ret : tensor<256x512x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x512x112x112xf32>, %arg1: tensor<256x512x112x112xf32>) -> tensor<256x512x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x512x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x512x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x512x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x512x112x112xf32>\n    return %1 : tensor<256x512x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x512x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x512x112x112xf32>) -> tensor<256x512x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x512x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x512x112x112xf32>) -> tensor<256x512x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x112x112xf32>) outs(%25 : tensor<256x512x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1548791068
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x14x14xf32>) outs(%25 : tensor<128x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x14x14xf32>) outs(%25 : tensor<128x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x14x14xf32>, %25: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x14x14xf32>) outs(%25 : tensor<128x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x14x14xf32>\n  return %ret : tensor<128x32x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x14x14xf32>, %arg1: tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x14x14xf32>\n    return %1 : tensor<128x32x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x14x14xf32>) -> tensor<128x32x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x14x14xf32>) outs(%25 : tensor<128x32x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 607155.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x240x240xf32>) outs(%25 : tensor<128x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x240x240xf32>) outs(%25 : tensor<128x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x240x240xf32>, %25: tensor<128x256x240x240xf32>) -> tensor<128x256x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x240x240xf32>) outs(%25 : tensor<128x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x240x240xf32>\n  return %ret : tensor<128x256x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x240x240xf32>, %arg1: tensor<128x256x240x240xf32>) -> tensor<128x256x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x240x240xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x240x240xf32>\n    return %1 : tensor<128x256x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x240x240xf32>) -> tensor<128x256x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x240x240xf32>) -> tensor<128x256x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x240x240xf32>) outs(%25 : tensor<128x256x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1720911957
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x15x15xf32>) outs(%25 : tensor<128x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x15x15xf32>) outs(%25 : tensor<128x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x64x15x15xf32>, %25: tensor<128x64x15x15xf32>) -> tensor<128x64x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x15x15xf32>) outs(%25 : tensor<128x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x15x15xf32>\n  return %ret : tensor<128x64x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x64x15x15xf32>, %arg1: tensor<128x64x15x15xf32>) -> tensor<128x64x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x64x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x64x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x64x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x64x15x15xf32>\n    return %1 : tensor<128x64x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x64x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x64x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x64x15x15xf32>) -> tensor<128x64x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x64x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x64x15x15xf32>) -> tensor<128x64x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x64x15x15xf32>) outs(%25 : tensor<128x64x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x64x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x64x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x64x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1396432
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x224x224xf32>) outs(%25 : tensor<256x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x224x224xf32>) outs(%25 : tensor<256x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x128x224x224xf32>, %25: tensor<256x128x224x224xf32>) -> tensor<256x128x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x224x224xf32>) outs(%25 : tensor<256x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x224x224xf32>\n  return %ret : tensor<256x128x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x128x224x224xf32>, %arg1: tensor<256x128x224x224xf32>) -> tensor<256x128x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x128x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x128x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x128x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x128x224x224xf32>\n    return %1 : tensor<256x128x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x128x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x128x224x224xf32>) -> tensor<256x128x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x128x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x128x224x224xf32>) -> tensor<256x128x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x224x224xf32>) outs(%25 : tensor<256x128x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1488880298
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x14x14xf32>) outs(%25 : tensor<128x128x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x14x14xf32>) outs(%25 : tensor<128x128x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x14x14xf32>, %25: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x14x14xf32>) outs(%25 : tensor<128x128x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x14x14xf32>\n  return %ret : tensor<128x128x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x14x14xf32>, %arg1: tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x14x14xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x14x14xf32>\n    return %1 : tensor<128x128x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x14x14xf32>) -> tensor<128x128x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x14x14xf32>) outs(%25 : tensor<128x128x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2483525
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x7x7xf32>) outs(%25 : tensor<128x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x7x7xf32>) outs(%25 : tensor<128x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x7x7xf32>, %25: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x7x7xf32>) outs(%25 : tensor<128x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x7x7xf32>\n  return %ret : tensor<128x96x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x7x7xf32>, %arg1: tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x7x7xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x7x7xf32>\n    return %1 : tensor<128x96x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x7x7xf32>) -> tensor<128x96x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x7x7xf32>) outs(%25 : tensor<128x96x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 483473.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x224x224xf32>) outs(%25 : tensor<256x288x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x224x224xf32>) outs(%25 : tensor<256x288x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x288x224x224xf32>, %25: tensor<256x288x224x224xf32>) -> tensor<256x288x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x224x224xf32>) outs(%25 : tensor<256x288x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x224x224xf32>\n  return %ret : tensor<256x288x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x288x224x224xf32>, %arg1: tensor<256x288x224x224xf32>) -> tensor<256x288x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x288x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x288x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x288x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x288x224x224xf32>\n    return %1 : tensor<256x288x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x288x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x288x224x224xf32>) -> tensor<256x288x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x288x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x288x224x224xf32>) -> tensor<256x288x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x224x224xf32>) outs(%25 : tensor<256x288x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3376639387
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x130x130xf32>) outs(%25 : tensor<256x240x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x130x130xf32>) outs(%25 : tensor<256x240x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x130x130xf32>, %25: tensor<256x240x130x130xf32>) -> tensor<256x240x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x130x130xf32>) outs(%25 : tensor<256x240x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x130x130xf32>\n  return %ret : tensor<256x240x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x130x130xf32>, %arg1: tensor<256x240x130x130xf32>) -> tensor<256x240x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x130x130xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x130x130xf32>\n    return %1 : tensor<256x240x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x130x130xf32>) -> tensor<256x240x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x130x130xf32>) -> tensor<256x240x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x130x130xf32>) outs(%25 : tensor<256x240x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 880703576
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x28x28xf32>) outs(%25 : tensor<256x48x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x28x28xf32>) outs(%25 : tensor<256x48x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x48x28x28xf32>, %25: tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x28x28xf32>) outs(%25 : tensor<256x48x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x28x28xf32>\n  return %ret : tensor<256x48x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x48x28x28xf32>, %arg1: tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x48x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x48x28x28xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x48x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x48x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x48x28x28xf32>\n    return %1 : tensor<256x48x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x48x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x48x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x48x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x48x28x28xf32>) -> tensor<256x48x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x48x28x28xf32>) outs(%25 : tensor<256x48x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x48x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x48x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x48x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 7337124
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x2048xf32>) outs(%35 : tensor<128x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x2048xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x2048xf32>) outs(%35 : tensor<128x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x2048xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<128x2048xf32>, %35: tensor<128x2048xf32>) -> tensor<128x2048xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x2048xf32>) outs(%35 : tensor<128x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x2048xf32>\n  return %ret : tensor<128x2048xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x2048xf32>, %arg1: tensor<128x2048xf32>) -> tensor<128x2048xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x2048xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x2048xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 2048 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<128x2048xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<128x2048xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x2048xf32>\n    return %1 : tensor<128x2048xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<128x2048xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<128x2048xf32>) -> tensor<128x2048xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<128x2048xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<128x2048xf32>) -> tensor<128x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<128x2048xf32>) outs(%35 : tensor<128x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<128x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x2048xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          2048,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 199739.5
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x228x228xf32>) outs(%25 : tensor<128x288x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x228x228xf32>) outs(%25 : tensor<128x288x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x288x228x228xf32>, %25: tensor<128x288x228x228xf32>) -> tensor<128x288x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x228x228xf32>) outs(%25 : tensor<128x288x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x228x228xf32>\n  return %ret : tensor<128x288x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x288x228x228xf32>, %arg1: tensor<128x288x228x228xf32>) -> tensor<128x288x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x288x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x228x228xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x288x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x288x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x288x228x228xf32>\n    return %1 : tensor<128x288x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x288x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x288x228x228xf32>) -> tensor<128x288x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x288x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x288x228x228xf32>) -> tensor<128x288x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x228x228xf32>) outs(%25 : tensor<128x288x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1729941843
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x240x240xf32>) outs(%25 : tensor<128x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x240x240xf32>) outs(%25 : tensor<128x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x240x240xf32>, %25: tensor<128x32x240x240xf32>) -> tensor<128x32x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x240x240xf32>) outs(%25 : tensor<128x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x240x240xf32>\n  return %ret : tensor<128x32x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x240x240xf32>, %arg1: tensor<128x32x240x240xf32>) -> tensor<128x32x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x240x240xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x240x240xf32>\n    return %1 : tensor<128x32x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x240x240xf32>) -> tensor<128x32x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x240x240xf32>) -> tensor<128x32x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x240x240xf32>) outs(%25 : tensor<128x32x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 189701689
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x56x56xf32>) outs(%25 : tensor<256x288x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x56x56xf32>) outs(%25 : tensor<256x288x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x288x56x56xf32>, %25: tensor<256x288x56x56xf32>) -> tensor<256x288x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x56x56xf32>) outs(%25 : tensor<256x288x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x56x56xf32>\n  return %ret : tensor<256x288x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x288x56x56xf32>, %arg1: tensor<256x288x56x56xf32>) -> tensor<256x288x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x288x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x288x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x288x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x288x56x56xf32>\n    return %1 : tensor<256x288x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x288x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x288x56x56xf32>) -> tensor<256x288x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x288x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x288x56x56xf32>) -> tensor<256x288x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x56x56xf32>) outs(%25 : tensor<256x288x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 197137819
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x240x240xf32>) outs(%25 : tensor<256x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x240x240xf32>) outs(%25 : tensor<256x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x240x240xf32>, %25: tensor<256x384x240x240xf32>) -> tensor<256x384x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x240x240xf32>) outs(%25 : tensor<256x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x240x240xf32>\n  return %ret : tensor<256x384x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x240x240xf32>, %arg1: tensor<256x384x240x240xf32>) -> tensor<256x384x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x240x240xf32>\n    return %1 : tensor<256x384x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x240x240xf32>) -> tensor<256x384x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x240x240xf32>) -> tensor<256x384x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x240x240xf32>) outs(%25 : tensor<256x384x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 5266212794
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x130x130xf32>) outs(%25 : tensor<128x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x130x130xf32>) outs(%25 : tensor<128x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x288x130x130xf32>, %25: tensor<128x288x130x130xf32>) -> tensor<128x288x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x130x130xf32>) outs(%25 : tensor<128x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x130x130xf32>\n  return %ret : tensor<128x288x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x288x130x130xf32>, %arg1: tensor<128x288x130x130xf32>) -> tensor<128x288x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x288x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x288x130x130xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x288x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x288x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x288x130x130xf32>\n    return %1 : tensor<128x288x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x288x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x288x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x288x130x130xf32>) -> tensor<128x288x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x288x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x288x130x130xf32>) -> tensor<128x288x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x288x130x130xf32>) outs(%25 : tensor<128x288x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x288x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x288x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x288x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 546535543
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x15x15xf32>) outs(%25 : tensor<128x192x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x15x15xf32>) outs(%25 : tensor<128x192x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x192x15x15xf32>, %25: tensor<128x192x15x15xf32>) -> tensor<128x192x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x15x15xf32>) outs(%25 : tensor<128x192x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x15x15xf32>\n  return %ret : tensor<128x192x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x192x15x15xf32>, %arg1: tensor<128x192x15x15xf32>) -> tensor<128x192x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x192x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x192x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x192x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x192x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x192x15x15xf32>\n    return %1 : tensor<128x192x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x192x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x192x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x192x15x15xf32>) -> tensor<128x192x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x192x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x192x15x15xf32>) -> tensor<128x192x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x192x15x15xf32>) outs(%25 : tensor<128x192x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x192x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x192x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x192x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4193847
  },
  "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x2048xf32>) outs(%35 : tensor<256x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x2048xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x2048xf32>) outs(%35 : tensor<256x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x2048xf32>",
    "wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nfunc.func @func_call(%38: tensor<256x2048xf32>, %35: tensor<256x2048xf32>) -> tensor<256x2048xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x2048xf32>) outs(%35 : tensor<256x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x2048xf32>\n  return %ret : tensor<256x2048xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x2048xf32>, %arg1: tensor<256x2048xf32>) -> tensor<256x2048xf32> {\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x2048xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x2048xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 2048 {\n        %2 = affine.load %0[%arg2, %arg3] : memref<256x2048xf32>\n        %3 = arith.cmpf ugt, %2, %cst : f32\n        %4 = arith.select %3, %2, %cst : f32\n        affine.store %4, %alloc[%arg2, %arg3] : memref<256x2048xf32>\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x2048xf32>\n    return %1 : tensor<256x2048xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map2 = affine_map<(d0, d1) -> (d0, d1)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_38 = bufferization.alloc_tensor() : tensor<256x2048xf32>\n%38 = linalg.fill ins(%val : f32) outs(%tmp_38 : tensor<256x2048xf32>) -> tensor<256x2048xf32>\n%tmp_35 = bufferization.alloc_tensor() : tensor<256x2048xf32>\n%35 = linalg.fill ins(%val : f32) outs(%tmp_35 : tensor<256x2048xf32>) -> tensor<256x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<256x2048xf32>) outs(%35 : tensor<256x2048xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %46 = arith.cmpf ugt, %in, %cst_1 : f32\n            %47 = arith.select %46, %in, %cst_1 : f32\n            linalg.yield %47 : f32\n        } -> tensor<256x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x2048xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          2048,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%arg2",
          "%arg3"
        ]
      ],
      "store_data": []
    },
    "execution_time": 399441
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x240x240xf32>) outs(%25 : tensor<256x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x240x240xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x240x240xf32>) outs(%25 : tensor<256x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x240x240xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x64x240x240xf32>, %25: tensor<256x64x240x240xf32>) -> tensor<256x64x240x240xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x240x240xf32>) outs(%25 : tensor<256x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x240x240xf32>\n  return %ret : tensor<256x64x240x240xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x64x240x240xf32>, %arg1: tensor<256x64x240x240xf32>) -> tensor<256x64x240x240xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x64x240x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x240x240xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 240 {\n          affine.for %arg5 = 0 to 240 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x64x240x240xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x64x240x240xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x64x240x240xf32>\n    return %1 : tensor<256x64x240x240xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x240x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x64x240x240xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x64x240x240xf32>) -> tensor<256x64x240x240xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x64x240x240xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x64x240x240xf32>) -> tensor<256x64x240x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x240x240xf32>) outs(%25 : tensor<256x64x240x240xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x240x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x240x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x240x240xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          240,
          1
        ],
        [
          "%arg5",
          0,
          240,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 771588596
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x28x28xf32>) outs(%25 : tensor<256x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x28x28xf32>) outs(%25 : tensor<256x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x28x28xf32>, %25: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x28x28xf32>) outs(%25 : tensor<256x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x28x28xf32>\n  return %ret : tensor<256x32x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x28x28xf32>, %arg1: tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x28x28xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x28x28xf32>\n    return %1 : tensor<256x32x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x28x28xf32>) -> tensor<256x32x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x28x28xf32>) outs(%25 : tensor<256x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 4677425
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x28x28xf32>) outs(%25 : tensor<128x256x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x28x28xf32>) outs(%25 : tensor<128x256x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x256x28x28xf32>, %25: tensor<128x256x28x28xf32>) -> tensor<128x256x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x28x28xf32>) outs(%25 : tensor<128x256x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x28x28xf32>\n  return %ret : tensor<128x256x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x256x28x28xf32>, %arg1: tensor<128x256x28x28xf32>) -> tensor<128x256x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x256x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x256x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x256x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x256x28x28xf32>\n    return %1 : tensor<128x256x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x256x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x256x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x256x28x28xf32>) -> tensor<128x256x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x256x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x256x28x28xf32>) -> tensor<128x256x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x256x28x28xf32>) outs(%25 : tensor<128x256x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x256x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x256x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x256x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 20063981
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x7x7xf32>) outs(%25 : tensor<256x240x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x7x7xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x7x7xf32>) outs(%25 : tensor<256x240x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x7x7xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x240x7x7xf32>, %25: tensor<256x240x7x7xf32>) -> tensor<256x240x7x7xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x7x7xf32>) outs(%25 : tensor<256x240x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x7x7xf32>\n  return %ret : tensor<256x240x7x7xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x240x7x7xf32>, %arg1: tensor<256x240x7x7xf32>) -> tensor<256x240x7x7xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x240x7x7xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x240x7x7xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 240 {\n        affine.for %arg4 = 0 to 7 {\n          affine.for %arg5 = 0 to 7 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x240x7x7xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x240x7x7xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x240x7x7xf32>\n    return %1 : tensor<256x240x7x7xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x240x7x7xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x240x7x7xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x240x7x7xf32>) -> tensor<256x240x7x7xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x240x7x7xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x240x7x7xf32>) -> tensor<256x240x7x7xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x240x7x7xf32>) outs(%25 : tensor<256x240x7x7xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x240x7x7xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x240x7x7xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x240x7x7xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          240,
          1
        ],
        [
          "%arg4",
          0,
          7,
          1
        ],
        [
          "%arg5",
          0,
          7,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2402506
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x56x56xf32>) outs(%25 : tensor<256x384x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x56x56xf32>) outs(%25 : tensor<256x384x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x56x56xf32>, %25: tensor<256x384x56x56xf32>) -> tensor<256x384x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x56x56xf32>) outs(%25 : tensor<256x384x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x56x56xf32>\n  return %ret : tensor<256x384x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x56x56xf32>, %arg1: tensor<256x384x56x56xf32>) -> tensor<256x384x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x56x56xf32>\n    return %1 : tensor<256x384x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x56x56xf32>) -> tensor<256x384x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x56x56xf32>) -> tensor<256x384x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x56x56xf32>) outs(%25 : tensor<256x384x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 261040031
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x112x112xf32>) outs(%25 : tensor<128x128x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x112x112xf32>) outs(%25 : tensor<128x128x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x112x112xf32>, %25: tensor<128x128x112x112xf32>) -> tensor<128x128x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x112x112xf32>) outs(%25 : tensor<128x128x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x112x112xf32>\n  return %ret : tensor<128x128x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x112x112xf32>, %arg1: tensor<128x128x112x112xf32>) -> tensor<128x128x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x112x112xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x112x112xf32>\n    return %1 : tensor<128x128x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x112x112xf32>) -> tensor<128x128x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x112x112xf32>) -> tensor<128x128x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x112x112xf32>) outs(%25 : tensor<128x128x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 169712452
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x228x228xf32>) outs(%25 : tensor<256x64x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x228x228xf32>) outs(%25 : tensor<256x64x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x64x228x228xf32>, %25: tensor<256x64x228x228xf32>) -> tensor<256x64x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x228x228xf32>) outs(%25 : tensor<256x64x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x228x228xf32>\n  return %ret : tensor<256x64x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x64x228x228xf32>, %arg1: tensor<256x64x228x228xf32>) -> tensor<256x64x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x64x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x228x228xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 64 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x64x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x64x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x64x228x228xf32>\n    return %1 : tensor<256x64x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x64x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x64x228x228xf32>) -> tensor<256x64x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x64x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x64x228x228xf32>) -> tensor<256x64x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x64x228x228xf32>) outs(%25 : tensor<256x64x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x64x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          64,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 688366870
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x56x56xf32>) outs(%25 : tensor<256x128x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x56x56xf32>) outs(%25 : tensor<256x128x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x128x56x56xf32>, %25: tensor<256x128x56x56xf32>) -> tensor<256x128x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x56x56xf32>) outs(%25 : tensor<256x128x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x56x56xf32>\n  return %ret : tensor<256x128x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x128x56x56xf32>, %arg1: tensor<256x128x56x56xf32>) -> tensor<256x128x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x128x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x128x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x128x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x128x56x56xf32>\n    return %1 : tensor<256x128x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x128x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x128x56x56xf32>) -> tensor<256x128x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x128x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x128x56x56xf32>) -> tensor<256x128x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x128x56x56xf32>) outs(%25 : tensor<256x128x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x128x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 87193888
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x14x14xf32>) outs(%25 : tensor<256x96x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x14x14xf32>) outs(%25 : tensor<256x96x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x14x14xf32>, %25: tensor<256x96x14x14xf32>) -> tensor<256x96x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x14x14xf32>) outs(%25 : tensor<256x96x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x14x14xf32>\n  return %ret : tensor<256x96x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x14x14xf32>, %arg1: tensor<256x96x14x14xf32>) -> tensor<256x96x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x14x14xf32>\n    return %1 : tensor<256x96x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x14x14xf32>) -> tensor<256x96x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x14x14xf32>) -> tensor<256x96x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x14x14xf32>) outs(%25 : tensor<256x96x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3646686
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x14x14xf32>) outs(%25 : tensor<256x288x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x14x14xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x14x14xf32>) outs(%25 : tensor<256x288x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x14x14xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x288x14x14xf32>, %25: tensor<256x288x14x14xf32>) -> tensor<256x288x14x14xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x14x14xf32>) outs(%25 : tensor<256x288x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x14x14xf32>\n  return %ret : tensor<256x288x14x14xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x288x14x14xf32>, %arg1: tensor<256x288x14x14xf32>) -> tensor<256x288x14x14xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x288x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x288x14x14xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 288 {\n        affine.for %arg4 = 0 to 14 {\n          affine.for %arg5 = 0 to 14 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x288x14x14xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x288x14x14xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x288x14x14xf32>\n    return %1 : tensor<256x288x14x14xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x288x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x288x14x14xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x288x14x14xf32>) -> tensor<256x288x14x14xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x288x14x14xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x288x14x14xf32>) -> tensor<256x288x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x288x14x14xf32>) outs(%25 : tensor<256x288x14x14xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x288x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x288x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x288x14x14xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          288,
          1
        ],
        [
          "%arg4",
          0,
          14,
          1
        ],
        [
          "%arg5",
          0,
          14,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 11918795
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x15x15xf32>) outs(%25 : tensor<128x96x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x15x15xf32>) outs(%25 : tensor<128x96x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x96x15x15xf32>, %25: tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x15x15xf32>) outs(%25 : tensor<128x96x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x15x15xf32>\n  return %ret : tensor<128x96x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x96x15x15xf32>, %arg1: tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x96x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x96x15x15xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x96x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x96x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x96x15x15xf32>\n    return %1 : tensor<128x96x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x96x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x96x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x96x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x96x15x15xf32>) -> tensor<128x96x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x96x15x15xf32>) outs(%25 : tensor<128x96x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x96x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x96x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x96x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2094711
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x150x150xf32>) outs(%25 : tensor<128x128x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x150x150xf32>) outs(%25 : tensor<128x128x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x128x150x150xf32>, %25: tensor<128x128x150x150xf32>) -> tensor<128x128x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x150x150xf32>) outs(%25 : tensor<128x128x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x150x150xf32>\n  return %ret : tensor<128x128x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x128x150x150xf32>, %arg1: tensor<128x128x150x150xf32>) -> tensor<128x128x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x128x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x150x150xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 128 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x128x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x128x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x128x150x150xf32>\n    return %1 : tensor<128x128x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x128x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x128x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x128x150x150xf32>) -> tensor<128x128x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x128x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x128x150x150xf32>) -> tensor<128x128x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x128x150x150xf32>) outs(%25 : tensor<128x128x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x128x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x128x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x128x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          128,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 303353493
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x224x224xf32>) outs(%25 : tensor<256x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x224x224xf32>) outs(%25 : tensor<256x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x256x224x224xf32>, %25: tensor<256x256x224x224xf32>) -> tensor<256x256x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x224x224xf32>) outs(%25 : tensor<256x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x224x224xf32>\n  return %ret : tensor<256x256x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x256x224x224xf32>, %arg1: tensor<256x256x224x224xf32>) -> tensor<256x256x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x256x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 256 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x256x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x256x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x256x224x224xf32>\n    return %1 : tensor<256x256x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x256x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x256x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x256x224x224xf32>) -> tensor<256x256x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x256x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x256x224x224xf32>) -> tensor<256x256x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x256x224x224xf32>) outs(%25 : tensor<256x256x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x256x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x256x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x256x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          256,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 3001118954
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x130x130xf32>) outs(%25 : tensor<128x48x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x130x130xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x130x130xf32>) outs(%25 : tensor<128x48x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x130x130xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x48x130x130xf32>, %25: tensor<128x48x130x130xf32>) -> tensor<128x48x130x130xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x130x130xf32>) outs(%25 : tensor<128x48x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x130x130xf32>\n  return %ret : tensor<128x48x130x130xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x48x130x130xf32>, %arg1: tensor<128x48x130x130xf32>) -> tensor<128x48x130x130xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x48x130x130xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x48x130x130xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 48 {\n        affine.for %arg4 = 0 to 130 {\n          affine.for %arg5 = 0 to 130 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x48x130x130xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x48x130x130xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x48x130x130xf32>\n    return %1 : tensor<128x48x130x130xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x48x130x130xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x48x130x130xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x48x130x130xf32>) -> tensor<128x48x130x130xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x48x130x130xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x48x130x130xf32>) -> tensor<128x48x130x130xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x48x130x130xf32>) outs(%25 : tensor<128x48x130x130xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x48x130x130xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x48x130x130xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x48x130x130xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          48,
          1
        ],
        [
          "%arg4",
          0,
          130,
          1
        ],
        [
          "%arg5",
          0,
          130,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 85763513
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x224x224xf32>) outs(%25 : tensor<256x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x224x224xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x224x224xf32>) outs(%25 : tensor<256x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x224x224xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x224x224xf32>, %25: tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x224x224xf32>) outs(%25 : tensor<256x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x224x224xf32>\n  return %ret : tensor<256x32x224x224xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x224x224xf32>, %arg1: tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x224x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x224x224xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 224 {\n          affine.for %arg5 = 0 to 224 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x224x224xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x224x224xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x224x224xf32>\n    return %1 : tensor<256x32x224x224xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x224x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x224x224xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x224x224xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x224x224xf32>) -> tensor<256x32x224x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x224x224xf32>) outs(%25 : tensor<256x32x224x224xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x224x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x224x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x224x224xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          224,
          1
        ],
        [
          "%arg5",
          0,
          224,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 330816074
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x150x150xf32>) outs(%25 : tensor<256x32x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x150x150xf32>) outs(%25 : tensor<256x32x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x150x150xf32>, %25: tensor<256x32x150x150xf32>) -> tensor<256x32x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x150x150xf32>) outs(%25 : tensor<256x32x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x150x150xf32>\n  return %ret : tensor<256x32x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x150x150xf32>, %arg1: tensor<256x32x150x150xf32>) -> tensor<256x32x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x150x150xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x150x150xf32>\n    return %1 : tensor<256x32x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x150x150xf32>) -> tensor<256x32x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x150x150xf32>) -> tensor<256x32x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x150x150xf32>) outs(%25 : tensor<256x32x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 150565058
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x228x228xf32>) outs(%25 : tensor<256x96x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x228x228xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x228x228xf32>) outs(%25 : tensor<256x96x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x228x228xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x96x228x228xf32>, %25: tensor<256x96x228x228xf32>) -> tensor<256x96x228x228xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x228x228xf32>) outs(%25 : tensor<256x96x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x228x228xf32>\n  return %ret : tensor<256x96x228x228xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x96x228x228xf32>, %arg1: tensor<256x96x228x228xf32>) -> tensor<256x96x228x228xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x96x228x228xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x96x228x228xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 96 {\n        affine.for %arg4 = 0 to 228 {\n          affine.for %arg5 = 0 to 228 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x96x228x228xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x96x228x228xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x96x228x228xf32>\n    return %1 : tensor<256x96x228x228xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x96x228x228xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x96x228x228xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x96x228x228xf32>) -> tensor<256x96x228x228xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x96x228x228xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x96x228x228xf32>) -> tensor<256x96x228x228xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x96x228x228xf32>) outs(%25 : tensor<256x96x228x228xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x96x228x228xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x96x228x228xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x96x228x228xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          96,
          1
        ],
        [
          "%arg4",
          0,
          228,
          1
        ],
        [
          "%arg5",
          0,
          228,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 1097713124
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x28x28xf32>) outs(%25 : tensor<128x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x28x28xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x28x28xf32>) outs(%25 : tensor<128x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x28x28xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<128x32x28x28xf32>, %25: tensor<128x32x28x28xf32>) -> tensor<128x32x28x28xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x28x28xf32>) outs(%25 : tensor<128x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x28x28xf32>\n  return %ret : tensor<128x32x28x28xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<128x32x28x28xf32>, %arg1: tensor<128x32x28x28xf32>) -> tensor<128x32x28x28xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<128x32x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x28x28xf32>\n    affine.for %arg2 = 0 to 128 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 28 {\n          affine.for %arg5 = 0 to 28 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<128x32x28x28xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<128x32x28x28xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<128x32x28x28xf32>\n    return %1 : tensor<128x32x28x28xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<128x32x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<128x32x28x28xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<128x32x28x28xf32>) -> tensor<128x32x28x28xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<128x32x28x28xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<128x32x28x28xf32>) -> tensor<128x32x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<128x32x28x28xf32>) outs(%25 : tensor<128x32x28x28xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<128x32x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<128x32x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<128x32x28x28xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          128,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          28,
          1
        ],
        [
          "%arg5",
          0,
          28,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 2335317
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x15x15xf32>) outs(%25 : tensor<256x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x15x15xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x15x15xf32>) outs(%25 : tensor<256x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x15x15xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x384x15x15xf32>, %25: tensor<256x384x15x15xf32>) -> tensor<256x384x15x15xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x15x15xf32>) outs(%25 : tensor<256x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x15x15xf32>\n  return %ret : tensor<256x384x15x15xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x384x15x15xf32>, %arg1: tensor<256x384x15x15xf32>) -> tensor<256x384x15x15xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x384x15x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x384x15x15xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 384 {\n        affine.for %arg4 = 0 to 15 {\n          affine.for %arg5 = 0 to 15 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x384x15x15xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x384x15x15xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x384x15x15xf32>\n    return %1 : tensor<256x384x15x15xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x384x15x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x384x15x15xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x384x15x15xf32>) -> tensor<256x384x15x15xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x384x15x15xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x384x15x15xf32>) -> tensor<256x384x15x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x384x15x15xf32>) outs(%25 : tensor<256x384x15x15xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x384x15x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x384x15x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x384x15x15xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          384,
          1
        ],
        [
          "%arg4",
          0,
          15,
          1
        ],
        [
          "%arg5",
          0,
          15,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 18126454
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x112x112xf32>) outs(%25 : tensor<256x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x112x112xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x112x112xf32>) outs(%25 : tensor<256x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x112x112xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x32x112x112xf32>, %25: tensor<256x32x112x112xf32>) -> tensor<256x32x112x112xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x112x112xf32>) outs(%25 : tensor<256x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x112x112xf32>\n  return %ret : tensor<256x32x112x112xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x32x112x112xf32>, %arg1: tensor<256x32x112x112xf32>) -> tensor<256x32x112x112xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x32x112x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x112x112xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 32 {\n        affine.for %arg4 = 0 to 112 {\n          affine.for %arg5 = 0 to 112 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x32x112x112xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x32x112x112xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x32x112x112xf32>\n    return %1 : tensor<256x32x112x112xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x112x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x32x112x112xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x32x112x112xf32>) -> tensor<256x32x112x112xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x32x112x112xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x32x112x112xf32>) -> tensor<256x32x112x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x32x112x112xf32>) outs(%25 : tensor<256x32x112x112xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x32x112x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x112x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x112x112xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          32,
          1
        ],
        [
          "%arg4",
          0,
          112,
          1
        ],
        [
          "%arg5",
          0,
          112,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 85198281
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x56x56xf32>) outs(%25 : tensor<256x512x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x56x56xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x56x56xf32>) outs(%25 : tensor<256x512x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x56x56xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x512x56x56xf32>, %25: tensor<256x512x56x56xf32>) -> tensor<256x512x56x56xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x56x56xf32>) outs(%25 : tensor<256x512x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x56x56xf32>\n  return %ret : tensor<256x512x56x56xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x512x56x56xf32>, %arg1: tensor<256x512x56x56xf32>) -> tensor<256x512x56x56xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x512x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x56x56xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 512 {\n        affine.for %arg4 = 0 to 56 {\n          affine.for %arg5 = 0 to 56 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x512x56x56xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x512x56x56xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x512x56x56xf32>\n    return %1 : tensor<256x512x56x56xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x512x56x56xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x512x56x56xf32>) -> tensor<256x512x56x56xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x512x56x56xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x512x56x56xf32>) -> tensor<256x512x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x512x56x56xf32>) outs(%25 : tensor<256x512x56x56xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x512x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x56x56xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          512,
          1
        ],
        [
          "%arg4",
          0,
          56,
          1
        ],
        [
          "%arg5",
          0,
          56,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 347747154
  },
  "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x150x150xf32>) outs(%25 : tensor<256x192x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x150x150xf32>": {
    "operation": "linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x150x150xf32>) outs(%25 : tensor<256x192x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x150x150xf32>",
    "wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nfunc.func @func_call(%28: tensor<256x192x150x150xf32>, %25: tensor<256x192x150x150xf32>) -> tensor<256x192x150x150xf32> {\n  %ret = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x150x150xf32>) outs(%25 : tensor<256x192x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x150x150xf32>\n  return %ret : tensor<256x192x150x150xf32>\n}",
    "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<256x192x150x150xf32>, %arg1: tensor<256x192x150x150xf32>) -> tensor<256x192x150x150xf32> {\n    %c0 = arith.constant 0 : index\n    %cst = arith.constant 0.000000e+00 : f32\n    %0 = bufferization.to_memref %arg0 : memref<256x192x150x150xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x192x150x150xf32>\n    affine.for %arg2 = 0 to 256 {\n      affine.for %arg3 = 0 to 192 {\n        affine.for %arg4 = 0 to 150 {\n          affine.for %arg5 = 0 to 150 {\n            %2 = affine.load %0[%c0, %arg3, %arg4, %arg5] : memref<256x192x150x150xf32>\n            %3 = arith.cmpf ugt, %2, %cst : f32\n            %4 = arith.select %3, %2, %cst : f32\n            affine.store %4, %alloc[%arg2, %arg3, %arg4, %arg5] : memref<256x192x150x150xf32>\n          }\n        }\n      }\n    }\n    %1 = bufferization.to_tensor %alloc : memref<256x192x150x150xf32>\n    return %1 : tensor<256x192x150x150xf32>\n  }\n}\n\n",
    "transform_wrapped_operation": "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n        #map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\nmodule attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x192x150x150xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_28 = bufferization.alloc_tensor() : tensor<256x192x150x150xf32>\n%28 = linalg.fill ins(%val : f32) outs(%tmp_28 : tensor<256x192x150x150xf32>) -> tensor<256x192x150x150xf32>\n%tmp_25 = bufferization.alloc_tensor() : tensor<256x192x150x150xf32>\n%25 = linalg.fill ins(%val : f32) outs(%tmp_25 : tensor<256x192x150x150xf32>) -> tensor<256x192x150x150xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<256x192x150x150xf32>) outs(%25 : tensor<256x192x150x150xf32>) {\n            ^bb0(%in: f32, %out: f32):\n            %cst_1 = arith.constant 0.000000e+00 : f32\n            %90 = arith.cmpf ugt, %in, %cst_1 : f32\n            %91 = arith.select %90, %in, %cst_1 : f32\n            linalg.yield %91 : f32\n        } -> tensor<256x192x150x150xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x192x150x150xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x192x150x150xf32>\n    }\n    return\n}\n}\n",
    "loops_data": {
      "nested_loops": [
        [
          "%arg2",
          0,
          256,
          1
        ],
        [
          "%arg3",
          0,
          192,
          1
        ],
        [
          "%arg4",
          0,
          150,
          1
        ],
        [
          "%arg5",
          0,
          150,
          1
        ]
      ],
      "op_count": {
        "+": 0,
        "-": 0,
        "*": 0,
        "/": 0,
        "exp": 0
      },
      "load_data": [
        [
          "%c0",
          "%arg3",
          "%arg4",
          "%arg5"
        ]
      ],
      "store_data": []
    },
    "execution_time": 942386419
  }
}