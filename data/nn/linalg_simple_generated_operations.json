{"linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_0": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>_1": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x128xi32>) -> tensor<256x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xi32>\n    memref.copy %2, %alloc : memref<256x128xi32> to memref<256x128xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128xi32>\n    return %3 : tensor<256x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<1x1xi32>, tensor<256x128xi32>) -> (tensor<256x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>_2": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x60xi32>) -> tensor<60x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>\n  return %ret : tensor<60x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x60xi32>) -> tensor<60x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x60xi32>\n    memref.copy %2, %alloc : memref<60x60xi32> to memref<60x60xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x60xi32>\n    return %3 : tensor<60x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x60xi32>) -> tensor<60x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>\n  return %ret : tensor<60x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<5x5xi32>, tensor<60x60xi32>) -> (tensor<60x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>_3": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x1022xi32>\n    memref.copy %2, %alloc : memref<126x1022xi32> to memref<126x1022xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x1022xi32>\n    return %3 : tensor<126x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<3x3xi32>, tensor<126x1022xi32>) -> (tensor<126x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>_4": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x254xi32>) -> tensor<126x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254xi32>\n    memref.copy %2, %alloc : memref<126x254xi32> to memref<126x254xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254xi32>\n    return %3 : tensor<126x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<3x3xi32>, tensor<126x254xi32>) -> (tensor<126x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_5": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>_6": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x508xi32>) -> tensor<60x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508xi32>\n    memref.copy %2, %alloc : memref<60x508xi32> to memref<60x508xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508xi32>\n    return %3 : tensor<60x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<5x5xi32>, tensor<60x508xi32>) -> (tensor<60x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>_7": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x512xi32>) -> tensor<32x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512xi32>\n    memref.copy %2, %alloc : memref<32x512xi32> to memref<32x512xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x512xi32>\n    return %3 : tensor<32x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<1x1xi32>, tensor<32x512xi32>) -> (tensor<32x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<7x7xi32>) outs(%output: tensor<58x26xi32>) -> tensor<58x26xi32>_8": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<7x7xi32>) outs(%output: tensor<58x26xi32>) -> tensor<58x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x26xi32>) -> tensor<58x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<7x7xi32>) outs(%output: tensor<58x26xi32>) -> tensor<58x26xi32>\n  return %ret : tensor<58x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x26xi32>) -> tensor<58x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x26xi32>\n    memref.copy %2, %alloc : memref<58x26xi32> to memref<58x26xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x26xi32>\n    return %3 : tensor<58x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x26xi32>) -> tensor<58x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<7x7xi32>) outs(%output: tensor<58x26xi32>) -> tensor<58x26xi32>\n  return %ret : tensor<58x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<7x7xi32>, tensor<58x26xi32>) -> (tensor<58x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>_9": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x512xi32>) -> tensor<128x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xi32>\n    memref.copy %2, %alloc : memref<128x512xi32> to memref<128x512xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512xi32>\n    return %3 : tensor<128x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<1x1xi32>, tensor<128x512xi32>) -> (tensor<128x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>_10": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x128xi32>) -> tensor<128x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128xi32>\n    memref.copy %2, %alloc : memref<128x128xi32> to memref<128x128xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128xi32>\n    return %3 : tensor<128x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<1x1xi32>, tensor<128x128xi32>) -> (tensor<128x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_11": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>_12": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024xi32>\n    memref.copy %2, %alloc : memref<256x1024xi32> to memref<256x1024xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024xi32>\n    return %3 : tensor<256x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<1x1xi32>, tensor<256x1024xi32>) -> (tensor<256x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>_13": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x62xi32>) -> tensor<254x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>\n  return %ret : tensor<254x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x62xi32>) -> tensor<254x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x62xi32>\n    memref.copy %2, %alloc : memref<254x62xi32> to memref<254x62xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x62xi32>\n    return %3 : tensor<254x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x62xi32>) -> tensor<254x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>\n  return %ret : tensor<254x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64xi32>, tensor<3x3xi32>, tensor<254x62xi32>) -> (tensor<254x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_14": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_15": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>_16": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x506xi32>) -> tensor<506x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506xi32>\n    memref.copy %2, %alloc : memref<506x506xi32> to memref<506x506xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506xi32>\n    return %3 : tensor<506x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<7x7xi32>, tensor<506x506xi32>) -> (tensor<506x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>_17": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018xi32>\n    memref.copy %2, %alloc : memref<250x1018xi32> to memref<250x1018xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018xi32>\n    return %3 : tensor<250x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<7x7xi32>, tensor<250x1018xi32>) -> (tensor<250x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_18": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>_19": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x122xi32>) -> tensor<250x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>\n  return %ret : tensor<250x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x122xi32>) -> tensor<250x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122xi32>\n    memref.copy %2, %alloc : memref<250x122xi32> to memref<250x122xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122xi32>\n    return %3 : tensor<250x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x122xi32>) -> tensor<250x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>\n  return %ret : tensor<250x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<7x7xi32>, tensor<250x122xi32>) -> (tensor<250x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>_20": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x512xi32>\n    memref.copy %2, %alloc : memref<1024x512xi32> to memref<1024x512xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x512xi32>\n    return %3 : tensor<1024x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<1x1xi32>, tensor<1024x512xi32>) -> (tensor<1024x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_21": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>_22": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x254xi32>) -> tensor<126x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254xi32>\n    memref.copy %2, %alloc : memref<126x254xi32> to memref<126x254xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254xi32>\n    return %3 : tensor<126x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<3x3xi32>, tensor<126x254xi32>) -> (tensor<126x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>_23": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x26xi32>) -> tensor<250x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26xi32>\n    memref.copy %2, %alloc : memref<250x26xi32> to memref<250x26xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26xi32>\n    return %3 : tensor<250x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<7x7xi32>, tensor<250x26xi32>) -> (tensor<250x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>_24": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x32xi32>) -> tensor<256x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32xi32>\n    memref.copy %2, %alloc : memref<256x32xi32> to memref<256x32xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32xi32>\n    return %3 : tensor<256x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<1x1xi32>, tensor<256x32xi32>) -> (tensor<256x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>_25": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x1022xi32>\n    memref.copy %2, %alloc : memref<1022x1022xi32> to memref<1022x1022xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x1022xi32>\n    return %3 : tensor<1022x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<3x3xi32>, tensor<1022x1022xi32>) -> (tensor<1022x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>_26": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x1022xi32>\n    memref.copy %2, %alloc : memref<1022x1022xi32> to memref<1022x1022xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x1022xi32>\n    return %3 : tensor<1022x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<3x3xi32>, tensor<1022x1022xi32>) -> (tensor<1022x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<5x5xi32>) outs(%output: tensor<124x124xi32>) -> tensor<124x124xi32>_27": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<5x5xi32>) outs(%output: tensor<124x124xi32>) -> tensor<124x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x124xi32>) -> tensor<124x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<5x5xi32>) outs(%output: tensor<124x124xi32>) -> tensor<124x124xi32>\n  return %ret : tensor<124x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x124xi32>) -> tensor<124x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x124xi32>\n    memref.copy %2, %alloc : memref<124x124xi32> to memref<124x124xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x124xi32>\n    return %3 : tensor<124x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x124xi32>) -> tensor<124x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<5x5xi32>) outs(%output: tensor<124x124xi32>) -> tensor<124x124xi32>\n  return %ret : tensor<124x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<5x5xi32>, tensor<124x124xi32>) -> (tensor<124x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>_28": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128xi32>\n    memref.copy %2, %alloc : memref<1024x128xi32> to memref<1024x128xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128xi32>\n    return %3 : tensor<1024x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<1x1xi32>, tensor<1024x128xi32>) -> (tensor<1024x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_29": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>_30": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x26xi32>) -> tensor<122x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26xi32>\n    memref.copy %2, %alloc : memref<122x26xi32> to memref<122x26xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26xi32>\n    return %3 : tensor<122x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<7x7xi32>, tensor<122x26xi32>) -> (tensor<122x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>_31": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x256xi32>) -> tensor<512x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>\n  return %ret : tensor<512x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x256xi32>) -> tensor<512x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256xi32>\n    memref.copy %2, %alloc : memref<512x256xi32> to memref<512x256xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256xi32>\n    return %3 : tensor<512x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x256xi32>) -> tensor<512x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>\n  return %ret : tensor<512x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<1x1xi32>, tensor<512x256xi32>) -> (tensor<512x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>_32": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>\n  return %ret : tensor<32x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024xi32>\n    memref.copy %2, %alloc : memref<32x1024xi32> to memref<32x1024xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024xi32>\n    return %3 : tensor<32x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>\n  return %ret : tensor<32x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<1x1xi32>, tensor<32x1024xi32>) -> (tensor<32x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>_33": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x58xi32>) -> tensor<58x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58xi32>\n    memref.copy %2, %alloc : memref<58x58xi32> to memref<58x58xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58xi32>\n    return %3 : tensor<58x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<7x7xi32>, tensor<58x58xi32>) -> (tensor<58x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<7x7xi32>) outs(%output: tensor<250x58xi32>) -> tensor<250x58xi32>_34": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<7x7xi32>) outs(%output: tensor<250x58xi32>) -> tensor<250x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x58xi32>) -> tensor<250x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<7x7xi32>) outs(%output: tensor<250x58xi32>) -> tensor<250x58xi32>\n  return %ret : tensor<250x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x58xi32>) -> tensor<250x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x58xi32>\n    memref.copy %2, %alloc : memref<250x58xi32> to memref<250x58xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x58xi32>\n    return %3 : tensor<250x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x58xi32>) -> tensor<250x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<7x7xi32>) outs(%output: tensor<250x58xi32>) -> tensor<250x58xi32>\n  return %ret : tensor<250x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64xi32>, tensor<7x7xi32>, tensor<250x58xi32>) -> (tensor<250x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>_35": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x506xi32>) -> tensor<122x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506xi32>\n    memref.copy %2, %alloc : memref<122x506xi32> to memref<122x506xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506xi32>\n    return %3 : tensor<122x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<7x7xi32>, tensor<122x506xi32>) -> (tensor<122x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>_36": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x508xi32>) -> tensor<60x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508xi32>\n    memref.copy %2, %alloc : memref<60x508xi32> to memref<60x508xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508xi32>\n    return %3 : tensor<60x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<5x5xi32>, tensor<60x508xi32>) -> (tensor<60x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>_37": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x126xi32>) -> tensor<126x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126xi32>\n    memref.copy %2, %alloc : memref<126x126xi32> to memref<126x126xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126xi32>\n    return %3 : tensor<126x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<3x3xi32>, tensor<126x126xi32>) -> (tensor<126x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>_38": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x506xi32>) -> tensor<506x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506xi32>\n    memref.copy %2, %alloc : memref<506x506xi32> to memref<506x506xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506xi32>\n    return %3 : tensor<506x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<7x7xi32>, tensor<506x506xi32>) -> (tensor<506x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>_39": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>\n  return %ret : tensor<1018x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018xi32>\n    memref.copy %2, %alloc : memref<1018x1018xi32> to memref<1018x1018xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018xi32>\n    return %3 : tensor<1018x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>\n  return %ret : tensor<1018x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<7x7xi32>, tensor<1018x1018xi32>) -> (tensor<1018x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>_40": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x58xi32>) -> tensor<506x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>\n  return %ret : tensor<506x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x58xi32>) -> tensor<506x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x58xi32>\n    memref.copy %2, %alloc : memref<506x58xi32> to memref<506x58xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x58xi32>\n    return %3 : tensor<506x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x58xi32>) -> tensor<506x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>\n  return %ret : tensor<506x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<7x7xi32>, tensor<506x58xi32>) -> (tensor<506x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>_41": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x28xi32>) -> tensor<252x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28xi32>\n    memref.copy %2, %alloc : memref<252x28xi32> to memref<252x28xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28xi32>\n    return %3 : tensor<252x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<5x5xi32>, tensor<252x28xi32>) -> (tensor<252x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>_42": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x60xi32>) -> tensor<28x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60xi32>\n    memref.copy %2, %alloc : memref<28x60xi32> to memref<28x60xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60xi32>\n    return %3 : tensor<28x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<5x5xi32>, tensor<28x60xi32>) -> (tensor<28x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>_43": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x250xi32>) -> tensor<122x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250xi32>\n    memref.copy %2, %alloc : memref<122x250xi32> to memref<122x250xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250xi32>\n    return %3 : tensor<122x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<7x7xi32>, tensor<122x250xi32>) -> (tensor<122x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>_44": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x506xi32>) -> tensor<122x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506xi32>\n    memref.copy %2, %alloc : memref<122x506xi32> to memref<122x506xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506xi32>\n    return %3 : tensor<122x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<7x7xi32>, tensor<122x506xi32>) -> (tensor<122x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>_45": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020xi32>\n    memref.copy %2, %alloc : memref<1020x1020xi32> to memref<1020x1020xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020xi32>\n    return %3 : tensor<1020x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<5x5xi32>, tensor<1020x1020xi32>) -> (tensor<1020x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_46": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>_47": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x250xi32>) -> tensor<250x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250xi32>\n    memref.copy %2, %alloc : memref<250x250xi32> to memref<250x250xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250xi32>\n    return %3 : tensor<250x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<7x7xi32>, tensor<250x250xi32>) -> (tensor<250x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>_48": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x122xi32>) -> tensor<58x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122xi32>\n    memref.copy %2, %alloc : memref<58x122xi32> to memref<58x122xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122xi32>\n    return %3 : tensor<58x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<7x7xi32>, tensor<58x122xi32>) -> (tensor<58x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>_49": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x26xi32>) -> tensor<26x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>\n  return %ret : tensor<26x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x26xi32>) -> tensor<26x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x26xi32>\n    memref.copy %2, %alloc : memref<26x26xi32> to memref<26x26xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x26xi32>\n    return %3 : tensor<26x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x26xi32>) -> tensor<26x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>\n  return %ret : tensor<26x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<7x7xi32>, tensor<26x26xi32>) -> (tensor<26x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>_50": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x256xi32>) -> tensor<64x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256xi32>\n    memref.copy %2, %alloc : memref<64x256xi32> to memref<64x256xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256xi32>\n    return %3 : tensor<64x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<1x1xi32>, tensor<64x256xi32>) -> (tensor<64x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>_51": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x250xi32>) -> tensor<250x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250xi32>\n    memref.copy %2, %alloc : memref<250x250xi32> to memref<250x250xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250xi32>\n    return %3 : tensor<250x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<7x7xi32>, tensor<250x250xi32>) -> (tensor<250x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>_52": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020xi32>\n    memref.copy %2, %alloc : memref<28x1020xi32> to memref<28x1020xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020xi32>\n    return %3 : tensor<28x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<5x5xi32>, tensor<28x1020xi32>) -> (tensor<28x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>_53": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x510xi32>) -> tensor<62x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510xi32>\n    memref.copy %2, %alloc : memref<62x510xi32> to memref<62x510xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510xi32>\n    return %3 : tensor<62x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<3x3xi32>, tensor<62x510xi32>) -> (tensor<62x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>_54": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x64xi32>) -> tensor<64x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64xi32>\n    memref.copy %2, %alloc : memref<64x64xi32> to memref<64x64xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64xi32>\n    return %3 : tensor<64x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<1x1xi32>, tensor<64x64xi32>) -> (tensor<64x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>_55": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x60xi32>) -> tensor<60x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>\n  return %ret : tensor<60x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x60xi32>) -> tensor<60x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x60xi32>\n    memref.copy %2, %alloc : memref<60x60xi32> to memref<60x60xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x60xi32>\n    return %3 : tensor<60x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x60xi32>) -> tensor<60x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<5x5xi32>) outs(%output: tensor<60x60xi32>) -> tensor<60x60xi32>\n  return %ret : tensor<60x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<5x5xi32>, tensor<60x60xi32>) -> (tensor<60x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_56": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>_57": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x128xi32>) -> tensor<64x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128xi32>\n    memref.copy %2, %alloc : memref<64x128xi32> to memref<64x128xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128xi32>\n    return %3 : tensor<64x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<1x1xi32>, tensor<64x128xi32>) -> (tensor<64x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>_58": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x250xi32>) -> tensor<58x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>\n  return %ret : tensor<58x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x250xi32>) -> tensor<58x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250xi32>\n    memref.copy %2, %alloc : memref<58x250xi32> to memref<58x250xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250xi32>\n    return %3 : tensor<58x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x250xi32>) -> tensor<58x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>\n  return %ret : tensor<58x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<7x7xi32>, tensor<58x250xi32>) -> (tensor<58x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_59": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>_60": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x58xi32>) -> tensor<58x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58xi32>\n    memref.copy %2, %alloc : memref<58x58xi32> to memref<58x58xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58xi32>\n    return %3 : tensor<58x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<7x7xi32>, tensor<58x58xi32>) -> (tensor<58x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_61": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>_62": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x58xi32>) -> tensor<58x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58xi32>\n    memref.copy %2, %alloc : memref<58x58xi32> to memref<58x58xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58xi32>\n    return %3 : tensor<58x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<7x7xi32>, tensor<58x58xi32>) -> (tensor<58x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>_63": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x64xi32>) -> tensor<64x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64xi32>\n    memref.copy %2, %alloc : memref<64x64xi32> to memref<64x64xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64xi32>\n    return %3 : tensor<64x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<1x1xi32>, tensor<64x64xi32>) -> (tensor<64x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>_64": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x508xi32>) -> tensor<508x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508xi32>\n    memref.copy %2, %alloc : memref<508x508xi32> to memref<508x508xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508xi32>\n    return %3 : tensor<508x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<5x5xi32>, tensor<508x508xi32>) -> (tensor<508x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<128x1024xi32>) -> tensor<128x1024xi32>_65": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<128x1024xi32>) -> tensor<128x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x1024xi32>) -> tensor<128x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<128x1024xi32>) -> tensor<128x1024xi32>\n  return %ret : tensor<128x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x1024xi32>) -> tensor<128x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024xi32>\n    memref.copy %2, %alloc : memref<128x1024xi32> to memref<128x1024xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024xi32>\n    return %3 : tensor<128x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x1024xi32>) -> tensor<128x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<128x1024xi32>) -> tensor<128x1024xi32>\n  return %ret : tensor<128x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<1x1xi32>, tensor<128x1024xi32>) -> (tensor<128x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_66": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>_67": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x30xi32>) -> tensor<510x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>\n  return %ret : tensor<510x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x30xi32>) -> tensor<510x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x30xi32>\n    memref.copy %2, %alloc : memref<510x30xi32> to memref<510x30xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x30xi32>\n    return %3 : tensor<510x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x30xi32>) -> tensor<510x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>\n  return %ret : tensor<510x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<3x3xi32>, tensor<510x30xi32>) -> (tensor<510x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>_68": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x128xi32>) -> tensor<512x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128xi32>\n    memref.copy %2, %alloc : memref<512x128xi32> to memref<512x128xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128xi32>\n    return %3 : tensor<512x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<1x1xi32>, tensor<512x128xi32>) -> (tensor<512x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>_69": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x250xi32>) -> tensor<506x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x250xi32>\n    memref.copy %2, %alloc : memref<506x250xi32> to memref<506x250xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x250xi32>\n    return %3 : tensor<506x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<7x7xi32>, tensor<506x250xi32>) -> (tensor<506x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>_70": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x1020xi32>\n    memref.copy %2, %alloc : memref<60x1020xi32> to memref<60x1020xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x1020xi32>\n    return %3 : tensor<60x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<5x5xi32>, tensor<60x1020xi32>) -> (tensor<60x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>_71": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x254xi32>) -> tensor<126x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254xi32>\n    memref.copy %2, %alloc : memref<126x254xi32> to memref<126x254xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254xi32>\n    return %3 : tensor<126x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<3x3xi32>, tensor<126x254xi32>) -> (tensor<126x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>_72": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024xi32>\n    memref.copy %2, %alloc : memref<256x1024xi32> to memref<256x1024xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024xi32>\n    return %3 : tensor<256x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<1x1xi32>, tensor<256x1024xi32>) -> (tensor<256x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>_73": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62xi32>\n    memref.copy %2, %alloc : memref<1022x62xi32> to memref<1022x62xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62xi32>\n    return %3 : tensor<1022x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<3x3xi32>, tensor<1022x62xi32>) -> (tensor<1022x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>_74": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x126xi32>) -> tensor<30x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126xi32>\n    memref.copy %2, %alloc : memref<30x126xi32> to memref<30x126xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126xi32>\n    return %3 : tensor<30x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<3x3xi32>, tensor<30x126xi32>) -> (tensor<30x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>_75": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x28xi32>) -> tensor<124x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28xi32>\n    memref.copy %2, %alloc : memref<124x28xi32> to memref<124x28xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28xi32>\n    return %3 : tensor<124x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<5x5xi32>, tensor<124x28xi32>) -> (tensor<124x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>_76": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x122xi32>) -> tensor<58x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122xi32>\n    memref.copy %2, %alloc : memref<58x122xi32> to memref<58x122xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122xi32>\n    return %3 : tensor<58x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<7x7xi32>, tensor<58x122xi32>) -> (tensor<58x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_77": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>_78": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x256xi32>) -> tensor<512x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>\n  return %ret : tensor<512x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x256xi32>) -> tensor<512x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256xi32>\n    memref.copy %2, %alloc : memref<512x256xi32> to memref<512x256xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256xi32>\n    return %3 : tensor<512x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x256xi32>) -> tensor<512x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<1x1xi32>) outs(%output: tensor<512x256xi32>) -> tensor<512x256xi32>\n  return %ret : tensor<512x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<1x1xi32>, tensor<512x256xi32>) -> (tensor<512x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>_79": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x128xi32>) -> tensor<32x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128xi32>\n    memref.copy %2, %alloc : memref<32x128xi32> to memref<32x128xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128xi32>\n    return %3 : tensor<32x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<1x1xi32>, tensor<32x128xi32>) -> (tensor<32x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>_80": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x126xi32>) -> tensor<62x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126xi32>\n    memref.copy %2, %alloc : memref<62x126xi32> to memref<62x126xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126xi32>\n    return %3 : tensor<62x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<3x3xi32>, tensor<62x126xi32>) -> (tensor<62x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>_81": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x124xi32>) -> tensor<508x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124xi32>\n    memref.copy %2, %alloc : memref<508x124xi32> to memref<508x124xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124xi32>\n    return %3 : tensor<508x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<5x5xi32>, tensor<508x124xi32>) -> (tensor<508x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>_82": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30xi32>\n    memref.copy %2, %alloc : memref<1022x30xi32> to memref<1022x30xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30xi32>\n    return %3 : tensor<1022x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<3x3xi32>, tensor<1022x30xi32>) -> (tensor<1022x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>_83": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x62xi32>) -> tensor<62x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62xi32>\n    memref.copy %2, %alloc : memref<62x62xi32> to memref<62x62xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62xi32>\n    return %3 : tensor<62x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<3x3xi32>, tensor<62x62xi32>) -> (tensor<62x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>_84": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x122xi32>) -> tensor<506x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>\n  return %ret : tensor<506x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x122xi32>) -> tensor<506x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x122xi32>\n    memref.copy %2, %alloc : memref<506x122xi32> to memref<506x122xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x122xi32>\n    return %3 : tensor<506x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x122xi32>) -> tensor<506x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>\n  return %ret : tensor<506x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<7x7xi32>, tensor<506x122xi32>) -> (tensor<506x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>_85": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x508xi32>) -> tensor<124x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508xi32>\n    memref.copy %2, %alloc : memref<124x508xi32> to memref<124x508xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508xi32>\n    return %3 : tensor<124x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<5x5xi32>, tensor<124x508xi32>) -> (tensor<124x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>_86": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x28xi32>) -> tensor<124x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28xi32>\n    memref.copy %2, %alloc : memref<124x28xi32> to memref<124x28xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28xi32>\n    return %3 : tensor<124x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<5x5xi32>, tensor<124x28xi32>) -> (tensor<124x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>_87": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x512xi32>) -> tensor<64x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512xi32>\n    memref.copy %2, %alloc : memref<64x512xi32> to memref<64x512xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512xi32>\n    return %3 : tensor<64x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<1x1xi32>, tensor<64x512xi32>) -> (tensor<64x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>_88": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x252xi32>) -> tensor<28x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252xi32>\n    memref.copy %2, %alloc : memref<28x252xi32> to memref<28x252xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252xi32>\n    return %3 : tensor<28x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<5x5xi32>, tensor<28x252xi32>) -> (tensor<28x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_89": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>_90": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024xi32>\n    memref.copy %2, %alloc : memref<512x1024xi32> to memref<512x1024xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024xi32>\n    return %3 : tensor<512x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<1x1xi32>, tensor<512x1024xi32>) -> (tensor<512x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>_91": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x254xi32>) -> tensor<30x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254xi32>\n    memref.copy %2, %alloc : memref<30x254xi32> to memref<30x254xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254xi32>\n    return %3 : tensor<30x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<3x3xi32>, tensor<30x254xi32>) -> (tensor<30x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>_92": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x28xi32>) -> tensor<60x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28xi32>\n    memref.copy %2, %alloc : memref<60x28xi32> to memref<60x28xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28xi32>\n    return %3 : tensor<60x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<5x5xi32>, tensor<60x28xi32>) -> (tensor<60x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>_93": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x26xi32>) -> tensor<250x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26xi32>\n    memref.copy %2, %alloc : memref<250x26xi32> to memref<250x26xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26xi32>\n    return %3 : tensor<250x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<7x7xi32>, tensor<250x26xi32>) -> (tensor<250x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>_94": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x508xi32>) -> tensor<124x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508xi32>\n    memref.copy %2, %alloc : memref<124x508xi32> to memref<124x508xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508xi32>\n    return %3 : tensor<124x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<5x5xi32>, tensor<124x508xi32>) -> (tensor<124x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>_95": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x60xi32>) -> tensor<28x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60xi32>\n    memref.copy %2, %alloc : memref<28x60xi32> to memref<28x60xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60xi32>\n    return %3 : tensor<28x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<5x5xi32>, tensor<28x60xi32>) -> (tensor<28x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>_96": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018xi32>\n    memref.copy %2, %alloc : memref<250x1018xi32> to memref<250x1018xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018xi32>\n    return %3 : tensor<250x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<7x7xi32>, tensor<250x1018xi32>) -> (tensor<250x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>_97": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250xi32>\n    memref.copy %2, %alloc : memref<1018x250xi32> to memref<1018x250xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250xi32>\n    return %3 : tensor<1018x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<7x7xi32>, tensor<1018x250xi32>) -> (tensor<1018x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>_98": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x254xi32>) -> tensor<126x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254xi32>\n    memref.copy %2, %alloc : memref<126x254xi32> to memref<126x254xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254xi32>\n    return %3 : tensor<126x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<3x3xi32>, tensor<126x254xi32>) -> (tensor<126x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>_99": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x26xi32>) -> tensor<122x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26xi32>\n    memref.copy %2, %alloc : memref<122x26xi32> to memref<122x26xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26xi32>\n    return %3 : tensor<122x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<7x7xi32>, tensor<122x26xi32>) -> (tensor<122x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>_100": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018xi32>\n    memref.copy %2, %alloc : memref<58x1018xi32> to memref<58x1018xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018xi32>\n    return %3 : tensor<58x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<7x7xi32>, tensor<58x1018xi32>) -> (tensor<58x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_101": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>_102": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x128xi32>) -> tensor<32x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128xi32>\n    memref.copy %2, %alloc : memref<32x128xi32> to memref<32x128xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128xi32>\n    return %3 : tensor<32x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<1x1xi32>, tensor<32x128xi32>) -> (tensor<32x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>_103": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x510xi32>) -> tensor<30x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x510xi32>\n    memref.copy %2, %alloc : memref<30x510xi32> to memref<30x510xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x510xi32>\n    return %3 : tensor<30x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<3x3xi32>, tensor<30x510xi32>) -> (tensor<30x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>_104": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x128xi32>) -> tensor<256x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xi32>\n    memref.copy %2, %alloc : memref<256x128xi32> to memref<256x128xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128xi32>\n    return %3 : tensor<256x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<1x1xi32>, tensor<256x128xi32>) -> (tensor<256x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_105": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>_106": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x252xi32>) -> tensor<124x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252xi32>\n    memref.copy %2, %alloc : memref<124x252xi32> to memref<124x252xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252xi32>\n    return %3 : tensor<124x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<5x5xi32>, tensor<124x252xi32>) -> (tensor<124x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>_107": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x126xi32>) -> tensor<254x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126xi32>\n    memref.copy %2, %alloc : memref<254x126xi32> to memref<254x126xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126xi32>\n    return %3 : tensor<254x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<3x3xi32>, tensor<254x126xi32>) -> (tensor<254x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>_108": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x252xi32>) -> tensor<252x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x252xi32>\n    memref.copy %2, %alloc : memref<252x252xi32> to memref<252x252xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x252xi32>\n    return %3 : tensor<252x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<5x5xi32>, tensor<252x252xi32>) -> (tensor<252x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>_109": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x512xi32>) -> tensor<64x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512xi32>\n    memref.copy %2, %alloc : memref<64x512xi32> to memref<64x512xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512xi32>\n    return %3 : tensor<64x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<1x1xi32>, tensor<64x512xi32>) -> (tensor<64x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>_110": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x256xi32>) -> tensor<64x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256xi32>\n    memref.copy %2, %alloc : memref<64x256xi32> to memref<64x256xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256xi32>\n    return %3 : tensor<64x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<1x1xi32>, tensor<64x256xi32>) -> (tensor<64x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>_111": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x510xi32>) -> tensor<62x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510xi32>\n    memref.copy %2, %alloc : memref<62x510xi32> to memref<62x510xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510xi32>\n    return %3 : tensor<62x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<3x3xi32>, tensor<62x510xi32>) -> (tensor<62x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_112": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>_113": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508xi32>\n    memref.copy %2, %alloc : memref<1020x508xi32> to memref<1020x508xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508xi32>\n    return %3 : tensor<1020x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<5x5xi32>, tensor<1020x508xi32>) -> (tensor<1020x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>_114": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018xi32>\n    memref.copy %2, %alloc : memref<506x1018xi32> to memref<506x1018xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018xi32>\n    return %3 : tensor<506x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<7x7xi32>, tensor<506x1018xi32>) -> (tensor<506x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>_115": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x256xi32>) -> tensor<256x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256xi32>\n    memref.copy %2, %alloc : memref<256x256xi32> to memref<256x256xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256xi32>\n    return %3 : tensor<256x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<1x1xi32>, tensor<256x256xi32>) -> (tensor<256x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>_116": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x124xi32>) -> tensor<60x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124xi32>\n    memref.copy %2, %alloc : memref<60x124xi32> to memref<60x124xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124xi32>\n    return %3 : tensor<60x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<5x5xi32>, tensor<60x124xi32>) -> (tensor<60x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>_117": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x28xi32>) -> tensor<508x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28xi32>\n    memref.copy %2, %alloc : memref<508x28xi32> to memref<508x28xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28xi32>\n    return %3 : tensor<508x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<5x5xi32>, tensor<508x28xi32>) -> (tensor<508x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>_118": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x508xi32>) -> tensor<60x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508xi32>\n    memref.copy %2, %alloc : memref<60x508xi32> to memref<60x508xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508xi32>\n    return %3 : tensor<60x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<5x5xi32>, tensor<60x508xi32>) -> (tensor<60x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_119": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>_120": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>\n  return %ret : tensor<1020x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x252xi32>\n    memref.copy %2, %alloc : memref<1020x252xi32> to memref<1020x252xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x252xi32>\n    return %3 : tensor<1020x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>\n  return %ret : tensor<1020x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<5x5xi32>, tensor<1020x252xi32>) -> (tensor<1020x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>_121": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x126xi32>) -> tensor<126x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126xi32>\n    memref.copy %2, %alloc : memref<126x126xi32> to memref<126x126xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126xi32>\n    return %3 : tensor<126x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<3x3xi32>, tensor<126x126xi32>) -> (tensor<126x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>_122": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x30xi32>) -> tensor<30x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>\n  return %ret : tensor<30x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x30xi32>) -> tensor<30x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30xi32>\n    memref.copy %2, %alloc : memref<30x30xi32> to memref<30x30xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30xi32>\n    return %3 : tensor<30x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x30xi32>) -> tensor<30x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>\n  return %ret : tensor<30x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<3x3xi32>, tensor<30x30xi32>) -> (tensor<30x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>_123": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126xi32>\n    memref.copy %2, %alloc : memref<1022x126xi32> to memref<1022x126xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126xi32>\n    return %3 : tensor<1022x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<3x3xi32>, tensor<1022x126xi32>) -> (tensor<1022x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>_124": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126xi32>\n    memref.copy %2, %alloc : memref<1022x126xi32> to memref<1022x126xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126xi32>\n    return %3 : tensor<1022x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<3x3xi32>, tensor<1022x126xi32>) -> (tensor<1022x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>_125": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x124xi32>\n    memref.copy %2, %alloc : memref<1020x124xi32> to memref<1020x124xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x124xi32>\n    return %3 : tensor<1020x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<5x5xi32>, tensor<1020x124xi32>) -> (tensor<1020x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>_126": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x256xi32>) -> tensor<256x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256xi32>\n    memref.copy %2, %alloc : memref<256x256xi32> to memref<256x256xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256xi32>\n    return %3 : tensor<256x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<1x1xi32>, tensor<256x256xi32>) -> (tensor<256x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_127": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>_128": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510xi32>\n    memref.copy %2, %alloc : memref<1022x510xi32> to memref<1022x510xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510xi32>\n    return %3 : tensor<1022x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<3x3xi32>, tensor<1022x510xi32>) -> (tensor<1022x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>_129": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x128xi32>) -> tensor<128x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128xi32>\n    memref.copy %2, %alloc : memref<128x128xi32> to memref<128x128xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128xi32>\n    return %3 : tensor<128x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<1x1xi32>, tensor<128x128xi32>) -> (tensor<128x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>_130": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x64xi32>) -> tensor<64x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64xi32>\n    memref.copy %2, %alloc : memref<64x64xi32> to memref<64x64xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64xi32>\n    return %3 : tensor<64x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<1x1xi32>, tensor<64x64xi32>) -> (tensor<64x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>_131": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x30xi32>) -> tensor<62x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30xi32>\n    memref.copy %2, %alloc : memref<62x30xi32> to memref<62x30xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30xi32>\n    return %3 : tensor<62x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<3x3xi32>, tensor<62x30xi32>) -> (tensor<62x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>_132": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>\n  return %ret : tensor<1020x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28xi32>\n    memref.copy %2, %alloc : memref<1020x28xi32> to memref<1020x28xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28xi32>\n    return %3 : tensor<1020x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>\n  return %ret : tensor<1020x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<5x5xi32>, tensor<1020x28xi32>) -> (tensor<1020x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>_133": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x512xi32>) -> tensor<128x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xi32>\n    memref.copy %2, %alloc : memref<128x512xi32> to memref<128x512xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512xi32>\n    return %3 : tensor<128x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<1x1xi32>, tensor<128x512xi32>) -> (tensor<128x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>_134": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x508xi32>) -> tensor<252x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>\n  return %ret : tensor<252x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x508xi32>) -> tensor<252x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x508xi32>\n    memref.copy %2, %alloc : memref<252x508xi32> to memref<252x508xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x508xi32>\n    return %3 : tensor<252x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x508xi32>) -> tensor<252x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>\n  return %ret : tensor<252x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<5x5xi32>, tensor<252x508xi32>) -> (tensor<252x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>_135": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x26xi32>) -> tensor<250x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26xi32>\n    memref.copy %2, %alloc : memref<250x26xi32> to memref<250x26xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26xi32>\n    return %3 : tensor<250x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<7x7xi32>, tensor<250x26xi32>) -> (tensor<250x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>_136": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020xi32>\n    memref.copy %2, %alloc : memref<508x1020xi32> to memref<508x1020xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020xi32>\n    return %3 : tensor<508x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<5x5xi32>, tensor<508x1020xi32>) -> (tensor<508x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>_137": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x28xi32>) -> tensor<508x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28xi32>\n    memref.copy %2, %alloc : memref<508x28xi32> to memref<508x28xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28xi32>\n    return %3 : tensor<508x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<5x5xi32>, tensor<508x28xi32>) -> (tensor<508x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>_138": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x1022xi32>\n    memref.copy %2, %alloc : memref<1022x1022xi32> to memref<1022x1022xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x1022xi32>\n    return %3 : tensor<1022x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x1022xi32>) -> tensor<1022x1022xi32>\n  return %ret : tensor<1022x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<3x3xi32>, tensor<1022x1022xi32>) -> (tensor<1022x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>_139": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x508xi32>) -> tensor<508x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508xi32>\n    memref.copy %2, %alloc : memref<508x508xi32> to memref<508x508xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508xi32>\n    return %3 : tensor<508x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<5x5xi32>, tensor<508x508xi32>) -> (tensor<508x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>_140": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x64xi32>) -> tensor<64x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64xi32>\n    memref.copy %2, %alloc : memref<64x64xi32> to memref<64x64xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64xi32>\n    return %3 : tensor<64x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<1x1xi32>, tensor<64x64xi32>) -> (tensor<64x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_141": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>_142": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506xi32>\n    memref.copy %2, %alloc : memref<1018x506xi32> to memref<1018x506xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506xi32>\n    return %3 : tensor<1018x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<7x7xi32>, tensor<1018x506xi32>) -> (tensor<1018x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>_143": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022xi32>\n    memref.copy %2, %alloc : memref<254x1022xi32> to memref<254x1022xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022xi32>\n    return %3 : tensor<254x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<3x3xi32>, tensor<254x1022xi32>) -> (tensor<254x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>_144": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x62xi32>) -> tensor<62x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62xi32>\n    memref.copy %2, %alloc : memref<62x62xi32> to memref<62x62xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62xi32>\n    return %3 : tensor<62x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<3x3xi32>, tensor<62x62xi32>) -> (tensor<62x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_145": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>_146": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x124xi32>) -> tensor<60x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124xi32>\n    memref.copy %2, %alloc : memref<60x124xi32> to memref<60x124xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124xi32>\n    return %3 : tensor<60x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<5x5xi32>, tensor<60x124xi32>) -> (tensor<60x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>_147": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x32xi32>) -> tensor<64x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32xi32>\n    memref.copy %2, %alloc : memref<64x32xi32> to memref<64x32xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32xi32>\n    return %3 : tensor<64x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<1x1xi32>, tensor<64x32xi32>) -> (tensor<64x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_148": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>_149": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x32xi32>) -> tensor<256x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32xi32>\n    memref.copy %2, %alloc : memref<256x32xi32> to memref<256x32xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32xi32>\n    return %3 : tensor<256x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<1x1xi32>, tensor<256x32xi32>) -> (tensor<256x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_150": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>_151": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x32xi32>) -> tensor<64x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32xi32>\n    memref.copy %2, %alloc : memref<64x32xi32> to memref<64x32xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32xi32>\n    return %3 : tensor<64x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<1x1xi32>, tensor<64x32xi32>) -> (tensor<64x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_152": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>_153": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024xi32>\n    memref.copy %2, %alloc : memref<64x1024xi32> to memref<64x1024xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x1024xi32>\n    return %3 : tensor<64x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<1x1xi32>, tensor<64x1024xi32>) -> (tensor<64x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_154": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>_155": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508xi32>\n    memref.copy %2, %alloc : memref<1020x508xi32> to memref<1020x508xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508xi32>\n    return %3 : tensor<1020x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<5x5xi32>, tensor<1020x508xi32>) -> (tensor<1020x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>_156": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510xi32>\n    memref.copy %2, %alloc : memref<1022x510xi32> to memref<1022x510xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510xi32>\n    return %3 : tensor<1022x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<3x3xi32>, tensor<1022x510xi32>) -> (tensor<1022x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>_157": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128xi32>\n    memref.copy %2, %alloc : memref<1024x128xi32> to memref<1024x128xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128xi32>\n    return %3 : tensor<1024x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<1x1xi32>, tensor<1024x128xi32>) -> (tensor<1024x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_158": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>_159": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x512xi32>) -> tensor<128x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xi32>\n    memref.copy %2, %alloc : memref<128x512xi32> to memref<128x512xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512xi32>\n    return %3 : tensor<128x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<1x1xi32>, tensor<128x512xi32>) -> (tensor<128x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_160": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_161": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>_162": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x508xi32>) -> tensor<60x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508xi32>\n    memref.copy %2, %alloc : memref<60x508xi32> to memref<60x508xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508xi32>\n    return %3 : tensor<60x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<5x5xi32>, tensor<60x508xi32>) -> (tensor<60x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>_163": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x512xi32>) -> tensor<32x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512xi32>\n    memref.copy %2, %alloc : memref<32x512xi32> to memref<32x512xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x512xi32>\n    return %3 : tensor<32x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<1x1xi32>, tensor<32x512xi32>) -> (tensor<32x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>_164": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018xi32>\n    memref.copy %2, %alloc : memref<250x1018xi32> to memref<250x1018xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018xi32>\n    return %3 : tensor<250x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<7x7xi32>, tensor<250x1018xi32>) -> (tensor<250x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_165": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>_166": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018xi32>\n    memref.copy %2, %alloc : memref<58x1018xi32> to memref<58x1018xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018xi32>\n    return %3 : tensor<58x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<7x7xi32>, tensor<58x1018xi32>) -> (tensor<58x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>_167": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x28xi32>) -> tensor<252x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28xi32>\n    memref.copy %2, %alloc : memref<252x28xi32> to memref<252x28xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28xi32>\n    return %3 : tensor<252x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<5x5xi32>, tensor<252x28xi32>) -> (tensor<252x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_168": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>_169": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x124xi32>\n    memref.copy %2, %alloc : memref<1020x124xi32> to memref<1020x124xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x124xi32>\n    return %3 : tensor<1020x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<5x5xi32>, tensor<1020x124xi32>) -> (tensor<1020x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>_170": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x128xi32>) -> tensor<512x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128xi32>\n    memref.copy %2, %alloc : memref<512x128xi32> to memref<512x128xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128xi32>\n    return %3 : tensor<512x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<1x1xi32>, tensor<512x128xi32>) -> (tensor<512x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>_171": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x32xi32>) -> tensor<256x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32xi32>\n    memref.copy %2, %alloc : memref<256x32xi32> to memref<256x32xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32xi32>\n    return %3 : tensor<256x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<1x1xi32>, tensor<256x32xi32>) -> (tensor<256x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>_172": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122xi32>\n    memref.copy %2, %alloc : memref<1018x122xi32> to memref<1018x122xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122xi32>\n    return %3 : tensor<1018x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<7x7xi32>, tensor<1018x122xi32>) -> (tensor<1018x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>_173": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x30xi32>) -> tensor<62x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30xi32>\n    memref.copy %2, %alloc : memref<62x30xi32> to memref<62x30xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30xi32>\n    return %3 : tensor<62x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<3x3xi32>, tensor<62x30xi32>) -> (tensor<62x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_174": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>_175": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58xi32>\n    memref.copy %2, %alloc : memref<1018x58xi32> to memref<1018x58xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58xi32>\n    return %3 : tensor<1018x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<7x7xi32>, tensor<1018x58xi32>) -> (tensor<1018x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>_176": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x60xi32>) -> tensor<28x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60xi32>\n    memref.copy %2, %alloc : memref<28x60xi32> to memref<28x60xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60xi32>\n    return %3 : tensor<28x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<5x5xi32>, tensor<28x60xi32>) -> (tensor<28x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>_177": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x250xi32>) -> tensor<122x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250xi32>\n    memref.copy %2, %alloc : memref<122x250xi32> to memref<122x250xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250xi32>\n    return %3 : tensor<122x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<7x7xi32>, tensor<122x250xi32>) -> (tensor<122x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>_178": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x252xi32>) -> tensor<60x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x252xi32>\n    memref.copy %2, %alloc : memref<60x252xi32> to memref<60x252xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x252xi32>\n    return %3 : tensor<60x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<5x5xi32>, tensor<60x252xi32>) -> (tensor<60x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>_179": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x30xi32>) -> tensor<126x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30xi32>\n    memref.copy %2, %alloc : memref<126x30xi32> to memref<126x30xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30xi32>\n    return %3 : tensor<126x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<3x3xi32>, tensor<126x30xi32>) -> (tensor<126x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_180": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>_181": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x124xi32>) -> tensor<508x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124xi32>\n    memref.copy %2, %alloc : memref<508x124xi32> to memref<508x124xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124xi32>\n    return %3 : tensor<508x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<5x5xi32>, tensor<508x124xi32>) -> (tensor<508x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>_182": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>\n  return %ret : tensor<1024x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x32xi32>\n    memref.copy %2, %alloc : memref<1024x32xi32> to memref<1024x32xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x32xi32>\n    return %3 : tensor<1024x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>\n  return %ret : tensor<1024x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<1x1xi32>, tensor<1024x32xi32>) -> (tensor<1024x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_183": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_184": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_185": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>_186": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024xi32>\n    memref.copy %2, %alloc : memref<512x1024xi32> to memref<512x1024xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024xi32>\n    return %3 : tensor<512x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<1x1xi32>, tensor<512x1024xi32>) -> (tensor<512x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>_187": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x122xi32>) -> tensor<250x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>\n  return %ret : tensor<250x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x122xi32>) -> tensor<250x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122xi32>\n    memref.copy %2, %alloc : memref<250x122xi32> to memref<250x122xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122xi32>\n    return %3 : tensor<250x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x122xi32>) -> tensor<250x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<7x7xi32>) outs(%output: tensor<250x122xi32>) -> tensor<250x122xi32>\n  return %ret : tensor<250x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<7x7xi32>, tensor<250x122xi32>) -> (tensor<250x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_188": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>_189": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62xi32>\n    memref.copy %2, %alloc : memref<1022x62xi32> to memref<1022x62xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62xi32>\n    return %3 : tensor<1022x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<3x3xi32>, tensor<1022x62xi32>) -> (tensor<1022x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>_190": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x122xi32>) -> tensor<122x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122xi32>\n    memref.copy %2, %alloc : memref<122x122xi32> to memref<122x122xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122xi32>\n    return %3 : tensor<122x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<7x7xi32>, tensor<122x122xi32>) -> (tensor<122x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>_191": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x26xi32>) -> tensor<250x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26xi32>\n    memref.copy %2, %alloc : memref<250x26xi32> to memref<250x26xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26xi32>\n    return %3 : tensor<250x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<7x7xi32>, tensor<250x26xi32>) -> (tensor<250x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>_192": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x64xi32>) -> tensor<512x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>\n  return %ret : tensor<512x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x64xi32>) -> tensor<512x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x64xi32>\n    memref.copy %2, %alloc : memref<512x64xi32> to memref<512x64xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x64xi32>\n    return %3 : tensor<512x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x64xi32>) -> tensor<512x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>\n  return %ret : tensor<512x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<1x1xi32>, tensor<512x64xi32>) -> (tensor<512x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_193": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_194": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>_195": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x26xi32>) -> tensor<122x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26xi32>\n    memref.copy %2, %alloc : memref<122x26xi32> to memref<122x26xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26xi32>\n    return %3 : tensor<122x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<7x7xi32>, tensor<122x26xi32>) -> (tensor<122x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>_196": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x28xi32>) -> tensor<252x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28xi32>\n    memref.copy %2, %alloc : memref<252x28xi32> to memref<252x28xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28xi32>\n    return %3 : tensor<252x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<5x5xi32>, tensor<252x28xi32>) -> (tensor<252x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>_197": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024xi32>\n    memref.copy %2, %alloc : memref<512x1024xi32> to memref<512x1024xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024xi32>\n    return %3 : tensor<512x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<1x1xi32>, tensor<512x1024xi32>) -> (tensor<512x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>_198": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x250xi32>) -> tensor<250x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250xi32>\n    memref.copy %2, %alloc : memref<250x250xi32> to memref<250x250xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250xi32>\n    return %3 : tensor<250x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<7x7xi32>, tensor<250x250xi32>) -> (tensor<250x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>_199": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x512xi32>) -> tensor<128x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xi32>\n    memref.copy %2, %alloc : memref<128x512xi32> to memref<128x512xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512xi32>\n    return %3 : tensor<128x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<1x1xi32>, tensor<128x512xi32>) -> (tensor<128x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>_200": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x28xi32>) -> tensor<124x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28xi32>\n    memref.copy %2, %alloc : memref<124x28xi32> to memref<124x28xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28xi32>\n    return %3 : tensor<124x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<5x5xi32>, tensor<124x28xi32>) -> (tensor<124x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>_201": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30xi32>\n    memref.copy %2, %alloc : memref<1022x30xi32> to memref<1022x30xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30xi32>\n    return %3 : tensor<1022x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<3x3xi32>, tensor<1022x30xi32>) -> (tensor<1022x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_202": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>_203": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022xi32>\n    memref.copy %2, %alloc : memref<254x1022xi32> to memref<254x1022xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022xi32>\n    return %3 : tensor<254x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<3x3xi32>, tensor<254x1022xi32>) -> (tensor<254x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>_204": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510xi32>\n    memref.copy %2, %alloc : memref<1022x510xi32> to memref<1022x510xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510xi32>\n    return %3 : tensor<1022x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<3x3xi32>, tensor<1022x510xi32>) -> (tensor<1022x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>_205": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x124xi32>) -> tensor<508x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124xi32>\n    memref.copy %2, %alloc : memref<508x124xi32> to memref<508x124xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124xi32>\n    return %3 : tensor<508x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<5x5xi32>, tensor<508x124xi32>) -> (tensor<508x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>_206": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x124xi32>) -> tensor<508x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124xi32>\n    memref.copy %2, %alloc : memref<508x124xi32> to memref<508x124xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124xi32>\n    return %3 : tensor<508x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x124xi32>) -> tensor<508x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<5x5xi32>) outs(%output: tensor<508x124xi32>) -> tensor<508x124xi32>\n  return %ret : tensor<508x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<5x5xi32>, tensor<508x124xi32>) -> (tensor<508x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_207": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>_208": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018xi32>\n    memref.copy %2, %alloc : memref<250x1018xi32> to memref<250x1018xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018xi32>\n    return %3 : tensor<250x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<7x7xi32>, tensor<250x1018xi32>) -> (tensor<250x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_209": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>_210": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122xi32>\n    memref.copy %2, %alloc : memref<1018x122xi32> to memref<1018x122xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122xi32>\n    return %3 : tensor<1018x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<7x7xi32>, tensor<1018x122xi32>) -> (tensor<1018x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>_211": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250xi32>\n    memref.copy %2, %alloc : memref<1018x250xi32> to memref<1018x250xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250xi32>\n    return %3 : tensor<1018x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<7x7xi32>, tensor<1018x250xi32>) -> (tensor<1018x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_212": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>_213": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122xi32>\n    memref.copy %2, %alloc : memref<1018x122xi32> to memref<1018x122xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122xi32>\n    return %3 : tensor<1018x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x122xi32>) -> tensor<1018x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x122xi32>) -> tensor<1018x122xi32>\n  return %ret : tensor<1018x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<7x7xi32>, tensor<1018x122xi32>) -> (tensor<1018x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>_214": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x512xi32>) -> tensor<128x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512xi32>\n    memref.copy %2, %alloc : memref<128x512xi32> to memref<128x512xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512xi32>\n    return %3 : tensor<128x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x512xi32>) -> tensor<128x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<1x1xi32>) outs(%output: tensor<128x512xi32>) -> tensor<128x512xi32>\n  return %ret : tensor<128x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<1x1xi32>, tensor<128x512xi32>) -> (tensor<128x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>_215": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x254xi32>) -> tensor<254x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>\n  return %ret : tensor<254x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x254xi32>) -> tensor<254x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254xi32>\n    memref.copy %2, %alloc : memref<254x254xi32> to memref<254x254xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254xi32>\n    return %3 : tensor<254x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x254xi32>) -> tensor<254x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>\n  return %ret : tensor<254x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<3x3xi32>, tensor<254x254xi32>) -> (tensor<254x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>_216": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x60xi32>) -> tensor<508x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60xi32>\n    memref.copy %2, %alloc : memref<508x60xi32> to memref<508x60xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60xi32>\n    return %3 : tensor<508x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<5x5xi32>, tensor<508x60xi32>) -> (tensor<508x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>_217": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>\n  return %ret : tensor<1018x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018xi32>\n    memref.copy %2, %alloc : memref<1018x1018xi32> to memref<1018x1018xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018xi32>\n    return %3 : tensor<1018x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x1018xi32>) -> tensor<1018x1018xi32>\n  return %ret : tensor<1018x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<7x7xi32>, tensor<1018x1018xi32>) -> (tensor<1018x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_218": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>_219": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x60xi32>) -> tensor<124x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60xi32>\n    memref.copy %2, %alloc : memref<124x60xi32> to memref<124x60xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60xi32>\n    return %3 : tensor<124x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<5x5xi32>, tensor<124x60xi32>) -> (tensor<124x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>_220": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>\n  return %ret : tensor<32x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024xi32>\n    memref.copy %2, %alloc : memref<32x1024xi32> to memref<32x1024xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024xi32>\n    return %3 : tensor<32x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x1024xi32>) -> tensor<32x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<32x1024xi32>) -> tensor<32x1024xi32>\n  return %ret : tensor<32x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<1x1xi32>, tensor<32x1024xi32>) -> (tensor<32x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>_221": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x254xi32>) -> tensor<510x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>\n  return %ret : tensor<510x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x254xi32>) -> tensor<510x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254xi32>\n    memref.copy %2, %alloc : memref<510x254xi32> to memref<510x254xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254xi32>\n    return %3 : tensor<510x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x254xi32>) -> tensor<510x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>\n  return %ret : tensor<510x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<3x3xi32>, tensor<510x254xi32>) -> (tensor<510x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>_222": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x1020xi32>\n    memref.copy %2, %alloc : memref<60x1020xi32> to memref<60x1020xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x1020xi32>\n    return %3 : tensor<60x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<5x5xi32>, tensor<60x1020xi32>) -> (tensor<60x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>_223": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x510xi32>) -> tensor<62x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510xi32>\n    memref.copy %2, %alloc : memref<62x510xi32> to memref<62x510xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510xi32>\n    return %3 : tensor<62x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<3x3xi32>, tensor<62x510xi32>) -> (tensor<62x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>_224": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30xi32>\n    memref.copy %2, %alloc : memref<1022x30xi32> to memref<1022x30xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30xi32>\n    return %3 : tensor<1022x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<3x3xi32>, tensor<1022x30xi32>) -> (tensor<1022x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>_225": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x250xi32>) -> tensor<26x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>\n  return %ret : tensor<26x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x250xi32>) -> tensor<26x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250xi32>\n    memref.copy %2, %alloc : memref<26x250xi32> to memref<26x250xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250xi32>\n    return %3 : tensor<26x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x250xi32>) -> tensor<26x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>\n  return %ret : tensor<26x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<7x7xi32>, tensor<26x250xi32>) -> (tensor<26x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>_226": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x60xi32>) -> tensor<124x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60xi32>\n    memref.copy %2, %alloc : memref<124x60xi32> to memref<124x60xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60xi32>\n    return %3 : tensor<124x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<5x5xi32>, tensor<124x60xi32>) -> (tensor<124x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>_227": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x122xi32>) -> tensor<122x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122xi32>\n    memref.copy %2, %alloc : memref<122x122xi32> to memref<122x122xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122xi32>\n    return %3 : tensor<122x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<7x7xi32>, tensor<122x122xi32>) -> (tensor<122x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>_228": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510xi32>\n    memref.copy %2, %alloc : memref<1022x510xi32> to memref<1022x510xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510xi32>\n    return %3 : tensor<1022x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<3x3xi32>, tensor<1022x510xi32>) -> (tensor<1022x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>_229": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x32xi32>) -> tensor<64x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32xi32>\n    memref.copy %2, %alloc : memref<64x32xi32> to memref<64x32xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32xi32>\n    return %3 : tensor<64x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<1x1xi32>, tensor<64x32xi32>) -> (tensor<64x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_230": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>_231": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x510xi32>) -> tensor<30x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x510xi32>\n    memref.copy %2, %alloc : memref<30x510xi32> to memref<30x510xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x510xi32>\n    return %3 : tensor<30x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<3x3xi32>, tensor<30x510xi32>) -> (tensor<30x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_232": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>_233": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x252xi32>) -> tensor<124x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252xi32>\n    memref.copy %2, %alloc : memref<124x252xi32> to memref<124x252xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252xi32>\n    return %3 : tensor<124x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<5x5xi32>, tensor<124x252xi32>) -> (tensor<124x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>_234": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x60xi32>) -> tensor<508x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60xi32>\n    memref.copy %2, %alloc : memref<508x60xi32> to memref<508x60xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60xi32>\n    return %3 : tensor<508x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<5x5xi32>, tensor<508x60xi32>) -> (tensor<508x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>_235": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024xi32>\n    memref.copy %2, %alloc : memref<64x1024xi32> to memref<64x1024xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x1024xi32>\n    return %3 : tensor<64x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<1x1xi32>, tensor<64x1024xi32>) -> (tensor<64x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_236": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>_237": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x128xi32>) -> tensor<64x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128xi32>\n    memref.copy %2, %alloc : memref<64x128xi32> to memref<64x128xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128xi32>\n    return %3 : tensor<64x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<1x1xi32>, tensor<64x128xi32>) -> (tensor<64x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>_238": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x252xi32>) -> tensor<252x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x252xi32>\n    memref.copy %2, %alloc : memref<252x252xi32> to memref<252x252xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x252xi32>\n    return %3 : tensor<252x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<5x5xi32>, tensor<252x252xi32>) -> (tensor<252x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>_239": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x30xi32>) -> tensor<126x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30xi32>\n    memref.copy %2, %alloc : memref<126x30xi32> to memref<126x30xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30xi32>\n    return %3 : tensor<126x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<3x3xi32>, tensor<126x30xi32>) -> (tensor<126x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>_240": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x60xi32>) -> tensor<508x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60xi32>\n    memref.copy %2, %alloc : memref<508x60xi32> to memref<508x60xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60xi32>\n    return %3 : tensor<508x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<5x5xi32>, tensor<508x60xi32>) -> (tensor<508x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>_241": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>\n  return %ret : tensor<122x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018xi32>\n    memref.copy %2, %alloc : memref<122x1018xi32> to memref<122x1018xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018xi32>\n    return %3 : tensor<122x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>\n  return %ret : tensor<122x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<7x7xi32>, tensor<122x1018xi32>) -> (tensor<122x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>_242": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020xi32>\n    memref.copy %2, %alloc : memref<508x1020xi32> to memref<508x1020xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020xi32>\n    return %3 : tensor<508x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<5x5xi32>, tensor<508x1020xi32>) -> (tensor<508x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>_243": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x28xi32>) -> tensor<60x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28xi32>\n    memref.copy %2, %alloc : memref<60x28xi32> to memref<60x28xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28xi32>\n    return %3 : tensor<60x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<5x5xi32>, tensor<60x28xi32>) -> (tensor<60x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<3x3xi32>) outs(%output: tensor<254x30xi32>) -> tensor<254x30xi32>_244": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<3x3xi32>) outs(%output: tensor<254x30xi32>) -> tensor<254x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x30xi32>) -> tensor<254x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<3x3xi32>) outs(%output: tensor<254x30xi32>) -> tensor<254x30xi32>\n  return %ret : tensor<254x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x30xi32>) -> tensor<254x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x30xi32>\n    memref.copy %2, %alloc : memref<254x30xi32> to memref<254x30xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x30xi32>\n    return %3 : tensor<254x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x30xi32>) -> tensor<254x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<3x3xi32>) outs(%output: tensor<254x30xi32>) -> tensor<254x30xi32>\n  return %ret : tensor<254x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<3x3xi32>, tensor<254x30xi32>) -> (tensor<254x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>_245": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x512xi32>) -> tensor<64x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512xi32>\n    memref.copy %2, %alloc : memref<64x512xi32> to memref<64x512xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512xi32>\n    return %3 : tensor<64x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<1x1xi32>, tensor<64x512xi32>) -> (tensor<64x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>_246": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x128xi32>) -> tensor<512x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128xi32>\n    memref.copy %2, %alloc : memref<512x128xi32> to memref<512x128xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128xi32>\n    return %3 : tensor<512x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<1x1xi32>, tensor<512x128xi32>) -> (tensor<512x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>_247": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x122xi32>) -> tensor<58x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122xi32>\n    memref.copy %2, %alloc : memref<58x122xi32> to memref<58x122xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122xi32>\n    return %3 : tensor<58x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<7x7xi32>, tensor<58x122xi32>) -> (tensor<58x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>_248": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x124xi32>\n    memref.copy %2, %alloc : memref<1020x124xi32> to memref<1020x124xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x124xi32>\n    return %3 : tensor<1020x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x124xi32>) -> tensor<1020x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x124xi32>) -> tensor<1020x124xi32>\n  return %ret : tensor<1020x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<5x5xi32>, tensor<1020x124xi32>) -> (tensor<1020x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>_249": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510xi32>\n    memref.copy %2, %alloc : memref<1022x510xi32> to memref<1022x510xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510xi32>\n    return %3 : tensor<1022x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x510xi32>) -> tensor<1022x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x510xi32>) -> tensor<1022x510xi32>\n  return %ret : tensor<1022x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<3x3xi32>, tensor<1022x510xi32>) -> (tensor<1022x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>_250": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x506xi32>) -> tensor<58x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506xi32>\n    memref.copy %2, %alloc : memref<58x506xi32> to memref<58x506xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506xi32>\n    return %3 : tensor<58x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<7x7xi32>, tensor<58x506xi32>) -> (tensor<58x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>_251": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x250xi32>) -> tensor<506x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x250xi32>\n    memref.copy %2, %alloc : memref<506x250xi32> to memref<506x250xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x250xi32>\n    return %3 : tensor<506x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<7x7xi32>, tensor<506x250xi32>) -> (tensor<506x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>_252": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x252xi32>) -> tensor<124x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252xi32>\n    memref.copy %2, %alloc : memref<124x252xi32> to memref<124x252xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252xi32>\n    return %3 : tensor<124x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<5x5xi32>, tensor<124x252xi32>) -> (tensor<124x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>_253": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x508xi32>) -> tensor<28x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508xi32>\n    memref.copy %2, %alloc : memref<28x508xi32> to memref<28x508xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508xi32>\n    return %3 : tensor<28x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<5x5xi32>, tensor<28x508xi32>) -> (tensor<28x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>_254": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018xi32>\n    memref.copy %2, %alloc : memref<506x1018xi32> to memref<506x1018xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018xi32>\n    return %3 : tensor<506x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<7x7xi32>, tensor<506x1018xi32>) -> (tensor<506x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>_255": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>\n  return %ret : tensor<1024x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x32xi32>\n    memref.copy %2, %alloc : memref<1024x32xi32> to memref<1024x32xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x32xi32>\n    return %3 : tensor<1024x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x32xi32>) -> tensor<1024x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x32xi32>) -> tensor<1024x32xi32>\n  return %ret : tensor<1024x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<1x1xi32>, tensor<1024x32xi32>) -> (tensor<1024x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>_256": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x250xi32>) -> tensor<26x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>\n  return %ret : tensor<26x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x250xi32>) -> tensor<26x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250xi32>\n    memref.copy %2, %alloc : memref<26x250xi32> to memref<26x250xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250xi32>\n    return %3 : tensor<26x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x250xi32>) -> tensor<26x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<7x7xi32>) outs(%output: tensor<26x250xi32>) -> tensor<26x250xi32>\n  return %ret : tensor<26x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<7x7xi32>, tensor<26x250xi32>) -> (tensor<26x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_257": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>_258": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x28xi32>) -> tensor<508x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28xi32>\n    memref.copy %2, %alloc : memref<508x28xi32> to memref<508x28xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28xi32>\n    return %3 : tensor<508x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<5x5xi32>, tensor<508x28xi32>) -> (tensor<508x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>_259": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x28xi32>) -> tensor<508x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28xi32>\n    memref.copy %2, %alloc : memref<508x28xi32> to memref<508x28xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28xi32>\n    return %3 : tensor<508x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x28xi32>) -> tensor<508x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<5x5xi32>) outs(%output: tensor<508x28xi32>) -> tensor<508x28xi32>\n  return %ret : tensor<508x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<5x5xi32>, tensor<508x28xi32>) -> (tensor<508x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>_260": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x1022xi32>\n    memref.copy %2, %alloc : memref<62x1022xi32> to memref<62x1022xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x1022xi32>\n    return %3 : tensor<62x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<3x3xi32>, tensor<62x1022xi32>) -> (tensor<62x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>_261": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x60xi32>) -> tensor<508x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60xi32>\n    memref.copy %2, %alloc : memref<508x60xi32> to memref<508x60xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60xi32>\n    return %3 : tensor<508x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<5x5xi32>, tensor<508x60xi32>) -> (tensor<508x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>_262": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x510xi32>) -> tensor<62x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510xi32>\n    memref.copy %2, %alloc : memref<62x510xi32> to memref<62x510xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510xi32>\n    return %3 : tensor<62x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<3x3xi32>, tensor<62x510xi32>) -> (tensor<62x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_263": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>_264": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024xi32>\n    memref.copy %2, %alloc : memref<512x1024xi32> to memref<512x1024xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024xi32>\n    return %3 : tensor<512x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x1024xi32>) -> tensor<512x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<512x1024xi32>) -> tensor<512x1024xi32>\n  return %ret : tensor<512x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<1x1xi32>, tensor<512x1024xi32>) -> (tensor<512x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>_265": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x28xi32>) -> tensor<124x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28xi32>\n    memref.copy %2, %alloc : memref<124x28xi32> to memref<124x28xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28xi32>\n    return %3 : tensor<124x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<5x5xi32>, tensor<124x28xi32>) -> (tensor<124x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>_266": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x508xi32>) -> tensor<508x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508xi32>\n    memref.copy %2, %alloc : memref<508x508xi32> to memref<508x508xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508xi32>\n    return %3 : tensor<508x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<5x5xi32>, tensor<508x508xi32>) -> (tensor<508x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_267": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>_268": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x508xi32>) -> tensor<124x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508xi32>\n    memref.copy %2, %alloc : memref<124x508xi32> to memref<124x508xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508xi32>\n    return %3 : tensor<124x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<5x5xi32>, tensor<124x508xi32>) -> (tensor<124x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>_269": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x252xi32>) -> tensor<60x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x252xi32>\n    memref.copy %2, %alloc : memref<60x252xi32> to memref<60x252xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x252xi32>\n    return %3 : tensor<60x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<5x5xi32>, tensor<60x252xi32>) -> (tensor<60x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>_270": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x250xi32>) -> tensor<506x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x250xi32>\n    memref.copy %2, %alloc : memref<506x250xi32> to memref<506x250xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x250xi32>\n    return %3 : tensor<506x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<7x7xi32>, tensor<506x250xi32>) -> (tensor<506x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>_271": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x508xi32>) -> tensor<60x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508xi32>\n    memref.copy %2, %alloc : memref<60x508xi32> to memref<60x508xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508xi32>\n    return %3 : tensor<60x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x508xi32>) -> tensor<60x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<5x5xi32>) outs(%output: tensor<60x508xi32>) -> tensor<60x508xi32>\n  return %ret : tensor<60x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<5x5xi32>, tensor<60x508xi32>) -> (tensor<60x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>_272": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020xi32>\n    memref.copy %2, %alloc : memref<1020x1020xi32> to memref<1020x1020xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020xi32>\n    return %3 : tensor<1020x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<5x5xi32>, tensor<1020x1020xi32>) -> (tensor<1020x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>_273": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x128xi32>) -> tensor<128x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128xi32>\n    memref.copy %2, %alloc : memref<128x128xi32> to memref<128x128xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128xi32>\n    return %3 : tensor<128x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<1x1xi32>, tensor<128x128xi32>) -> (tensor<128x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>_274": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x122xi32>) -> tensor<506x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>\n  return %ret : tensor<506x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x122xi32>) -> tensor<506x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x122xi32>\n    memref.copy %2, %alloc : memref<506x122xi32> to memref<506x122xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x122xi32>\n    return %3 : tensor<506x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x122xi32>) -> tensor<506x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<7x7xi32>) outs(%output: tensor<506x122xi32>) -> tensor<506x122xi32>\n  return %ret : tensor<506x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<7x7xi32>, tensor<506x122xi32>) -> (tensor<506x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>_275": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x512xi32>) -> tensor<256x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>\n  return %ret : tensor<256x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x512xi32>) -> tensor<256x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512xi32>\n    memref.copy %2, %alloc : memref<256x512xi32> to memref<256x512xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512xi32>\n    return %3 : tensor<256x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x512xi32>) -> tensor<256x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>\n  return %ret : tensor<256x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<1x1xi32>, tensor<256x512xi32>) -> (tensor<256x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>_276": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x126xi32>) -> tensor<30x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126xi32>\n    memref.copy %2, %alloc : memref<30x126xi32> to memref<30x126xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126xi32>\n    return %3 : tensor<30x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<3x3xi32>, tensor<30x126xi32>) -> (tensor<30x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>_277": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018xi32>\n    memref.copy %2, %alloc : memref<506x1018xi32> to memref<506x1018xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018xi32>\n    return %3 : tensor<506x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<7x7xi32>, tensor<506x1018xi32>) -> (tensor<506x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>_278": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x60xi32>) -> tensor<508x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60xi32>\n    memref.copy %2, %alloc : memref<508x60xi32> to memref<508x60xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60xi32>\n    return %3 : tensor<508x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x60xi32>) -> tensor<508x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<5x5xi32>) outs(%output: tensor<508x60xi32>) -> tensor<508x60xi32>\n  return %ret : tensor<508x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<5x5xi32>, tensor<508x60xi32>) -> (tensor<508x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>_279": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x1022xi32>\n    memref.copy %2, %alloc : memref<62x1022xi32> to memref<62x1022xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x1022xi32>\n    return %3 : tensor<62x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<3x3xi32>, tensor<62x1022xi32>) -> (tensor<62x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>_280": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x128xi32>) -> tensor<128x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128xi32>\n    memref.copy %2, %alloc : memref<128x128xi32> to memref<128x128xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128xi32>\n    return %3 : tensor<128x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x128xi32>) -> tensor<128x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<1x1xi32>) outs(%output: tensor<128x128xi32>) -> tensor<128x128xi32>\n  return %ret : tensor<128x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<1x1xi32>, tensor<128x128xi32>) -> (tensor<128x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_281": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>_282": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020xi32>\n    memref.copy %2, %alloc : memref<508x1020xi32> to memref<508x1020xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020xi32>\n    return %3 : tensor<508x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<5x5xi32>, tensor<508x1020xi32>) -> (tensor<508x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<1x1xi32>) outs(%output: tensor<512x512xi32>) -> tensor<512x512xi32>_283": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<1x1xi32>) outs(%output: tensor<512x512xi32>) -> tensor<512x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x512xi32>) -> tensor<512x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<1x1xi32>) outs(%output: tensor<512x512xi32>) -> tensor<512x512xi32>\n  return %ret : tensor<512x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x512xi32>) -> tensor<512x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x512xi32>\n    memref.copy %2, %alloc : memref<512x512xi32> to memref<512x512xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x512xi32>\n    return %3 : tensor<512x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x512xi32>) -> tensor<512x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<1x1xi32>) outs(%output: tensor<512x512xi32>) -> tensor<512x512xi32>\n  return %ret : tensor<512x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<1x1xi32>, tensor<512x512xi32>) -> (tensor<512x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_284": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>_285": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x28xi32>) -> tensor<124x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28xi32>\n    memref.copy %2, %alloc : memref<124x28xi32> to memref<124x28xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28xi32>\n    return %3 : tensor<124x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x28xi32>) -> tensor<124x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<5x5xi32>) outs(%output: tensor<124x28xi32>) -> tensor<124x28xi32>\n  return %ret : tensor<124x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<5x5xi32>, tensor<124x28xi32>) -> (tensor<124x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>_286": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x126xi32>) -> tensor<254x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126xi32>\n    memref.copy %2, %alloc : memref<254x126xi32> to memref<254x126xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126xi32>\n    return %3 : tensor<254x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<3x3xi32>, tensor<254x126xi32>) -> (tensor<254x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>_287": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x256xi32>) -> tensor<128x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256xi32>\n    memref.copy %2, %alloc : memref<128x256xi32> to memref<128x256xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256xi32>\n    return %3 : tensor<128x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<1x1xi32>, tensor<128x256xi32>) -> (tensor<128x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>_288": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x512xi32>) -> tensor<32x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512xi32>\n    memref.copy %2, %alloc : memref<32x512xi32> to memref<32x512xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x512xi32>\n    return %3 : tensor<32x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<1x1xi32>, tensor<32x512xi32>) -> (tensor<32x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>_289": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>\n  return %ret : tensor<122x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018xi32>\n    memref.copy %2, %alloc : memref<122x1018xi32> to memref<122x1018xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018xi32>\n    return %3 : tensor<122x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x1018xi32>) -> tensor<122x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<122x1018xi32>) -> tensor<122x1018xi32>\n  return %ret : tensor<122x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<7x7xi32>, tensor<122x1018xi32>) -> (tensor<122x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>_290": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250xi32>\n    memref.copy %2, %alloc : memref<1018x250xi32> to memref<1018x250xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250xi32>\n    return %3 : tensor<1018x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<7x7xi32>, tensor<1018x250xi32>) -> (tensor<1018x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>_291": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x252xi32>) -> tensor<124x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252xi32>\n    memref.copy %2, %alloc : memref<124x252xi32> to memref<124x252xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252xi32>\n    return %3 : tensor<124x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<5x5xi32>, tensor<124x252xi32>) -> (tensor<124x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>_292": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x506xi32>) -> tensor<122x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506xi32>\n    memref.copy %2, %alloc : memref<122x506xi32> to memref<122x506xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506xi32>\n    return %3 : tensor<122x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<7x7xi32>, tensor<122x506xi32>) -> (tensor<122x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>_293": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506xi32>\n    memref.copy %2, %alloc : memref<1018x506xi32> to memref<1018x506xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506xi32>\n    return %3 : tensor<1018x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<7x7xi32>, tensor<1018x506xi32>) -> (tensor<1018x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>_294": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x506xi32>) -> tensor<122x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506xi32>\n    memref.copy %2, %alloc : memref<122x506xi32> to memref<122x506xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506xi32>\n    return %3 : tensor<122x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<7x7xi32>, tensor<122x506xi32>) -> (tensor<122x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_295": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>_296": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x254xi32>) -> tensor<254x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>\n  return %ret : tensor<254x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x254xi32>) -> tensor<254x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254xi32>\n    memref.copy %2, %alloc : memref<254x254xi32> to memref<254x254xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254xi32>\n    return %3 : tensor<254x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x254xi32>) -> tensor<254x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<3x3xi32>) outs(%output: tensor<254x254xi32>) -> tensor<254x254xi32>\n  return %ret : tensor<254x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<3x3xi32>, tensor<254x254xi32>) -> (tensor<254x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>_297": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x506xi32>) -> tensor<122x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506xi32>\n    memref.copy %2, %alloc : memref<122x506xi32> to memref<122x506xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506xi32>\n    return %3 : tensor<122x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x506xi32>) -> tensor<122x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<7x7xi32>) outs(%output: tensor<122x506xi32>) -> tensor<122x506xi32>\n  return %ret : tensor<122x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<7x7xi32>, tensor<122x506xi32>) -> (tensor<122x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>_298": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x252xi32>) -> tensor<252x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x252xi32>\n    memref.copy %2, %alloc : memref<252x252xi32> to memref<252x252xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x252xi32>\n    return %3 : tensor<252x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x252xi32>) -> tensor<252x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<5x5xi32>) outs(%output: tensor<252x252xi32>) -> tensor<252x252xi32>\n  return %ret : tensor<252x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<5x5xi32>, tensor<252x252xi32>) -> (tensor<252x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>_299": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x60xi32>) -> tensor<124x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60xi32>\n    memref.copy %2, %alloc : memref<124x60xi32> to memref<124x60xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60xi32>\n    return %3 : tensor<124x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<5x5xi32>, tensor<124x60xi32>) -> (tensor<124x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>_300": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018xi32>\n    memref.copy %2, %alloc : memref<506x1018xi32> to memref<506x1018xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018xi32>\n    return %3 : tensor<506x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<7x7xi32>, tensor<506x1018xi32>) -> (tensor<506x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_301": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>_302": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x62xi32>) -> tensor<254x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>\n  return %ret : tensor<254x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x62xi32>) -> tensor<254x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x62xi32>\n    memref.copy %2, %alloc : memref<254x62xi32> to memref<254x62xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x62xi32>\n    return %3 : tensor<254x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x62xi32>) -> tensor<254x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x64xi32>, tensor<3x3xi32>) outs(%output: tensor<254x62xi32>) -> tensor<254x62xi32>\n  return %ret : tensor<254x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64xi32>, tensor<3x3xi32>, tensor<254x62xi32>) -> (tensor<254x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>_303": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x506xi32>) -> tensor<506x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506xi32>\n    memref.copy %2, %alloc : memref<506x506xi32> to memref<506x506xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506xi32>\n    return %3 : tensor<506x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<7x7xi32>, tensor<506x506xi32>) -> (tensor<506x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<510x1022xi32>) -> tensor<510x1022xi32>_304": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<510x1022xi32>) -> tensor<510x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x1022xi32>) -> tensor<510x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<510x1022xi32>) -> tensor<510x1022xi32>\n  return %ret : tensor<510x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x1022xi32>) -> tensor<510x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x1022xi32>\n    memref.copy %2, %alloc : memref<510x1022xi32> to memref<510x1022xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x1022xi32>\n    return %3 : tensor<510x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x1022xi32>) -> tensor<510x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<510x1022xi32>) -> tensor<510x1022xi32>\n  return %ret : tensor<510x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<3x3xi32>, tensor<510x1022xi32>) -> (tensor<510x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>_305": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x126xi32>) -> tensor<62x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126xi32>\n    memref.copy %2, %alloc : memref<62x126xi32> to memref<62x126xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126xi32>\n    return %3 : tensor<62x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<3x3xi32>, tensor<62x126xi32>) -> (tensor<62x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_306": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>_307": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62xi32>\n    memref.copy %2, %alloc : memref<1022x62xi32> to memref<1022x62xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62xi32>\n    return %3 : tensor<1022x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<3x3xi32>, tensor<1022x62xi32>) -> (tensor<1022x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>_308": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x506xi32>) -> tensor<250x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506xi32>\n    memref.copy %2, %alloc : memref<250x506xi32> to memref<250x506xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506xi32>\n    return %3 : tensor<250x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<7x7xi32>, tensor<250x506xi32>) -> (tensor<250x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>_309": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x252xi32>) -> tensor<28x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252xi32>\n    memref.copy %2, %alloc : memref<28x252xi32> to memref<28x252xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252xi32>\n    return %3 : tensor<28x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<5x5xi32>, tensor<28x252xi32>) -> (tensor<28x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>_310": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024xi32>\n    memref.copy %2, %alloc : memref<256x1024xi32> to memref<256x1024xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024xi32>\n    return %3 : tensor<256x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<1x1xi32>, tensor<256x1024xi32>) -> (tensor<256x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>_311": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250xi32>\n    memref.copy %2, %alloc : memref<1018x250xi32> to memref<1018x250xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250xi32>\n    return %3 : tensor<1018x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<7x7xi32>, tensor<1018x250xi32>) -> (tensor<1018x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<5x5xi32>) outs(%output: tensor<252x124xi32>) -> tensor<252x124xi32>_312": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<5x5xi32>) outs(%output: tensor<252x124xi32>) -> tensor<252x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x124xi32>) -> tensor<252x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<5x5xi32>) outs(%output: tensor<252x124xi32>) -> tensor<252x124xi32>\n  return %ret : tensor<252x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x124xi32>) -> tensor<252x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x124xi32>\n    memref.copy %2, %alloc : memref<252x124xi32> to memref<252x124xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x124xi32>\n    return %3 : tensor<252x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x124xi32>) -> tensor<252x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<5x5xi32>) outs(%output: tensor<252x124xi32>) -> tensor<252x124xi32>\n  return %ret : tensor<252x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<5x5xi32>, tensor<252x124xi32>) -> (tensor<252x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>_313": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x508xi32>) -> tensor<252x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>\n  return %ret : tensor<252x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x508xi32>) -> tensor<252x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x508xi32>\n    memref.copy %2, %alloc : memref<252x508xi32> to memref<252x508xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x508xi32>\n    return %3 : tensor<252x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x508xi32>) -> tensor<252x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<5x5xi32>) outs(%output: tensor<252x508xi32>) -> tensor<252x508xi32>\n  return %ret : tensor<252x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<5x5xi32>, tensor<252x508xi32>) -> (tensor<252x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>_314": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022xi32>\n    memref.copy %2, %alloc : memref<254x1022xi32> to memref<254x1022xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022xi32>\n    return %3 : tensor<254x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<3x3xi32>, tensor<254x1022xi32>) -> (tensor<254x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_315": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>_316": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x1022xi32>\n    memref.copy %2, %alloc : memref<62x1022xi32> to memref<62x1022xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x1022xi32>\n    return %3 : tensor<62x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x1022xi32>) -> tensor<62x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<62x1022xi32>) -> tensor<62x1022xi32>\n  return %ret : tensor<62x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<3x3xi32>, tensor<62x1022xi32>) -> (tensor<62x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>_317": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x62xi32>) -> tensor<62x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62xi32>\n    memref.copy %2, %alloc : memref<62x62xi32> to memref<62x62xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62xi32>\n    return %3 : tensor<62x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x62xi32>) -> tensor<62x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<3x3xi32>) outs(%output: tensor<62x62xi32>) -> tensor<62x62xi32>\n  return %ret : tensor<62x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<3x3xi32>, tensor<62x62xi32>) -> (tensor<62x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>_318": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>\n  return %ret : tensor<1020x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28xi32>\n    memref.copy %2, %alloc : memref<1020x28xi32> to memref<1020x28xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28xi32>\n    return %3 : tensor<1020x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x28xi32>) -> tensor<1020x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x28xi32>) -> tensor<1020x28xi32>\n  return %ret : tensor<1020x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<5x5xi32>, tensor<1020x28xi32>) -> (tensor<1020x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>_319": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022xi32>\n    memref.copy %2, %alloc : memref<254x1022xi32> to memref<254x1022xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022xi32>\n    return %3 : tensor<254x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<3x3xi32>, tensor<254x1022xi32>) -> (tensor<254x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>_320": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x512xi32>) -> tensor<256x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>\n  return %ret : tensor<256x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x512xi32>) -> tensor<256x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512xi32>\n    memref.copy %2, %alloc : memref<256x512xi32> to memref<256x512xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512xi32>\n    return %3 : tensor<256x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x512xi32>) -> tensor<256x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<1x1xi32>) outs(%output: tensor<256x512xi32>) -> tensor<256x512xi32>\n  return %ret : tensor<256x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<1x1xi32>, tensor<256x512xi32>) -> (tensor<256x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_321": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_322": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>_323": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30xi32>\n    memref.copy %2, %alloc : memref<1022x30xi32> to memref<1022x30xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30xi32>\n    return %3 : tensor<1022x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x30xi32>) -> tensor<1022x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x30xi32>) -> tensor<1022x30xi32>\n  return %ret : tensor<1022x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<3x3xi32>, tensor<1022x30xi32>) -> (tensor<1022x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_324": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<3x3xi32>) outs(%output: tensor<510x126xi32>) -> tensor<510x126xi32>_325": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<3x3xi32>) outs(%output: tensor<510x126xi32>) -> tensor<510x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x126xi32>) -> tensor<510x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<3x3xi32>) outs(%output: tensor<510x126xi32>) -> tensor<510x126xi32>\n  return %ret : tensor<510x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x126xi32>) -> tensor<510x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x126xi32>\n    memref.copy %2, %alloc : memref<510x126xi32> to memref<510x126xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x126xi32>\n    return %3 : tensor<510x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x126xi32>) -> tensor<510x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<3x3xi32>) outs(%output: tensor<510x126xi32>) -> tensor<510x126xi32>\n  return %ret : tensor<510x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<3x3xi32>, tensor<510x126xi32>) -> (tensor<510x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>_326": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x30xi32>) -> tensor<62x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30xi32>\n    memref.copy %2, %alloc : memref<62x30xi32> to memref<62x30xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30xi32>\n    return %3 : tensor<62x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x30xi32>) -> tensor<62x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<3x3xi32>) outs(%output: tensor<62x30xi32>) -> tensor<62x30xi32>\n  return %ret : tensor<62x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<3x3xi32>, tensor<62x30xi32>) -> (tensor<62x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<7x7xi32>) outs(%output: tensor<122x58xi32>) -> tensor<122x58xi32>_327": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<7x7xi32>) outs(%output: tensor<122x58xi32>) -> tensor<122x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x58xi32>) -> tensor<122x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<7x7xi32>) outs(%output: tensor<122x58xi32>) -> tensor<122x58xi32>\n  return %ret : tensor<122x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x58xi32>) -> tensor<122x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x58xi32>\n    memref.copy %2, %alloc : memref<122x58xi32> to memref<122x58xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x58xi32>\n    return %3 : tensor<122x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x58xi32>) -> tensor<122x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<7x7xi32>) outs(%output: tensor<122x58xi32>) -> tensor<122x58xi32>\n  return %ret : tensor<122x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<7x7xi32>, tensor<122x58xi32>) -> (tensor<122x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>_328": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128xi32>\n    memref.copy %2, %alloc : memref<1024x128xi32> to memref<1024x128xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128xi32>\n    return %3 : tensor<1024x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x128xi32>) -> tensor<1024x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x128xi32>) -> tensor<1024x128xi32>\n  return %ret : tensor<1024x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<1x1xi32>, tensor<1024x128xi32>) -> (tensor<1024x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_329": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>_330": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x508xi32>) -> tensor<508x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508xi32>\n    memref.copy %2, %alloc : memref<508x508xi32> to memref<508x508xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508xi32>\n    return %3 : tensor<508x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x508xi32>) -> tensor<508x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<5x5xi32>) outs(%output: tensor<508x508xi32>) -> tensor<508x508xi32>\n  return %ret : tensor<508x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<5x5xi32>, tensor<508x508xi32>) -> (tensor<508x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>_331": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x128xi32>) -> tensor<64x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128xi32>\n    memref.copy %2, %alloc : memref<64x128xi32> to memref<64x128xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128xi32>\n    return %3 : tensor<64x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<1x1xi32>, tensor<64x128xi32>) -> (tensor<64x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>_332": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x506xi32>) -> tensor<506x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506xi32>\n    memref.copy %2, %alloc : memref<506x506xi32> to memref<506x506xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506xi32>\n    return %3 : tensor<506x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x506xi32>) -> tensor<506x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<7x7xi32>) outs(%output: tensor<506x506xi32>) -> tensor<506x506xi32>\n  return %ret : tensor<506x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<7x7xi32>, tensor<506x506xi32>) -> (tensor<506x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>_333": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508xi32>\n    memref.copy %2, %alloc : memref<1020x508xi32> to memref<1020x508xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508xi32>\n    return %3 : tensor<1020x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x508xi32>) -> tensor<1020x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x508xi32>) -> tensor<1020x508xi32>\n  return %ret : tensor<1020x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<5x5xi32>, tensor<1020x508xi32>) -> (tensor<1020x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>_334": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x250xi32>) -> tensor<506x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x250xi32>\n    memref.copy %2, %alloc : memref<506x250xi32> to memref<506x250xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x250xi32>\n    return %3 : tensor<506x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x250xi32>) -> tensor<506x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<7x7xi32>) outs(%output: tensor<506x250xi32>) -> tensor<506x250xi32>\n  return %ret : tensor<506x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<7x7xi32>, tensor<506x250xi32>) -> (tensor<506x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_335": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_336": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>_337": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x26xi32>) -> tensor<122x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26xi32>\n    memref.copy %2, %alloc : memref<122x26xi32> to memref<122x26xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26xi32>\n    return %3 : tensor<122x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x26xi32>) -> tensor<122x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<7x7xi32>) outs(%output: tensor<122x26xi32>) -> tensor<122x26xi32>\n  return %ret : tensor<122x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<7x7xi32>, tensor<122x26xi32>) -> (tensor<122x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>_338": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x250xi32>) -> tensor<250x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250xi32>\n    memref.copy %2, %alloc : memref<250x250xi32> to memref<250x250xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250xi32>\n    return %3 : tensor<250x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x250xi32>) -> tensor<250x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<7x7xi32>) outs(%output: tensor<250x250xi32>) -> tensor<250x250xi32>\n  return %ret : tensor<250x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<7x7xi32>, tensor<250x250xi32>) -> (tensor<250x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>_339": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x510xi32>) -> tensor<254x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>\n  return %ret : tensor<254x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x510xi32>) -> tensor<254x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x510xi32>\n    memref.copy %2, %alloc : memref<254x510xi32> to memref<254x510xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x510xi32>\n    return %3 : tensor<254x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x510xi32>) -> tensor<254x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>\n  return %ret : tensor<254x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<3x3xi32>, tensor<254x510xi32>) -> (tensor<254x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>_340": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x30xi32>) -> tensor<126x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30xi32>\n    memref.copy %2, %alloc : memref<126x30xi32> to memref<126x30xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30xi32>\n    return %3 : tensor<126x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<3x3xi32>, tensor<126x30xi32>) -> (tensor<126x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>_341": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020xi32>\n    memref.copy %2, %alloc : memref<28x1020xi32> to memref<28x1020xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020xi32>\n    return %3 : tensor<28x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<5x5xi32>, tensor<28x1020xi32>) -> (tensor<28x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_342": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>_343": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62xi32>\n    memref.copy %2, %alloc : memref<1022x62xi32> to memref<1022x62xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62xi32>\n    return %3 : tensor<1022x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<3x3xi32>, tensor<1022x62xi32>) -> (tensor<1022x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>_344": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018xi32>\n    memref.copy %2, %alloc : memref<58x1018xi32> to memref<58x1018xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018xi32>\n    return %3 : tensor<58x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<7x7xi32>, tensor<58x1018xi32>) -> (tensor<58x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>_345": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x508xi32>) -> tensor<28x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508xi32>\n    memref.copy %2, %alloc : memref<28x508xi32> to memref<28x508xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508xi32>\n    return %3 : tensor<28x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<5x5xi32>, tensor<28x508xi32>) -> (tensor<28x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_346": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>_347": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018xi32>\n    memref.copy %2, %alloc : memref<58x1018xi32> to memref<58x1018xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018xi32>\n    return %3 : tensor<58x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<7x7xi32>, tensor<58x1018xi32>) -> (tensor<58x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>_348": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x256xi32>) -> tensor<256x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256xi32>\n    memref.copy %2, %alloc : memref<256x256xi32> to memref<256x256xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256xi32>\n    return %3 : tensor<256x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x256xi32>) -> tensor<256x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x256xi32>, tensor<1x1xi32>) outs(%output: tensor<256x256xi32>) -> tensor<256x256xi32>\n  return %ret : tensor<256x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256xi32>, tensor<1x1xi32>, tensor<256x256xi32>) -> (tensor<256x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_349": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>_350": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x58xi32>) -> tensor<506x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>\n  return %ret : tensor<506x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x58xi32>) -> tensor<506x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x58xi32>\n    memref.copy %2, %alloc : memref<506x58xi32> to memref<506x58xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x58xi32>\n    return %3 : tensor<506x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x58xi32>) -> tensor<506x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<7x7xi32>) outs(%output: tensor<506x58xi32>) -> tensor<506x58xi32>\n  return %ret : tensor<506x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<7x7xi32>, tensor<506x58xi32>) -> (tensor<506x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>_351": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62xi32>\n    memref.copy %2, %alloc : memref<1022x62xi32> to memref<1022x62xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62xi32>\n    return %3 : tensor<1022x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x62xi32>) -> tensor<1022x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x62xi32>) -> tensor<1022x62xi32>\n  return %ret : tensor<1022x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<3x3xi32>, tensor<1022x62xi32>) -> (tensor<1022x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_352": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_353": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_354": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_355": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>_356": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x124xi32>) -> tensor<60x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124xi32>\n    memref.copy %2, %alloc : memref<60x124xi32> to memref<60x124xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124xi32>\n    return %3 : tensor<60x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<5x5xi32>, tensor<60x124xi32>) -> (tensor<60x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_357": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>_358": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x28xi32>) -> tensor<60x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28xi32>\n    memref.copy %2, %alloc : memref<60x28xi32> to memref<60x28xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28xi32>\n    return %3 : tensor<60x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<5x5xi32>, tensor<60x28xi32>) -> (tensor<60x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_359": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>_360": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020xi32>\n    memref.copy %2, %alloc : memref<508x1020xi32> to memref<508x1020xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020xi32>\n    return %3 : tensor<508x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<5x5xi32>, tensor<508x1020xi32>) -> (tensor<508x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>_361": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x64xi32>) -> tensor<64x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64xi32>\n    memref.copy %2, %alloc : memref<64x64xi32> to memref<64x64xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64xi32>\n    return %3 : tensor<64x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x64xi32>) -> tensor<64x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<1x1xi32>) outs(%output: tensor<64x64xi32>) -> tensor<64x64xi32>\n  return %ret : tensor<64x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<1x1xi32>, tensor<64x64xi32>) -> (tensor<64x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>_362": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58xi32>\n    memref.copy %2, %alloc : memref<1018x58xi32> to memref<1018x58xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58xi32>\n    return %3 : tensor<1018x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<7x7xi32>, tensor<1018x58xi32>) -> (tensor<1018x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_363": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_364": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>_365": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x122xi32>) -> tensor<122x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122xi32>\n    memref.copy %2, %alloc : memref<122x122xi32> to memref<122x122xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122xi32>\n    return %3 : tensor<122x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<7x7xi32>, tensor<122x122xi32>) -> (tensor<122x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>_366": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x506xi32>) -> tensor<58x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506xi32>\n    memref.copy %2, %alloc : memref<58x506xi32> to memref<58x506xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506xi32>\n    return %3 : tensor<58x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<7x7xi32>, tensor<58x506xi32>) -> (tensor<58x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>_367": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x128xi32>) -> tensor<512x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128xi32>\n    memref.copy %2, %alloc : memref<512x128xi32> to memref<512x128xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128xi32>\n    return %3 : tensor<512x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<1x1xi32>, tensor<512x128xi32>) -> (tensor<512x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>_368": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x256xi32>) -> tensor<128x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256xi32>\n    memref.copy %2, %alloc : memref<128x256xi32> to memref<128x256xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256xi32>\n    return %3 : tensor<128x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<1x1xi32>, tensor<128x256xi32>) -> (tensor<128x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>_369": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>\n  return %ret : tensor<252x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x1020xi32>\n    memref.copy %2, %alloc : memref<252x1020xi32> to memref<252x1020xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x1020xi32>\n    return %3 : tensor<252x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>\n  return %ret : tensor<252x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<5x5xi32>, tensor<252x1020xi32>) -> (tensor<252x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>_370": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x128xi32>) -> tensor<256x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xi32>\n    memref.copy %2, %alloc : memref<256x128xi32> to memref<256x128xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128xi32>\n    return %3 : tensor<256x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<1x1xi32>, tensor<256x128xi32>) -> (tensor<256x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>_371": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x30xi32>) -> tensor<126x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30xi32>\n    memref.copy %2, %alloc : memref<126x30xi32> to memref<126x30xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30xi32>\n    return %3 : tensor<126x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x30xi32>) -> tensor<126x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x32xi32>, tensor<3x3xi32>) outs(%output: tensor<126x30xi32>) -> tensor<126x30xi32>\n  return %ret : tensor<126x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32xi32>, tensor<3x3xi32>, tensor<126x30xi32>) -> (tensor<126x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>_372": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x512xi32>) -> tensor<32x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512xi32>\n    memref.copy %2, %alloc : memref<32x512xi32> to memref<32x512xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x512xi32>\n    return %3 : tensor<32x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x512xi32>) -> tensor<32x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<1x1xi32>) outs(%output: tensor<32x512xi32>) -> tensor<32x512xi32>\n  return %ret : tensor<32x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<1x1xi32>, tensor<32x512xi32>) -> (tensor<32x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>_373": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x254xi32>) -> tensor<510x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>\n  return %ret : tensor<510x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x254xi32>) -> tensor<510x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254xi32>\n    memref.copy %2, %alloc : memref<510x254xi32> to memref<510x254xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254xi32>\n    return %3 : tensor<510x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x254xi32>) -> tensor<510x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<3x3xi32>) outs(%output: tensor<510x254xi32>) -> tensor<510x254xi32>\n  return %ret : tensor<510x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<3x3xi32>, tensor<510x254xi32>) -> (tensor<510x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_374": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>_375": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>\n  return %ret : tensor<252x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x1020xi32>\n    memref.copy %2, %alloc : memref<252x1020xi32> to memref<252x1020xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x1020xi32>\n    return %3 : tensor<252x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x1020xi32>) -> tensor<252x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<252x1020xi32>) -> tensor<252x1020xi32>\n  return %ret : tensor<252x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<5x5xi32>, tensor<252x1020xi32>) -> (tensor<252x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>_376": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x254xi32>) -> tensor<30x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254xi32>\n    memref.copy %2, %alloc : memref<30x254xi32> to memref<30x254xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254xi32>\n    return %3 : tensor<30x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<3x3xi32>, tensor<30x254xi32>) -> (tensor<30x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>_377": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x60xi32>) -> tensor<124x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60xi32>\n    memref.copy %2, %alloc : memref<124x60xi32> to memref<124x60xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60xi32>\n    return %3 : tensor<124x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<5x5xi32>, tensor<124x60xi32>) -> (tensor<124x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>_378": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x28xi32>) -> tensor<60x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28xi32>\n    memref.copy %2, %alloc : memref<60x28xi32> to memref<60x28xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28xi32>\n    return %3 : tensor<60x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<5x5xi32>, tensor<60x28xi32>) -> (tensor<60x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>_379": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018xi32>\n    memref.copy %2, %alloc : memref<506x1018xi32> to memref<506x1018xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018xi32>\n    return %3 : tensor<506x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x1018xi32>) -> tensor<506x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<506x1018xi32>) -> tensor<506x1018xi32>\n  return %ret : tensor<506x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<7x7xi32>, tensor<506x1018xi32>) -> (tensor<506x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>_380": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>\n  return %ret : tensor<1024x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x1024xi32>\n    memref.copy %2, %alloc : memref<1024x1024xi32> to memref<1024x1024xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x1024xi32>\n    return %3 : tensor<1024x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>\n  return %ret : tensor<1024x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<1x1xi32>, tensor<1024x1024xi32>) -> (tensor<1024x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>_381": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018xi32>\n    memref.copy %2, %alloc : memref<250x1018xi32> to memref<250x1018xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018xi32>\n    return %3 : tensor<250x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x1018xi32>) -> tensor<250x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<250x1018xi32>) -> tensor<250x1018xi32>\n  return %ret : tensor<250x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<7x7xi32>, tensor<250x1018xi32>) -> (tensor<250x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>_382": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x122xi32>) -> tensor<122x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122xi32>\n    memref.copy %2, %alloc : memref<122x122xi32> to memref<122x122xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122xi32>\n    return %3 : tensor<122x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x122xi32>) -> tensor<122x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<7x7xi32>) outs(%output: tensor<122x122xi32>) -> tensor<122x122xi32>\n  return %ret : tensor<122x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<7x7xi32>, tensor<122x122xi32>) -> (tensor<122x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>_383": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x250xi32>) -> tensor<122x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250xi32>\n    memref.copy %2, %alloc : memref<122x250xi32> to memref<122x250xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250xi32>\n    return %3 : tensor<122x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<7x7xi32>, tensor<122x250xi32>) -> (tensor<122x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>_384": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022xi32>\n    memref.copy %2, %alloc : memref<30x1022xi32> to memref<30x1022xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022xi32>\n    return %3 : tensor<30x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<3x3xi32>, tensor<30x1022xi32>) -> (tensor<30x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>_385": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<128x256xi32>) -> tensor<128x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256xi32>\n    memref.copy %2, %alloc : memref<128x256xi32> to memref<128x256xi32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<128x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<128x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256xi32>\n    return %3 : tensor<128x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<128x256xi32>) -> tensor<128x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<1x1xi32>) outs(%output: tensor<128x256xi32>) -> tensor<128x256xi32>\n  return %ret : tensor<128x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<1x1xi32>, tensor<128x256xi32>) -> (tensor<128x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>_386": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x1020xi32>\n    memref.copy %2, %alloc : memref<60x1020xi32> to memref<60x1020xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x1020xi32>\n    return %3 : tensor<60x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x1020xi32>) -> tensor<60x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<60x1020xi32>) -> tensor<60x1020xi32>\n  return %ret : tensor<60x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<5x5xi32>, tensor<60x1020xi32>) -> (tensor<60x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_387": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>_388": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x512xi32>\n    memref.copy %2, %alloc : memref<1024x512xi32> to memref<1024x512xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x512xi32>\n    return %3 : tensor<1024x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<1x1xi32>, tensor<1024x512xi32>) -> (tensor<1024x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>_389": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x32xi32>) -> tensor<32x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32xi32>\n    memref.copy %2, %alloc : memref<32x32xi32> to memref<32x32xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32xi32>\n    return %3 : tensor<32x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x32xi32>) -> tensor<32x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<1x1xi32>) outs(%output: tensor<32x32xi32>) -> tensor<32x32xi32>\n  return %ret : tensor<32x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<1x1xi32>, tensor<32x32xi32>) -> (tensor<32x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_390": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>_391": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022xi32>\n    memref.copy %2, %alloc : memref<30x1022xi32> to memref<30x1022xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022xi32>\n    return %3 : tensor<30x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<3x3xi32>, tensor<30x1022xi32>) -> (tensor<30x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>_392": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x508xi32>) -> tensor<28x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508xi32>\n    memref.copy %2, %alloc : memref<28x508xi32> to memref<28x508xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508xi32>\n    return %3 : tensor<28x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<5x5xi32>, tensor<28x508xi32>) -> (tensor<28x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_393": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>_394": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x58xi32>) -> tensor<58x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58xi32>\n    memref.copy %2, %alloc : memref<58x58xi32> to memref<58x58xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58xi32>\n    return %3 : tensor<58x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<7x7xi32>, tensor<58x58xi32>) -> (tensor<58x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>_395": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>\n  return %ret : tensor<26x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x1018xi32>\n    memref.copy %2, %alloc : memref<26x1018xi32> to memref<26x1018xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x1018xi32>\n    return %3 : tensor<26x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>\n  return %ret : tensor<26x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<7x7xi32>, tensor<26x1018xi32>) -> (tensor<26x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>_396": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x30xi32>) -> tensor<510x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>\n  return %ret : tensor<510x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x30xi32>) -> tensor<510x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x30xi32>\n    memref.copy %2, %alloc : memref<510x30xi32> to memref<510x30xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x30xi32>\n    return %3 : tensor<510x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x30xi32>) -> tensor<510x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<3x3xi32>) outs(%output: tensor<510x30xi32>) -> tensor<510x30xi32>\n  return %ret : tensor<510x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<3x3xi32>, tensor<510x30xi32>) -> (tensor<510x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_397": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>_398": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x252xi32>) -> tensor<124x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252xi32>\n    memref.copy %2, %alloc : memref<124x252xi32> to memref<124x252xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252xi32>\n    return %3 : tensor<124x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x252xi32>) -> tensor<124x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<5x5xi32>) outs(%output: tensor<124x252xi32>) -> tensor<124x252xi32>\n  return %ret : tensor<124x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<5x5xi32>, tensor<124x252xi32>) -> (tensor<124x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>_399": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x122xi32>) -> tensor<58x122xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122xi32>\n    memref.copy %2, %alloc : memref<58x122xi32> to memref<58x122xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x122xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x122xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122xi32>\n    return %3 : tensor<58x122xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x122xi32>) -> tensor<58x122xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<7x7xi32>) outs(%output: tensor<58x122xi32>) -> tensor<58x122xi32>\n  return %ret : tensor<58x122xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x122xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<7x7xi32>, tensor<58x122xi32>) -> (tensor<58x122xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>_400": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x252xi32>) -> tensor<28x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252xi32>\n    memref.copy %2, %alloc : memref<28x252xi32> to memref<28x252xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252xi32>\n    return %3 : tensor<28x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<5x5xi32>, tensor<28x252xi32>) -> (tensor<28x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>_401": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x126xi32>) -> tensor<254x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126xi32>\n    memref.copy %2, %alloc : memref<254x126xi32> to memref<254x126xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126xi32>\n    return %3 : tensor<254x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<3x3xi32>, tensor<254x126xi32>) -> (tensor<254x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>_402": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022xi32>\n    memref.copy %2, %alloc : memref<254x1022xi32> to memref<254x1022xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022xi32>\n    return %3 : tensor<254x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x1022xi32>) -> tensor<254x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<254x1022xi32>) -> tensor<254x1022xi32>\n  return %ret : tensor<254x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<3x3xi32>, tensor<254x1022xi32>) -> (tensor<254x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>_403": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x256xi32>) -> tensor<64x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256xi32>\n    memref.copy %2, %alloc : memref<64x256xi32> to memref<64x256xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256xi32>\n    return %3 : tensor<64x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<1x1xi32>, tensor<64x256xi32>) -> (tensor<64x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>_404": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x60xi32>) -> tensor<28x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60xi32>\n    memref.copy %2, %alloc : memref<28x60xi32> to memref<28x60xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60xi32>\n    return %3 : tensor<28x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x60xi32>) -> tensor<28x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<5x5xi32>) outs(%output: tensor<28x60xi32>) -> tensor<28x60xi32>\n  return %ret : tensor<28x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<5x5xi32>, tensor<28x60xi32>) -> (tensor<28x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>_405": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x252xi32>) -> tensor<508x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252xi32>\n    memref.copy %2, %alloc : memref<508x252xi32> to memref<508x252xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252xi32>\n    return %3 : tensor<508x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x252xi32>) -> tensor<508x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x256xi32>, tensor<5x5xi32>) outs(%output: tensor<508x252xi32>) -> tensor<508x252xi32>\n  return %ret : tensor<508x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256xi32>, tensor<5x5xi32>, tensor<508x252xi32>) -> (tensor<508x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>_406": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x126xi32>) -> tensor<30x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126xi32>\n    memref.copy %2, %alloc : memref<30x126xi32> to memref<30x126xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126xi32>\n    return %3 : tensor<30x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x126xi32>) -> tensor<30x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<3x3xi32>) outs(%output: tensor<30x126xi32>) -> tensor<30x126xi32>\n  return %ret : tensor<30x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<3x3xi32>, tensor<30x126xi32>) -> (tensor<30x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>_407": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x506xi32>) -> tensor<250x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506xi32>\n    memref.copy %2, %alloc : memref<250x506xi32> to memref<250x506xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506xi32>\n    return %3 : tensor<250x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<7x7xi32>, tensor<250x506xi32>) -> (tensor<250x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>_408": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x28xi32>) -> tensor<60x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28xi32>\n    memref.copy %2, %alloc : memref<60x28xi32> to memref<60x28xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28xi32>\n    return %3 : tensor<60x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x28xi32>) -> tensor<60x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<5x5xi32>) outs(%output: tensor<60x28xi32>) -> tensor<60x28xi32>\n  return %ret : tensor<60x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<5x5xi32>, tensor<60x28xi32>) -> (tensor<60x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>_409": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x64xi32>\n    memref.copy %2, %alloc : memref<1024x64xi32> to memref<1024x64xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x64xi32>\n    return %3 : tensor<1024x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<1x1xi32>, tensor<1024x64xi32>) -> (tensor<1024x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>_410": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x32xi32>) -> tensor<64x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32xi32>\n    memref.copy %2, %alloc : memref<64x32xi32> to memref<64x32xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32xi32>\n    return %3 : tensor<64x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<1x1xi32>, tensor<64x32xi32>) -> (tensor<64x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>_411": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506xi32>\n    memref.copy %2, %alloc : memref<1018x506xi32> to memref<1018x506xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506xi32>\n    return %3 : tensor<1018x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x506xi32>) -> tensor<1018x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x506xi32>) -> tensor<1018x506xi32>\n  return %ret : tensor<1018x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<7x7xi32>, tensor<1018x506xi32>) -> (tensor<1018x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_412": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>_413": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x256xi32>) -> tensor<64x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256xi32>\n    memref.copy %2, %alloc : memref<64x256xi32> to memref<64x256xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256xi32>\n    return %3 : tensor<64x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x256xi32>) -> tensor<64x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<1x1xi32>) outs(%output: tensor<64x256xi32>) -> tensor<64x256xi32>\n  return %ret : tensor<64x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<1x1xi32>, tensor<64x256xi32>) -> (tensor<64x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>_414": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x506xi32>) -> tensor<250x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506xi32>\n    memref.copy %2, %alloc : memref<250x506xi32> to memref<250x506xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506xi32>\n    return %3 : tensor<250x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<7x7xi32>, tensor<250x506xi32>) -> (tensor<250x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>_415": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x26xi32>) -> tensor<250x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26xi32>\n    memref.copy %2, %alloc : memref<250x26xi32> to memref<250x26xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26xi32>\n    return %3 : tensor<250x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x26xi32>) -> tensor<250x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<7x7xi32>) outs(%output: tensor<250x26xi32>) -> tensor<250x26xi32>\n  return %ret : tensor<250x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<7x7xi32>, tensor<250x26xi32>) -> (tensor<250x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_416": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>_417": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58xi32>\n    memref.copy %2, %alloc : memref<1018x58xi32> to memref<1018x58xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58xi32>\n    return %3 : tensor<1018x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<7x7xi32>, tensor<1018x58xi32>) -> (tensor<1018x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>_418": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x64xi32>) -> tensor<512x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>\n  return %ret : tensor<512x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x64xi32>) -> tensor<512x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x64xi32>\n    memref.copy %2, %alloc : memref<512x64xi32> to memref<512x64xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x64xi32>\n    return %3 : tensor<512x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x64xi32>) -> tensor<512x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<1x1xi32>) outs(%output: tensor<512x64xi32>) -> tensor<512x64xi32>\n  return %ret : tensor<512x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<1x1xi32>, tensor<512x64xi32>) -> (tensor<512x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>_419": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x128xi32>) -> tensor<32x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128xi32>\n    memref.copy %2, %alloc : memref<32x128xi32> to memref<32x128xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128xi32>\n    return %3 : tensor<32x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x128xi32>) -> tensor<32x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<1x1xi32>) outs(%output: tensor<32x128xi32>) -> tensor<32x128xi32>\n  return %ret : tensor<32x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<1x1xi32>, tensor<32x128xi32>) -> (tensor<32x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>_420": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x32xi32>) -> tensor<256x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32xi32>\n    memref.copy %2, %alloc : memref<256x32xi32> to memref<256x32xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32xi32>\n    return %3 : tensor<256x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<1x1xi32>, tensor<256x32xi32>) -> (tensor<256x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>_421": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x250xi32>) -> tensor<58x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>\n  return %ret : tensor<58x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x250xi32>) -> tensor<58x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250xi32>\n    memref.copy %2, %alloc : memref<58x250xi32> to memref<58x250xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250xi32>\n    return %3 : tensor<58x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x250xi32>) -> tensor<58x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<7x7xi32>) outs(%output: tensor<58x250xi32>) -> tensor<58x250xi32>\n  return %ret : tensor<58x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<7x7xi32>, tensor<58x250xi32>) -> (tensor<58x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_422": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_423": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_424": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>_425": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x510xi32>) -> tensor<254x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>\n  return %ret : tensor<254x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x510xi32>) -> tensor<254x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x510xi32>\n    memref.copy %2, %alloc : memref<254x510xi32> to memref<254x510xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x510xi32>\n    return %3 : tensor<254x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x510xi32>) -> tensor<254x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<3x3xi32>) outs(%output: tensor<254x510xi32>) -> tensor<254x510xi32>\n  return %ret : tensor<254x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<3x3xi32>, tensor<254x510xi32>) -> (tensor<254x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>_426": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x32xi32>) -> tensor<64x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32xi32>\n    memref.copy %2, %alloc : memref<64x32xi32> to memref<64x32xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32xi32>\n    return %3 : tensor<64x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x32xi32>) -> tensor<64x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x32xi32>, tensor<1x1xi32>) outs(%output: tensor<64x32xi32>) -> tensor<64x32xi32>\n  return %ret : tensor<64x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32xi32>, tensor<1x1xi32>, tensor<64x32xi32>) -> (tensor<64x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>_427": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x254xi32>) -> tensor<62x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>\n  return %ret : tensor<62x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x254xi32>) -> tensor<62x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254xi32>\n    memref.copy %2, %alloc : memref<62x254xi32> to memref<62x254xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254xi32>\n    return %3 : tensor<62x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x254xi32>) -> tensor<62x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>\n  return %ret : tensor<62x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<3x3xi32>, tensor<62x254xi32>) -> (tensor<62x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>_428": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x64xi32>\n    memref.copy %2, %alloc : memref<1024x64xi32> to memref<1024x64xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x64xi32>\n    return %3 : tensor<1024x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<1x1xi32>, tensor<1024x64xi32>) -> (tensor<1024x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>_429": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<122x250xi32>) -> tensor<122x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250xi32>\n    memref.copy %2, %alloc : memref<122x250xi32> to memref<122x250xi32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<122x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<122x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250xi32>\n    return %3 : tensor<122x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<122x250xi32>) -> tensor<122x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<7x7xi32>) outs(%output: tensor<122x250xi32>) -> tensor<122x250xi32>\n  return %ret : tensor<122x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<122x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<7x7xi32>, tensor<122x250xi32>) -> (tensor<122x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>_430": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58xi32>\n    memref.copy %2, %alloc : memref<1018x58xi32> to memref<1018x58xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58xi32>\n    return %3 : tensor<1018x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x58xi32>) -> tensor<1018x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x58xi32>) -> tensor<1018x58xi32>\n  return %ret : tensor<1018x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<7x7xi32>, tensor<1018x58xi32>) -> (tensor<1018x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>_431": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x126xi32>) -> tensor<62x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126xi32>\n    memref.copy %2, %alloc : memref<62x126xi32> to memref<62x126xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126xi32>\n    return %3 : tensor<62x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x126xi32>) -> tensor<62x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<3x3xi32>) outs(%output: tensor<62x126xi32>) -> tensor<62x126xi32>\n  return %ret : tensor<62x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<3x3xi32>, tensor<62x126xi32>) -> (tensor<62x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>_432": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020xi32>\n    memref.copy %2, %alloc : memref<28x1020xi32> to memref<28x1020xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020xi32>\n    return %3 : tensor<28x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<5x5xi32>, tensor<28x1020xi32>) -> (tensor<28x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>_433": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024xi32>\n    memref.copy %2, %alloc : memref<64x1024xi32> to memref<64x1024xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x1024xi32>\n    return %3 : tensor<64x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x1024xi32>) -> tensor<64x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<64x1024xi32>) -> tensor<64x1024xi32>\n  return %ret : tensor<64x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<1x1xi32>, tensor<64x1024xi32>) -> (tensor<64x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_434": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>_435": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x30xi32>) -> tensor<30x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>\n  return %ret : tensor<30x30xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x30xi32>) -> tensor<30x30xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30xi32>\n    memref.copy %2, %alloc : memref<30x30xi32> to memref<30x30xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x30xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x30xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30xi32>\n    return %3 : tensor<30x30xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x30xi32>) -> tensor<30x30xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<3x3xi32>) outs(%output: tensor<30x30xi32>) -> tensor<30x30xi32>\n  return %ret : tensor<30x30xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x30xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<3x3xi32>, tensor<30x30xi32>) -> (tensor<30x30xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>_436": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x256xi32>) -> tensor<32x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256xi32>\n    memref.copy %2, %alloc : memref<32x256xi32> to memref<32x256xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256xi32>\n    return %3 : tensor<32x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x256xi32>) -> tensor<32x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<1x1xi32>) outs(%output: tensor<32x256xi32>) -> tensor<32x256xi32>\n  return %ret : tensor<32x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<1x1xi32>, tensor<32x256xi32>) -> (tensor<32x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>_437": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26xi32>\n    memref.copy %2, %alloc : memref<1018x26xi32> to memref<1018x26xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26xi32>\n    return %3 : tensor<1018x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x26xi32>) -> tensor<1018x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x32xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x26xi32>) -> tensor<1018x26xi32>\n  return %ret : tensor<1018x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32xi32>, tensor<7x7xi32>, tensor<1018x26xi32>) -> (tensor<1018x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>_438": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x128xi32>) -> tensor<64x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128xi32>\n    memref.copy %2, %alloc : memref<64x128xi32> to memref<64x128xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128xi32>\n    return %3 : tensor<64x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x128xi32>) -> tensor<64x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<1x1xi32>) outs(%output: tensor<64x128xi32>) -> tensor<64x128xi32>\n  return %ret : tensor<64x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<1x1xi32>, tensor<64x128xi32>) -> (tensor<64x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>_439": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x126xi32>) -> tensor<126x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126xi32>\n    memref.copy %2, %alloc : memref<126x126xi32> to memref<126x126xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126xi32>\n    return %3 : tensor<126x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<3x3xi32>, tensor<126x126xi32>) -> (tensor<126x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>_440": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x62xi32>) -> tensor<510x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62xi32>\n    memref.copy %2, %alloc : memref<510x62xi32> to memref<510x62xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62xi32>\n    return %3 : tensor<510x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x62xi32>) -> tensor<510x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x64xi32>, tensor<3x3xi32>) outs(%output: tensor<510x62xi32>) -> tensor<510x62xi32>\n  return %ret : tensor<510x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64xi32>, tensor<3x3xi32>, tensor<510x62xi32>) -> (tensor<510x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>_441": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x510xi32>) -> tensor<30x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x510xi32>\n    memref.copy %2, %alloc : memref<30x510xi32> to memref<30x510xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x510xi32>\n    return %3 : tensor<30x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x510xi32>) -> tensor<30x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<3x3xi32>) outs(%output: tensor<30x510xi32>) -> tensor<30x510xi32>\n  return %ret : tensor<30x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<3x3xi32>, tensor<30x510xi32>) -> (tensor<30x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>_442": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x28xi32>) -> tensor<28x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28xi32>\n    memref.copy %2, %alloc : memref<28x28xi32> to memref<28x28xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28xi32>\n    return %3 : tensor<28x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x28xi32>) -> tensor<28x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<5x5xi32>) outs(%output: tensor<28x28xi32>) -> tensor<28x28xi32>\n  return %ret : tensor<28x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<5x5xi32>, tensor<28x28xi32>) -> (tensor<28x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>_443": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x506xi32>) -> tensor<58x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506xi32>\n    memref.copy %2, %alloc : memref<58x506xi32> to memref<58x506xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506xi32>\n    return %3 : tensor<58x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x506xi32>) -> tensor<58x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<7x7xi32>) outs(%output: tensor<58x506xi32>) -> tensor<58x506xi32>\n  return %ret : tensor<58x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<7x7xi32>, tensor<58x506xi32>) -> (tensor<58x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>_444": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<510x510xi32>) -> tensor<510x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510xi32>\n    memref.copy %2, %alloc : memref<510x510xi32> to memref<510x510xi32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<510x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<510x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510xi32>\n    return %3 : tensor<510x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<510x510xi32>) -> tensor<510x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x512xi32>, tensor<3x3xi32>) outs(%output: tensor<510x510xi32>) -> tensor<510x510xi32>\n  return %ret : tensor<510x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<510x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512xi32>, tensor<3x3xi32>, tensor<510x510xi32>) -> (tensor<510x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>_445": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020xi32>\n    memref.copy %2, %alloc : memref<28x1020xi32> to memref<28x1020xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020xi32>\n    return %3 : tensor<28x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x1020xi32>) -> tensor<28x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<28x1020xi32>) -> tensor<28x1020xi32>\n  return %ret : tensor<28x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<5x5xi32>, tensor<28x1020xi32>) -> (tensor<28x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>_446": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022xi32>\n    memref.copy %2, %alloc : memref<30x1022xi32> to memref<30x1022xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022xi32>\n    return %3 : tensor<30x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x1022xi32>) -> tensor<30x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<30x1022xi32>) -> tensor<30x1022xi32>\n  return %ret : tensor<30x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<3x3xi32>, tensor<30x1022xi32>) -> (tensor<30x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>_447": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x254xi32>) -> tensor<30x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254xi32>\n    memref.copy %2, %alloc : memref<30x254xi32> to memref<30x254xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254xi32>\n    return %3 : tensor<30x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x254xi32>) -> tensor<30x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<3x3xi32>) outs(%output: tensor<30x254xi32>) -> tensor<30x254xi32>\n  return %ret : tensor<30x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<3x3xi32>, tensor<30x254xi32>) -> (tensor<30x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>_448": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126xi32>\n    memref.copy %2, %alloc : memref<1022x126xi32> to memref<1022x126xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126xi32>\n    return %3 : tensor<1022x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<3x3xi32>, tensor<1022x126xi32>) -> (tensor<1022x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>_449": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<506x26xi32>) -> tensor<506x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26xi32>\n    memref.copy %2, %alloc : memref<506x26xi32> to memref<506x26xi32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<506x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<506x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26xi32>\n    return %3 : tensor<506x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<506x26xi32>) -> tensor<506x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x32xi32>, tensor<7x7xi32>) outs(%output: tensor<506x26xi32>) -> tensor<506x26xi32>\n  return %ret : tensor<506x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<506x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32xi32>, tensor<7x7xi32>, tensor<506x26xi32>) -> (tensor<506x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_450": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>_451": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x508xi32>) -> tensor<28x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508xi32>\n    memref.copy %2, %alloc : memref<28x508xi32> to memref<28x508xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508xi32>\n    return %3 : tensor<28x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x508xi32>) -> tensor<28x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x512xi32>, tensor<5x5xi32>) outs(%output: tensor<28x508xi32>) -> tensor<28x508xi32>\n  return %ret : tensor<28x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512xi32>, tensor<5x5xi32>, tensor<28x508xi32>) -> (tensor<28x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>_452": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x58xi32>) -> tensor<58x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58xi32>\n    memref.copy %2, %alloc : memref<58x58xi32> to memref<58x58xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58xi32>\n    return %3 : tensor<58x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x58xi32>) -> tensor<58x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x64xi32>, tensor<7x7xi32>) outs(%output: tensor<58x58xi32>) -> tensor<58x58xi32>\n  return %ret : tensor<58x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64xi32>, tensor<7x7xi32>, tensor<58x58xi32>) -> (tensor<58x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>_453": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020xi32>\n    memref.copy %2, %alloc : memref<124x1020xi32> to memref<124x1020xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020xi32>\n    return %3 : tensor<124x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x1020xi32>) -> tensor<124x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<124x1020xi32>) -> tensor<124x1020xi32>\n  return %ret : tensor<124x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<5x5xi32>, tensor<124x1020xi32>) -> (tensor<124x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_454": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<5x5xi32>) outs(%output: tensor<28x124xi32>) -> tensor<28x124xi32>_455": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<5x5xi32>) outs(%output: tensor<28x124xi32>) -> tensor<28x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x124xi32>) -> tensor<28x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<5x5xi32>) outs(%output: tensor<28x124xi32>) -> tensor<28x124xi32>\n  return %ret : tensor<28x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x124xi32>) -> tensor<28x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x124xi32>\n    memref.copy %2, %alloc : memref<28x124xi32> to memref<28x124xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x124xi32>\n    return %3 : tensor<28x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x124xi32>) -> tensor<28x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x128xi32>, tensor<5x5xi32>) outs(%output: tensor<28x124xi32>) -> tensor<28x124xi32>\n  return %ret : tensor<28x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128xi32>, tensor<5x5xi32>, tensor<28x124xi32>) -> (tensor<28x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>_456": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020xi32>\n    memref.copy %2, %alloc : memref<1020x1020xi32> to memref<1020x1020xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020xi32>\n    return %3 : tensor<1020x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<5x5xi32>, tensor<1020x1020xi32>) -> (tensor<1020x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>_457": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x510xi32>) -> tensor<62x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510xi32>\n    memref.copy %2, %alloc : memref<62x510xi32> to memref<62x510xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510xi32>\n    return %3 : tensor<62x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x510xi32>) -> tensor<62x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<3x3xi32>) outs(%output: tensor<62x510xi32>) -> tensor<62x510xi32>\n  return %ret : tensor<62x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<3x3xi32>, tensor<62x510xi32>) -> (tensor<62x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_458": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>_459": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x512xi32>\n    memref.copy %2, %alloc : memref<1024x512xi32> to memref<1024x512xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x512xi32>\n    return %3 : tensor<1024x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x512xi32>) -> tensor<1024x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x512xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x512xi32>) -> tensor<1024x512xi32>\n  return %ret : tensor<1024x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512xi32>, tensor<1x1xi32>, tensor<1024x512xi32>) -> (tensor<1024x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>_460": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<28x252xi32>) -> tensor<28x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252xi32>\n    memref.copy %2, %alloc : memref<28x252xi32> to memref<28x252xi32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<28x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<28x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252xi32>\n    return %3 : tensor<28x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<28x252xi32>) -> tensor<28x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x256xi32>, tensor<5x5xi32>) outs(%output: tensor<28x252xi32>) -> tensor<28x252xi32>\n  return %ret : tensor<28x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<28x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256xi32>, tensor<5x5xi32>, tensor<28x252xi32>) -> (tensor<28x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>_461": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x254xi32>) -> tensor<62x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>\n  return %ret : tensor<62x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<62x254xi32>) -> tensor<62x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254xi32>\n    memref.copy %2, %alloc : memref<62x254xi32> to memref<62x254xi32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<62x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<62x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254xi32>\n    return %3 : tensor<62x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<62x254xi32>) -> tensor<62x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<3x3xi32>) outs(%output: tensor<62x254xi32>) -> tensor<62x254xi32>\n  return %ret : tensor<62x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<62x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<3x3xi32>, tensor<62x254xi32>) -> (tensor<62x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>_462": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018xi32>\n    memref.copy %2, %alloc : memref<58x1018xi32> to memref<58x1018xi32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<58x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<58x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018xi32>\n    return %3 : tensor<58x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<58x1018xi32>) -> tensor<58x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<58x1018xi32>) -> tensor<58x1018xi32>\n  return %ret : tensor<58x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<58x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024xi32>, tensor<7x7xi32>, tensor<58x1018xi32>) -> (tensor<58x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>_463": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024xi32>\n    memref.copy %2, %alloc : memref<256x1024xi32> to memref<256x1024xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024xi32>\n    return %3 : tensor<256x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x1024xi32>) -> tensor<256x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<256x1024xi32>) -> tensor<256x1024xi32>\n  return %ret : tensor<256x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024xi32>, tensor<1x1xi32>, tensor<256x1024xi32>) -> (tensor<256x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>_464": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x510xi32>) -> tensor<126x510xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x510xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x510xi32>\n    memref.copy %2, %alloc : memref<126x510xi32> to memref<126x510xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x510xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x510xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x510xi32>\n    return %3 : tensor<126x510xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x510xi32>) -> tensor<126x510xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<3x3xi32>) outs(%output: tensor<126x510xi32>) -> tensor<126x510xi32>\n  return %ret : tensor<126x510xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c510) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x510xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<3x3xi32>, tensor<126x510xi32>) -> (tensor<126x510xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_465": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>_466": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x1022xi32>\n    memref.copy %2, %alloc : memref<126x1022xi32> to memref<126x1022xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x1022xi32>\n    return %3 : tensor<126x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<3x3xi32>, tensor<126x1022xi32>) -> (tensor<126x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>_467": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126xi32>\n    memref.copy %2, %alloc : memref<1022x126xi32> to memref<1022x126xi32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1022x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1022x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126xi32>\n    return %3 : tensor<1022x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<1022x126xi32>) -> tensor<1022x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x128xi32>, tensor<3x3xi32>) outs(%output: tensor<1022x126xi32>) -> tensor<1022x126xi32>\n  return %ret : tensor<1022x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1022x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128xi32>, tensor<3x3xi32>, tensor<1022x126xi32>) -> (tensor<1022x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>_468": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x254xi32>) -> tensor<126x254xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254xi32>\n    memref.copy %2, %alloc : memref<126x254xi32> to memref<126x254xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x254xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x254xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254xi32>\n    return %3 : tensor<126x254xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x254xi32>) -> tensor<126x254xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x256xi32>, tensor<3x3xi32>) outs(%output: tensor<126x254xi32>) -> tensor<126x254xi32>\n  return %ret : tensor<126x254xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x254xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256xi32>, tensor<3x3xi32>, tensor<126x254xi32>) -> (tensor<126x254xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>_469": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60xi32>\n    memref.copy %2, %alloc : memref<1020x60xi32> to memref<1020x60xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60xi32>\n    return %3 : tensor<1020x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x60xi32>) -> tensor<1020x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x60xi32>) -> tensor<1020x60xi32>\n  return %ret : tensor<1020x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<5x5xi32>, tensor<1020x60xi32>) -> (tensor<1020x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>_470": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x124xi32>) -> tensor<60x124xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124xi32>\n    memref.copy %2, %alloc : memref<60x124xi32> to memref<60x124xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x124xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x124xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124xi32>\n    return %3 : tensor<60x124xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x124xi32>) -> tensor<60x124xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x128xi32>, tensor<5x5xi32>) outs(%output: tensor<60x124xi32>) -> tensor<60x124xi32>\n  return %ret : tensor<60x124xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x124xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128xi32>, tensor<5x5xi32>, tensor<60x124xi32>) -> (tensor<60x124xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>_471": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<32x64xi32>) -> tensor<32x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64xi32>\n    memref.copy %2, %alloc : memref<32x64xi32> to memref<32x64xi32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<32x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<32x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64xi32>\n    return %3 : tensor<32x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<32x64xi32>) -> tensor<32x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<1x1xi32>) outs(%output: tensor<32x64xi32>) -> tensor<32x64xi32>\n  return %ret : tensor<32x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<1x1xi32>, tensor<32x64xi32>) -> (tensor<32x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>_472": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x508xi32>) -> tensor<124x508xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508xi32>\n    memref.copy %2, %alloc : memref<124x508xi32> to memref<124x508xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x508xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x508xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508xi32>\n    return %3 : tensor<124x508xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x508xi32>) -> tensor<124x508xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x512xi32>, tensor<5x5xi32>) outs(%output: tensor<124x508xi32>) -> tensor<124x508xi32>\n  return %ret : tensor<124x508xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x508xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512xi32>, tensor<5x5xi32>, tensor<124x508xi32>) -> (tensor<124x508xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>_473": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<124x60xi32>) -> tensor<124x60xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60xi32>\n    memref.copy %2, %alloc : memref<124x60xi32> to memref<124x60xi32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<124x60xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<124x60xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60xi32>\n    return %3 : tensor<124x60xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<5x5xi32>, %output: tensor<124x60xi32>) -> tensor<124x60xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<5x5xi32>) outs(%output: tensor<124x60xi32>) -> tensor<124x60xi32>\n  return %ret : tensor<124x60xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<124x60xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<5x5xi32>, tensor<124x60xi32>) -> (tensor<124x60xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_474": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_475": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>_476": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>\n  return %ret : tensor<1024x1024xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x1024xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x1024xi32>\n    memref.copy %2, %alloc : memref<1024x1024xi32> to memref<1024x1024xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x1024xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x1024xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x1024xi32>\n    return %3 : tensor<1024x1024xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x1024xi32>) -> tensor<1024x1024xi32>\n  return %ret : tensor<1024x1024xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<1x1xi32>, tensor<1024x1024xi32>) -> (tensor<1024x1024xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>_477": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020xi32>\n    memref.copy %2, %alloc : memref<508x1020xi32> to memref<508x1020xi32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<508x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<508x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020xi32>\n    return %3 : tensor<508x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<508x1020xi32>) -> tensor<508x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<508x1020xi32>) -> tensor<508x1020xi32>\n  return %ret : tensor<508x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<508x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024xi32>, tensor<5x5xi32>, tensor<508x1020xi32>) -> (tensor<508x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>_478": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>\n  return %ret : tensor<26x1018xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x1018xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x1018xi32>\n    memref.copy %2, %alloc : memref<26x1018xi32> to memref<26x1018xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x1018xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x1018xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x1018xi32>\n    return %3 : tensor<26x1018xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x1018xi32>) -> tensor<26x1018xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x1024xi32>, tensor<7x7xi32>) outs(%output: tensor<26x1018xi32>) -> tensor<26x1018xi32>\n  return %ret : tensor<26x1018xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c1018) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x1018xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024xi32>, tensor<7x7xi32>, tensor<26x1018xi32>) -> (tensor<26x1018xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>_479": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<254x126xi32>) -> tensor<254x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126xi32>\n    memref.copy %2, %alloc : memref<254x126xi32> to memref<254x126xi32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<254x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<254x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126xi32>\n    return %3 : tensor<254x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<254x126xi32>) -> tensor<254x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<3x3xi32>) outs(%output: tensor<254x126xi32>) -> tensor<254x126xi32>\n  return %ret : tensor<254x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<254x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<3x3xi32>, tensor<254x126xi32>) -> (tensor<254x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>_480": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x128xi32>) -> tensor<256x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xi32>\n    memref.copy %2, %alloc : memref<256x128xi32> to memref<256x128xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128xi32>\n    return %3 : tensor<256x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<1x1xi32>, tensor<256x128xi32>) -> (tensor<256x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>_481": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x128xi32>) -> tensor<256x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128xi32>\n    memref.copy %2, %alloc : memref<256x128xi32> to memref<256x128xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128xi32>\n    return %3 : tensor<256x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x128xi32>) -> tensor<256x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x128xi32>, tensor<1x1xi32>) outs(%output: tensor<256x128xi32>) -> tensor<256x128xi32>\n  return %ret : tensor<256x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128xi32>, tensor<1x1xi32>, tensor<256x128xi32>) -> (tensor<256x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>_482": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x58xi32>) -> tensor<26x58xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58xi32>\n    memref.copy %2, %alloc : memref<26x58xi32> to memref<26x58xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x58xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x58xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58xi32>\n    return %3 : tensor<26x58xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x58xi32>) -> tensor<26x58xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<7x7xi32>) outs(%output: tensor<26x58xi32>) -> tensor<26x58xi32>\n  return %ret : tensor<26x58xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x58xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<7x7xi32>, tensor<26x58xi32>) -> (tensor<26x58xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>_483": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x64xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x64xi32>\n    memref.copy %2, %alloc : memref<1024x64xi32> to memref<1024x64xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x64xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x64xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x64xi32>\n    return %3 : tensor<1024x64xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x64xi32>) -> tensor<1024x64xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x64xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x64xi32>) -> tensor<1024x64xi32>\n  return %ret : tensor<1024x64xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c64) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x64xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64xi32>, tensor<1x1xi32>, tensor<1024x64xi32>) -> (tensor<1024x64xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>_484": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<256x32xi32>) -> tensor<256x32xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32xi32>\n    memref.copy %2, %alloc : memref<256x32xi32> to memref<256x32xi32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<256x32xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<256x32xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32xi32>\n    return %3 : tensor<256x32xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<1x1xi32>, %output: tensor<256x32xi32>) -> tensor<256x32xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<1x1xi32>) outs(%output: tensor<256x32xi32>) -> tensor<256x32xi32>\n  return %ret : tensor<256x32xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<1x1xi32>, tensor<256x32xi32>) -> (tensor<256x32xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>_485": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>\n  return %ret : tensor<1020x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x252xi32>\n    memref.copy %2, %alloc : memref<1020x252xi32> to memref<1020x252xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x252xi32>\n    return %3 : tensor<1020x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x252xi32>) -> tensor<1020x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x252xi32>) -> tensor<1020x252xi32>\n  return %ret : tensor<1020x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<5x5xi32>, tensor<1020x252xi32>) -> (tensor<1020x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>_486": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<64x512xi32>) -> tensor<64x512xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512xi32>\n    memref.copy %2, %alloc : memref<64x512xi32> to memref<64x512xi32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<64x512xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<64x512xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512xi32>\n    return %3 : tensor<64x512xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512xi32>, %filter: tensor<1x1xi32>, %output: tensor<64x512xi32>) -> tensor<64x512xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x512xi32>, tensor<1x1xi32>) outs(%output: tensor<64x512xi32>) -> tensor<64x512xi32>\n  return %ret : tensor<64x512xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<64x512xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512xi32>, tensor<1x1xi32>, tensor<64x512xi32>) -> (tensor<64x512xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>_487": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<60x252xi32>) -> tensor<60x252xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<60x252xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x252xi32>\n    memref.copy %2, %alloc : memref<60x252xi32> to memref<60x252xi32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<64x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<60x252xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<60x252xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x252xi32>\n    return %3 : tensor<60x252xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256xi32>, %filter: tensor<5x5xi32>, %output: tensor<60x252xi32>) -> tensor<60x252xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<64x256xi32>, tensor<5x5xi32>) outs(%output: tensor<60x252xi32>) -> tensor<60x252xi32>\n  return %ret : tensor<60x252xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<64x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c252) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<60x252xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256xi32>, tensor<5x5xi32>, tensor<60x252xi32>) -> (tensor<60x252xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>_488": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x28xi32>) -> tensor<252x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28xi32>\n    memref.copy %2, %alloc : memref<252x28xi32> to memref<252x28xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28xi32>\n    return %3 : tensor<252x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<5x5xi32>, tensor<252x28xi32>) -> (tensor<252x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>_489": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020xi32>\n    memref.copy %2, %alloc : memref<1020x1020xi32> to memref<1020x1020xi32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1020x1020xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020xi32>\n    return %3 : tensor<1020x1020xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024xi32>, %filter: tensor<5x5xi32>, %output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x1024xi32>, tensor<5x5xi32>) outs(%output: tensor<1020x1020xi32>) -> tensor<1020x1020xi32>\n  return %ret : tensor<1020x1020xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1020x1020xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024xi32>, tensor<5x5xi32>, tensor<1020x1020xi32>) -> (tensor<1020x1020xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>_490": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<250x506xi32>) -> tensor<250x506xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512xi32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506xi32>\n    memref.copy %2, %alloc : memref<250x506xi32> to memref<250x506xi32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x512xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<250x506xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<250x506xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506xi32>\n    return %3 : tensor<250x506xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512xi32>, %filter: tensor<7x7xi32>, %output: tensor<250x506xi32>) -> tensor<250x506xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x512xi32>, tensor<7x7xi32>) outs(%output: tensor<250x506xi32>) -> tensor<250x506xi32>\n  return %ret : tensor<250x506xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x512xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<250x506xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512xi32>, tensor<7x7xi32>, tensor<250x506xi32>) -> (tensor<250x506xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>_491": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<512x128xi32>) -> tensor<512x128xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128xi32>\n    memref.copy %2, %alloc : memref<512x128xi32> to memref<512x128xi32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<512x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<512x128xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<512x128xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128xi32>\n    return %3 : tensor<512x128xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128xi32>, %filter: tensor<1x1xi32>, %output: tensor<512x128xi32>) -> tensor<512x128xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<512x128xi32>, tensor<1x1xi32>) outs(%output: tensor<512x128xi32>) -> tensor<512x128xi32>\n  return %ret : tensor<512x128xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<512x128xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128xi32>, tensor<1x1xi32>, tensor<512x128xi32>) -> (tensor<512x128xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>_492": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250xi32>\n    memref.copy %2, %alloc : memref<1018x250xi32> to memref<1018x250xi32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1018x250xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1018x250xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250xi32>\n    return %3 : tensor<1018x250xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<7x7xi32>, %output: tensor<1018x250xi32>) -> tensor<1018x250xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<7x7xi32>) outs(%output: tensor<1018x250xi32>) -> tensor<1018x250xi32>\n  return %ret : tensor<1018x250xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1018x250xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<7x7xi32>, tensor<1018x250xi32>) -> (tensor<1018x250xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>_493": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256xi32>, %arg1: tensor<1x1xi32>, %arg2: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1xi32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256xi32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256xi32>\n    memref.copy %2, %alloc : memref<1024x256xi32> to memref<1024x256xi32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<1024x256xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<1x1xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<1024x256xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<1024x256xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256xi32>\n    return %3 : tensor<1024x256xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256xi32>, %filter: tensor<1x1xi32>, %output: tensor<1024x256xi32>) -> tensor<1024x256xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<1024x256xi32>, tensor<1x1xi32>) outs(%output: tensor<1024x256xi32>) -> tensor<1024x256xi32>\n  return %ret : tensor<1024x256xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<1x1xi32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<1024x256xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256xi32>, tensor<1x1xi32>, tensor<1024x256xi32>) -> (tensor<1024x256xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>_494": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32xi32>, %arg1: tensor<5x5xi32>, %arg2: tensor<252x28xi32>) -> tensor<252x28xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5xi32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28xi32>\n    memref.copy %2, %alloc : memref<252x28xi32> to memref<252x28xi32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 5 {\n          affine.for %arg6 = 0 to 5 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<256x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<5x5xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<252x28xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<252x28xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28xi32>\n    return %3 : tensor<252x28xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32xi32>, %filter: tensor<5x5xi32>, %output: tensor<252x28xi32>) -> tensor<252x28xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<256x32xi32>, tensor<5x5xi32>) outs(%output: tensor<252x28xi32>) -> tensor<252x28xi32>\n  return %ret : tensor<252x28xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<256x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<5x5xi32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<252x28xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32xi32>, tensor<5x5xi32>, tensor<252x28xi32>) -> (tensor<252x28xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 5, 1], ["%arg6", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>_495": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x26xi32>) -> tensor<26x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>\n  return %ret : tensor<26x26xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32xi32>, %arg1: tensor<7x7xi32>, %arg2: tensor<26x26xi32>) -> tensor<26x26xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32xi32>\n    %2 = bufferization.to_memref %arg2 : memref<26x26xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x26xi32>\n    memref.copy %2, %alloc : memref<26x26xi32> to memref<26x26xi32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 7 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x32xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<7x7xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<26x26xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<26x26xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x26xi32>\n    return %3 : tensor<26x26xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32xi32>, %filter: tensor<7x7xi32>, %output: tensor<26x26xi32>) -> tensor<26x26xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x32xi32>, tensor<7x7xi32>) outs(%output: tensor<26x26xi32>) -> tensor<26x26xi32>\n  return %ret : tensor<26x26xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x32xi32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<7x7xi32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c26) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<26x26xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32xi32>, tensor<7x7xi32>, tensor<26x26xi32>) -> (tensor<26x26xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>_496": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x1022xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x1022xi32>\n    memref.copy %2, %alloc : memref<126x1022xi32> to memref<126x1022xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x1024xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x1022xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x1022xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x1022xi32>\n    return %3 : tensor<126x1022xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x1022xi32>) -> tensor<126x1022xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x1024xi32>, tensor<3x3xi32>) outs(%output: tensor<126x1022xi32>) -> tensor<126x1022xi32>\n  return %ret : tensor<126x1022xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x1024xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c1022) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x1022xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024xi32>, tensor<3x3xi32>, tensor<126x1022xi32>) -> (tensor<126x1022xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>_497": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x126xi32>) -> tensor<126x126xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126xi32>\n    memref.copy %2, %alloc : memref<126x126xi32> to memref<126x126xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x128xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x126xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x126xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126xi32>\n    return %3 : tensor<126x126xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x126xi32>) -> tensor<126x126xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x128xi32>, tensor<3x3xi32>) outs(%output: tensor<126x126xi32>) -> tensor<126x126xi32>\n  return %ret : tensor<126x126xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x128xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x126xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128xi32>, tensor<3x3xi32>, tensor<126x126xi32>) -> (tensor<126x126xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>_498": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<126x62xi32>) -> tensor<126x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62xi32>\n    memref.copy %2, %alloc : memref<126x62xi32> to memref<126x62xi32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<128x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<126x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<126x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62xi32>\n    return %3 : tensor<126x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<126x62xi32>) -> tensor<126x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<128x64xi32>, tensor<3x3xi32>) outs(%output: tensor<126x62xi32>) -> tensor<126x62xi32>\n  return %ret : tensor<126x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<128x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<126x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64xi32>, tensor<3x3xi32>, tensor<126x62xi32>) -> (tensor<126x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>_499": {"operation": "linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64xi32>, %arg1: tensor<3x3xi32>, %arg2: tensor<30x62xi32>) -> tensor<30x62xi32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3xi32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64xi32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62xi32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62xi32>\n    memref.copy %2, %alloc : memref<30x62xi32> to memref<30x62xi32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 3 {\n          affine.for %arg6 = 0 to 3 {\n            %4 = affine.apply #map(%arg3, %arg5)\n            %5 = affine.apply #map(%arg4, %arg6)\n            %6 = affine.load %1[%4, %5] : memref<32x64xi32>\n            %7 = affine.load %0[%arg5, %arg6] : memref<3x3xi32>\n            %8 = affine.load %alloc[%arg3, %arg4] : memref<30x62xi32>\n            %9 = arith.muli %6, %7 : i32\n            %10 = arith.addi %8, %9 : i32\n            affine.store %10, %alloc[%arg3, %arg4] : memref<30x62xi32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62xi32>\n    return %3 : tensor<30x62xi32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64xi32>, %filter: tensor<3x3xi32>, %output: tensor<30x62xi32>) -> tensor<30x62xi32> {\n  %ret = linalg.conv_2d ins(%input, %filter: tensor<32x64xi32>, tensor<3x3xi32>) outs(%output: tensor<30x62xi32>) -> tensor<30x62xi32>\n  return %ret : tensor<30x62xi32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64) : tensor<?x?xi32>\n  %input = tensor.cast %input_temp : tensor<?x?xi32> to tensor<32x64xi32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3) : tensor<?x?xi32>\n  %filter = tensor.cast %filter_temp : tensor<?x?xi32> to tensor<3x3xi32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62) : tensor<?x?xi32>\n  %output = tensor.cast %output_temp : tensor<?x?xi32> to tensor<30x62xi32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64xi32>, tensor<3x3xi32>, tensor<30x62xi32>) -> (tensor<30x62xi32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 3, 1], ["%arg6", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg5", "%arg4 + %arg6"], ["%arg5", "%arg6"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>_0": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>\n  return %ret : tensor<58x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x1018xf32>\n    memref.copy %2, %alloc : memref<58x122x1018xf32> to memref<58x122x1018xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x1018xf32>\n    return %3 : tensor<58x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>\n  return %ret : tensor<58x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x1024xf32>, tensor<7x7x7xf32>, tensor<58x122x1018xf32>) -> (tensor<58x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32>_1": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32>\n  return %ret : tensor<254x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x30x62xf32>) -> tensor<254x30x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x30x62xf32>\n    memref.copy %2, %alloc : memref<254x30x62xf32> to memref<254x30x62xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x30x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x30x62xf32>\n    return %3 : tensor<254x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x62xf32>) -> tensor<254x30x62xf32>\n  return %ret : tensor<254x30x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c30, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x30x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x64xf32>, tensor<3x3x3xf32>, tensor<254x30x62xf32>) -> (tensor<254x30x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32>_2": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32>\n  return %ret : tensor<254x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x254xf32>) -> tensor<254x254x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x254xf32>\n    memref.copy %2, %alloc : memref<254x254x254xf32> to memref<254x254x254xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x254xf32>\n    return %3 : tensor<254x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x254xf32>) -> tensor<254x254x254xf32>\n  return %ret : tensor<254x254x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x256xf32>, tensor<3x3x3xf32>, tensor<254x254x254xf32>) -> (tensor<254x254x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32>_3": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32>\n  return %ret : tensor<32x512x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x512x64xf32>) -> tensor<32x512x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x512x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x64xf32>\n    memref.copy %2, %alloc : memref<32x512x64xf32> to memref<32x512x64xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x512x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x512x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x512x64xf32>\n    return %3 : tensor<32x512x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x512x64xf32>) -> tensor<32x512x64xf32>\n  return %ret : tensor<32x512x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x64xf32>, tensor<1x1x1xf32>, tensor<32x512x64xf32>) -> (tensor<32x512x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32>_4": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32>\n  return %ret : tensor<1022x1022x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x1022x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x1022x254xf32>\n    memref.copy %2, %alloc : memref<1022x1022x254xf32> to memref<1022x1022x254xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x1022x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x1022x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x1022x254xf32>\n    return %3 : tensor<1022x1022x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x254xf32>) -> tensor<1022x1022x254xf32>\n  return %ret : tensor<1022x1022x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c1022, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x1022x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x256xf32>, tensor<3x3x3xf32>, tensor<1022x1022x254xf32>) -> (tensor<1022x1022x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32>_5": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32>\n  return %ret : tensor<122x506x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x506x122xf32>) -> tensor<122x506x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506x122xf32>\n    memref.copy %2, %alloc : memref<122x506x122xf32> to memref<122x506x122xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x506x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x506x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506x122xf32>\n    return %3 : tensor<122x506x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x122xf32>) -> tensor<122x506x122xf32>\n  return %ret : tensor<122x506x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x506x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x128xf32>, tensor<7x7x7xf32>, tensor<122x506x122xf32>) -> (tensor<122x506x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>_6": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>\n  return %ret : tensor<508x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28x60xf32>\n    memref.copy %2, %alloc : memref<508x28x60xf32> to memref<508x28x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28x60xf32>\n    return %3 : tensor<508x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>\n  return %ret : tensor<508x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x64xf32>, tensor<5x5x5xf32>, tensor<508x28x60xf32>) -> (tensor<508x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32>_7": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32>\n  return %ret : tensor<58x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x58xf32>) -> tensor<58x122x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x58xf32>\n    memref.copy %2, %alloc : memref<58x122x58xf32> to memref<58x122x58xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x58xf32>\n    return %3 : tensor<58x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x58xf32>) -> tensor<58x122x58xf32>\n  return %ret : tensor<58x122x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x64xf32>, tensor<7x7x7xf32>, tensor<58x122x58xf32>) -> (tensor<58x122x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32>_8": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32>\n  return %ret : tensor<62x510x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x510x254xf32>) -> tensor<62x510x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510x254xf32>\n    memref.copy %2, %alloc : memref<62x510x254xf32> to memref<62x510x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x510x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x510x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510x254xf32>\n    return %3 : tensor<62x510x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x254xf32>) -> tensor<62x510x254xf32>\n  return %ret : tensor<62x510x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x510x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x256xf32>, tensor<3x3x3xf32>, tensor<62x510x254xf32>) -> (tensor<62x510x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32>_9": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32>\n  return %ret : tensor<122x26x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x26x122xf32>) -> tensor<122x26x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26x122xf32>\n    memref.copy %2, %alloc : memref<122x26x122xf32> to memref<122x26x122xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x26x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x26x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26x122xf32>\n    return %3 : tensor<122x26x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x122xf32>) -> tensor<122x26x122xf32>\n  return %ret : tensor<122x26x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x26x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x128xf32>, tensor<7x7x7xf32>, tensor<122x26x122xf32>) -> (tensor<122x26x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>_10": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>\n  return %ret : tensor<124x124x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x124x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x124x1020xf32>\n    memref.copy %2, %alloc : memref<124x124x1020xf32> to memref<124x124x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x124x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x124x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x124x1020xf32>\n    return %3 : tensor<124x124x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>\n  return %ret : tensor<124x124x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c124, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x124x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x1024xf32>, tensor<5x5x5xf32>, tensor<124x124x1020xf32>) -> (tensor<124x124x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32>_11": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32>\n  return %ret : tensor<252x1020x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x1020x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x1020x28xf32>\n    memref.copy %2, %alloc : memref<252x1020x28xf32> to memref<252x1020x28xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x1020x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x1020x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x1020x28xf32>\n    return %3 : tensor<252x1020x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x1020x28xf32>) -> tensor<252x1020x28xf32>\n  return %ret : tensor<252x1020x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c1020, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x1020x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x32xf32>, tensor<5x5x5xf32>, tensor<252x1020x28xf32>) -> (tensor<252x1020x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32>_12": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32>\n  return %ret : tensor<58x26x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x26x122xf32>) -> tensor<58x26x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x26x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x26x122xf32>\n    memref.copy %2, %alloc : memref<58x26x122xf32> to memref<58x26x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x26x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x26x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x26x122xf32>\n    return %3 : tensor<58x26x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x122xf32>) -> tensor<58x26x122xf32>\n  return %ret : tensor<58x26x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c26, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x26x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x128xf32>, tensor<7x7x7xf32>, tensor<58x26x122xf32>) -> (tensor<58x26x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32>_13": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32>\n  return %ret : tensor<124x60x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x60x508xf32>) -> tensor<124x60x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60x508xf32>\n    memref.copy %2, %alloc : memref<124x60x508xf32> to memref<124x60x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x60x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x60x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60x508xf32>\n    return %3 : tensor<124x60x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x508xf32>) -> tensor<124x60x508xf32>\n  return %ret : tensor<124x60x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x60x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x512xf32>, tensor<5x5x5xf32>, tensor<124x60x508xf32>) -> (tensor<124x60x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>_14": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>\n  return %ret : tensor<62x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x254xf32>\n    memref.copy %2, %alloc : memref<62x126x254xf32> to memref<62x126x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x254xf32>\n    return %3 : tensor<62x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>\n  return %ret : tensor<62x126x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<3x3x3xf32>, tensor<62x126x254xf32>) -> (tensor<62x126x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32>_15": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32>\n  return %ret : tensor<256x1024x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x256xf32>\n    memref.copy %2, %alloc : memref<256x1024x256xf32> to memref<256x1024x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x1024x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x1024x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024x256xf32>\n    return %3 : tensor<256x1024x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x256xf32>) -> tensor<256x1024x256xf32>\n  return %ret : tensor<256x1024x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x1024x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x256xf32>, tensor<1x1x1xf32>, tensor<256x1024x256xf32>) -> (tensor<256x1024x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32>_16": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32>\n  return %ret : tensor<254x254x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x126xf32>) -> tensor<254x254x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x126xf32>\n    memref.copy %2, %alloc : memref<254x254x126xf32> to memref<254x254x126xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x126xf32>\n    return %3 : tensor<254x254x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x126xf32>) -> tensor<254x254x126xf32>\n  return %ret : tensor<254x254x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x128xf32>, tensor<3x3x3xf32>, tensor<254x254x126xf32>) -> (tensor<254x254x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32>_17": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32>\n  return %ret : tensor<28x60x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x60x252xf32>) -> tensor<28x60x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60x252xf32>\n    memref.copy %2, %alloc : memref<28x60x252xf32> to memref<28x60x252xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x60x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x60x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60x252xf32>\n    return %3 : tensor<28x60x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x252xf32>) -> tensor<28x60x252xf32>\n  return %ret : tensor<28x60x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x60x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x256xf32>, tensor<5x5x5xf32>, tensor<28x60x252xf32>) -> (tensor<28x60x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>_18": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>\n  return %ret : tensor<26x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x1018x506xf32>\n    memref.copy %2, %alloc : memref<26x1018x506xf32> to memref<26x1018x506xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x1018x506xf32>\n    return %3 : tensor<26x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>\n  return %ret : tensor<26x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x512xf32>, tensor<7x7x7xf32>, tensor<26x1018x506xf32>) -> (tensor<26x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>_19": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x28xf32>\n    memref.copy %2, %alloc : memref<508x252x28xf32> to memref<508x252x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x28xf32>\n    return %3 : tensor<508x252x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<5x5x5xf32>, tensor<508x252x28xf32>) -> (tensor<508x252x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32>_20": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32>\n  return %ret : tensor<1020x508x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508x28xf32>\n    memref.copy %2, %alloc : memref<1020x508x28xf32> to memref<1020x508x28xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x508x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x508x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508x28xf32>\n    return %3 : tensor<1020x508x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x28xf32>) -> tensor<1020x508x28xf32>\n  return %ret : tensor<1020x508x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x508x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x32xf32>, tensor<5x5x5xf32>, tensor<1020x508x28xf32>) -> (tensor<1020x508x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>_21": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250x26xf32>\n    memref.copy %2, %alloc : memref<1018x250x26xf32> to memref<1018x250x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250x26xf32>\n    return %3 : tensor<1018x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<7x7x7xf32>, tensor<1018x250x26xf32>) -> (tensor<1018x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>_22": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x1024xf32>\n    memref.copy %2, %alloc : memref<256x256x1024xf32> to memref<256x256x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x1024xf32>\n    return %3 : tensor<256x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<1x1x1xf32>, tensor<256x256x1024xf32>) -> (tensor<256x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32>_23": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32>\n  return %ret : tensor<252x124x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x124x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x124x1020xf32>\n    memref.copy %2, %alloc : memref<252x124x1020xf32> to memref<252x124x1020xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x124x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x124x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x124x1020xf32>\n    return %3 : tensor<252x124x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x1020xf32>) -> tensor<252x124x1020xf32>\n  return %ret : tensor<252x124x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c124, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x124x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x1024xf32>, tensor<5x5x5xf32>, tensor<252x124x1020xf32>) -> (tensor<252x124x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>_24": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>\n  return %ret : tensor<250x506x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506x506xf32>\n    memref.copy %2, %alloc : memref<250x506x506xf32> to memref<250x506x506xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x506x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x506x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506x506xf32>\n    return %3 : tensor<250x506x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>\n  return %ret : tensor<250x506x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x506x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x512xf32>, tensor<7x7x7xf32>, tensor<250x506x506xf32>) -> (tensor<250x506x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32>_25": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32>\n  return %ret : tensor<128x256x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x256x512xf32>) -> tensor<128x256x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x512xf32>\n    memref.copy %2, %alloc : memref<128x256x512xf32> to memref<128x256x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x256x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x256x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x512xf32>\n    return %3 : tensor<128x256x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x512xf32>) -> tensor<128x256x512xf32>\n  return %ret : tensor<128x256x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x512xf32>, tensor<1x1x1xf32>, tensor<128x256x512xf32>) -> (tensor<128x256x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>_26": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>\n  return %ret : tensor<254x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126x126xf32>\n    memref.copy %2, %alloc : memref<254x126x126xf32> to memref<254x126x126xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126x126xf32>\n    return %3 : tensor<254x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>\n  return %ret : tensor<254x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x128xf32>, tensor<3x3x3xf32>, tensor<254x126x126xf32>) -> (tensor<254x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>_27": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>\n  return %ret : tensor<26x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x58xf32>\n    memref.copy %2, %alloc : memref<26x250x58xf32> to memref<26x250x58xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x58xf32>\n    return %3 : tensor<26x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>\n  return %ret : tensor<26x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x64xf32>, tensor<7x7x7xf32>, tensor<26x250x58xf32>) -> (tensor<26x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32>_28": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32>\n  return %ret : tensor<256x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x1024xf32>\n    memref.copy %2, %alloc : memref<256x32x1024xf32> to memref<256x32x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x1024xf32>\n    return %3 : tensor<256x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x1024xf32>) -> tensor<256x32x1024xf32>\n  return %ret : tensor<256x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x1024xf32>, tensor<1x1x1xf32>, tensor<256x32x1024xf32>) -> (tensor<256x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>_29": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>\n  return %ret : tensor<1022x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x510xf32>\n    memref.copy %2, %alloc : memref<1022x254x510xf32> to memref<1022x254x510xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x510xf32>\n    return %3 : tensor<1022x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>\n  return %ret : tensor<1022x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x512xf32>, tensor<3x3x3xf32>, tensor<1022x254x510xf32>) -> (tensor<1022x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>_30": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>\n  return %ret : tensor<1024x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x32x1024xf32>\n    memref.copy %2, %alloc : memref<1024x32x1024xf32> to memref<1024x32x1024xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x32x1024xf32>\n    return %3 : tensor<1024x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>\n  return %ret : tensor<1024x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x1024xf32>, tensor<1x1x1xf32>, tensor<1024x32x1024xf32>) -> (tensor<1024x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>_31": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>\n  return %ret : tensor<58x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250x58xf32>\n    memref.copy %2, %alloc : memref<58x250x58xf32> to memref<58x250x58xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250x58xf32>\n    return %3 : tensor<58x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>\n  return %ret : tensor<58x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x64xf32>, tensor<7x7x7xf32>, tensor<58x250x58xf32>) -> (tensor<58x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>_32": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x1024xf32>\n    memref.copy %2, %alloc : memref<256x256x1024xf32> to memref<256x256x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x1024xf32>\n    return %3 : tensor<256x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<1x1x1xf32>, tensor<256x256x1024xf32>) -> (tensor<256x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32>_33": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32>\n  return %ret : tensor<32x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x256x256xf32>) -> tensor<32x256x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256x256xf32>\n    memref.copy %2, %alloc : memref<32x256x256xf32> to memref<32x256x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x256x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x256x256xf32>\n    return %3 : tensor<32x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x256x256xf32>) -> tensor<32x256x256xf32>\n  return %ret : tensor<32x256x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x256xf32>, tensor<1x1x1xf32>, tensor<32x256x256xf32>) -> (tensor<32x256x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>_34": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>\n  return %ret : tensor<28x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508x60xf32>\n    memref.copy %2, %alloc : memref<28x508x60xf32> to memref<28x508x60xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508x60xf32>\n    return %3 : tensor<28x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>\n  return %ret : tensor<28x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x64xf32>, tensor<5x5x5xf32>, tensor<28x508x60xf32>) -> (tensor<28x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>_35": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>\n  return %ret : tensor<512x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x64x128xf32>\n    memref.copy %2, %alloc : memref<512x64x128xf32> to memref<512x64x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x64x128xf32>\n    return %3 : tensor<512x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>\n  return %ret : tensor<512x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x128xf32>, tensor<1x1x1xf32>, tensor<512x64x128xf32>) -> (tensor<512x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32>_36": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32>\n  return %ret : tensor<128x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x256x32xf32>) -> tensor<128x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x32xf32>\n    memref.copy %2, %alloc : memref<128x256x32xf32> to memref<128x256x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x32xf32>\n    return %3 : tensor<128x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x32xf32>) -> tensor<128x256x32xf32>\n  return %ret : tensor<128x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<1x1x1xf32>, tensor<128x256x32xf32>) -> (tensor<128x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32>_37": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32>\n  return %ret : tensor<510x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x126x62xf32>) -> tensor<510x126x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x126x62xf32>\n    memref.copy %2, %alloc : memref<510x126x62xf32> to memref<510x126x62xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x126x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x126x62xf32>\n    return %3 : tensor<510x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x62xf32>) -> tensor<510x126x62xf32>\n  return %ret : tensor<510x126x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c126, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x126x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x64xf32>, tensor<3x3x3xf32>, tensor<510x126x62xf32>) -> (tensor<510x126x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>_38": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>\n  return %ret : tensor<250x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018x122xf32>\n    memref.copy %2, %alloc : memref<250x1018x122xf32> to memref<250x1018x122xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018x122xf32>\n    return %3 : tensor<250x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>\n  return %ret : tensor<250x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x128xf32>, tensor<7x7x7xf32>, tensor<250x1018x122xf32>) -> (tensor<250x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32>_39": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32>\n  return %ret : tensor<124x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x60x124xf32>) -> tensor<124x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60x124xf32>\n    memref.copy %2, %alloc : memref<124x60x124xf32> to memref<124x60x124xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60x124xf32>\n    return %3 : tensor<124x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x124xf32>) -> tensor<124x60x124xf32>\n  return %ret : tensor<124x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x128xf32>, tensor<5x5x5xf32>, tensor<124x60x124xf32>) -> (tensor<124x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>_40": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x28xf32>\n    memref.copy %2, %alloc : memref<508x252x28xf32> to memref<508x252x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x28xf32>\n    return %3 : tensor<508x252x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<5x5x5xf32>, tensor<508x252x28xf32>) -> (tensor<508x252x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32>_41": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32>\n  return %ret : tensor<26x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x506x58xf32>) -> tensor<26x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x506x58xf32>\n    memref.copy %2, %alloc : memref<26x506x58xf32> to memref<26x506x58xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x506x58xf32>\n    return %3 : tensor<26x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x58xf32>) -> tensor<26x506x58xf32>\n  return %ret : tensor<26x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x64xf32>, tensor<7x7x7xf32>, tensor<26x506x58xf32>) -> (tensor<26x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32>_42": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32>\n  return %ret : tensor<256x1024x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x128xf32>\n    memref.copy %2, %alloc : memref<256x1024x128xf32> to memref<256x1024x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x1024x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x1024x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024x128xf32>\n    return %3 : tensor<256x1024x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x128xf32>) -> tensor<256x1024x128xf32>\n  return %ret : tensor<256x1024x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x1024x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x128xf32>, tensor<1x1x1xf32>, tensor<256x1024x128xf32>) -> (tensor<256x1024x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>_43": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>\n  return %ret : tensor<28x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x124x28xf32>\n    memref.copy %2, %alloc : memref<28x124x28xf32> to memref<28x124x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x124x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x124x28xf32>\n    return %3 : tensor<28x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>\n  return %ret : tensor<28x124x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c124, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x124x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x32xf32>, tensor<5x5x5xf32>, tensor<28x124x28xf32>) -> (tensor<28x124x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32>_44": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32>\n  return %ret : tensor<254x510x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x510x510xf32>) -> tensor<254x510x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x510x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x510x510xf32>\n    memref.copy %2, %alloc : memref<254x510x510xf32> to memref<254x510x510xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x510x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x510x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x510x510xf32>\n    return %3 : tensor<254x510x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x510xf32>) -> tensor<254x510x510xf32>\n  return %ret : tensor<254x510x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c510, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x510x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x512xf32>, tensor<3x3x3xf32>, tensor<254x510x510xf32>) -> (tensor<254x510x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>_45": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>\n  return %ret : tensor<28x252x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252x508xf32>\n    memref.copy %2, %alloc : memref<28x252x508xf32> to memref<28x252x508xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x252x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x252x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252x508xf32>\n    return %3 : tensor<28x252x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>\n  return %ret : tensor<28x252x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x252x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x512xf32>, tensor<5x5x5xf32>, tensor<28x252x508xf32>) -> (tensor<28x252x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32>_46": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32>\n  return %ret : tensor<506x250x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x250x250xf32>) -> tensor<506x250x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x250x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x250x250xf32>\n    memref.copy %2, %alloc : memref<506x250x250xf32> to memref<506x250x250xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x250x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x250x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x250x250xf32>\n    return %3 : tensor<506x250x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x250x250xf32>) -> tensor<506x250x250xf32>\n  return %ret : tensor<506x250x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c250, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x250x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x256xf32>, tensor<7x7x7xf32>, tensor<506x250x250xf32>) -> (tensor<506x250x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32>_47": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32>\n  return %ret : tensor<252x28x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x28x252xf32>) -> tensor<252x28x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28x252xf32>\n    memref.copy %2, %alloc : memref<252x28x252xf32> to memref<252x28x252xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x28x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x28x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28x252xf32>\n    return %3 : tensor<252x28x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x252xf32>) -> tensor<252x28x252xf32>\n  return %ret : tensor<252x28x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x28x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x256xf32>, tensor<5x5x5xf32>, tensor<252x28x252xf32>) -> (tensor<252x28x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32>_48": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32>\n  return %ret : tensor<64x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x256x256xf32>) -> tensor<64x256x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x256xf32>\n    memref.copy %2, %alloc : memref<64x256x256xf32> to memref<64x256x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x256x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256x256xf32>\n    return %3 : tensor<64x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x256xf32>) -> tensor<64x256x256xf32>\n  return %ret : tensor<64x256x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x256x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x256xf32>, tensor<1x1x1xf32>, tensor<64x256x256xf32>) -> (tensor<64x256x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32>_49": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32>\n  return %ret : tensor<62x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x62xf32>) -> tensor<62x126x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x62xf32>\n    memref.copy %2, %alloc : memref<62x126x62xf32> to memref<62x126x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x62xf32>\n    return %3 : tensor<62x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x62xf32>) -> tensor<62x126x62xf32>\n  return %ret : tensor<62x126x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x64xf32>, tensor<3x3x3xf32>, tensor<62x126x62xf32>) -> (tensor<62x126x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>_50": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>\n  return %ret : tensor<126x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254x30xf32>\n    memref.copy %2, %alloc : memref<126x254x30xf32> to memref<126x254x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254x30xf32>\n    return %3 : tensor<126x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>\n  return %ret : tensor<126x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<3x3x3xf32>, tensor<126x254x30xf32>) -> (tensor<126x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>_51": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x1022xf32>\n    memref.copy %2, %alloc : memref<254x254x1022xf32> to memref<254x254x1022xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x1022xf32>\n    return %3 : tensor<254x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<3x3x3xf32>, tensor<254x254x1022xf32>) -> (tensor<254x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>_52": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>\n  return %ret : tensor<126x1022x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x1022x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x1022x30xf32>\n    memref.copy %2, %alloc : memref<126x1022x30xf32> to memref<126x1022x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x1022x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x1022x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x1022x30xf32>\n    return %3 : tensor<126x1022x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>\n  return %ret : tensor<126x1022x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c1022, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x1022x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<3x3x3xf32>, tensor<126x1022x30xf32>) -> (tensor<126x1022x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>_53": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>\n  return %ret : tensor<32x1024x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024x256xf32>\n    memref.copy %2, %alloc : memref<32x1024x256xf32> to memref<32x1024x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x1024x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x1024x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024x256xf32>\n    return %3 : tensor<32x1024x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>\n  return %ret : tensor<32x1024x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x1024x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x256xf32>, tensor<1x1x1xf32>, tensor<32x1024x256xf32>) -> (tensor<32x1024x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>_54": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>\n  return %ret : tensor<508x252x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x252xf32>\n    memref.copy %2, %alloc : memref<508x252x252xf32> to memref<508x252x252xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x252xf32>\n    return %3 : tensor<508x252x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>\n  return %ret : tensor<508x252x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x256xf32>, tensor<5x5x5xf32>, tensor<508x252x252xf32>) -> (tensor<508x252x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32>_55": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32>\n  return %ret : tensor<1024x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256x64xf32>\n    memref.copy %2, %alloc : memref<1024x256x64xf32> to memref<1024x256x64xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256x64xf32>\n    return %3 : tensor<1024x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x64xf32>) -> tensor<1024x256x64xf32>\n  return %ret : tensor<1024x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x64xf32>, tensor<1x1x1xf32>, tensor<1024x256x64xf32>) -> (tensor<1024x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>_56": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>\n  return %ret : tensor<1024x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x32x1024xf32>\n    memref.copy %2, %alloc : memref<1024x32x1024xf32> to memref<1024x32x1024xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x32x1024xf32>\n    return %3 : tensor<1024x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x1024xf32>) -> tensor<1024x32x1024xf32>\n  return %ret : tensor<1024x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x1024xf32>, tensor<1x1x1xf32>, tensor<1024x32x1024xf32>) -> (tensor<1024x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>_57": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x1022xf32>\n    memref.copy %2, %alloc : memref<254x254x1022xf32> to memref<254x254x1022xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x1022xf32>\n    return %3 : tensor<254x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<3x3x3xf32>, tensor<254x254x1022xf32>) -> (tensor<254x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32>_58": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32>\n  return %ret : tensor<256x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x64xf32>) -> tensor<256x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x64xf32>\n    memref.copy %2, %alloc : memref<256x256x64xf32> to memref<256x256x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x64xf32>\n    return %3 : tensor<256x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x64xf32>) -> tensor<256x256x64xf32>\n  return %ret : tensor<256x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x64xf32>, tensor<1x1x1xf32>, tensor<256x256x64xf32>) -> (tensor<256x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32>_59": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32>\n  return %ret : tensor<64x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x128x64xf32>) -> tensor<64x128x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128x64xf32>\n    memref.copy %2, %alloc : memref<64x128x64xf32> to memref<64x128x64xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x128x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128x64xf32>\n    return %3 : tensor<64x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x64xf32>) -> tensor<64x128x64xf32>\n  return %ret : tensor<64x128x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x128x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x64xf32>, tensor<1x1x1xf32>, tensor<64x128x64xf32>) -> (tensor<64x128x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>_60": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>\n  return %ret : tensor<122x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x26xf32>\n    memref.copy %2, %alloc : memref<122x250x26xf32> to memref<122x250x26xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x26xf32>\n    return %3 : tensor<122x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>\n  return %ret : tensor<122x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<7x7x7xf32>, tensor<122x250x26xf32>) -> (tensor<122x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32>_61": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32>\n  return %ret : tensor<126x126x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x126x510xf32>) -> tensor<126x126x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126x510xf32>\n    memref.copy %2, %alloc : memref<126x126x510xf32> to memref<126x126x510xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x126x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x126x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126x510xf32>\n    return %3 : tensor<126x126x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x510xf32>) -> tensor<126x126x510xf32>\n  return %ret : tensor<126x126x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x126x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x512xf32>, tensor<3x3x3xf32>, tensor<126x126x510xf32>) -> (tensor<126x126x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32>_62": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32>\n  return %ret : tensor<32x64x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x1024xf32>\n    memref.copy %2, %alloc : memref<32x64x1024xf32> to memref<32x64x1024xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x64x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x64x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64x1024xf32>\n    return %3 : tensor<32x64x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x1024xf32>) -> tensor<32x64x1024xf32>\n  return %ret : tensor<32x64x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x64x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x1024xf32>, tensor<1x1x1xf32>, tensor<32x64x1024xf32>) -> (tensor<32x64x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>_63": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>\n  return %ret : tensor<254x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126x254xf32>\n    memref.copy %2, %alloc : memref<254x126x254xf32> to memref<254x126x254xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x126x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126x254xf32>\n    return %3 : tensor<254x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>\n  return %ret : tensor<254x126x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x126x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x256xf32>, tensor<3x3x3xf32>, tensor<254x126x254xf32>) -> (tensor<254x126x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32>_64": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32>\n  return %ret : tensor<64x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x128x128xf32>) -> tensor<64x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128x128xf32>\n    memref.copy %2, %alloc : memref<64x128x128xf32> to memref<64x128x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128x128xf32>\n    return %3 : tensor<64x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x128xf32>) -> tensor<64x128x128xf32>\n  return %ret : tensor<64x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x128xf32>, tensor<1x1x1xf32>, tensor<64x128x128xf32>) -> (tensor<64x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>_65": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>\n  return %ret : tensor<62x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x126xf32>\n    memref.copy %2, %alloc : memref<62x62x126xf32> to memref<62x62x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x126xf32>\n    return %3 : tensor<62x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>\n  return %ret : tensor<62x62x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<3x3x3xf32>, tensor<62x62x126xf32>) -> (tensor<62x62x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32>_66": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32>\n  return %ret : tensor<512x1024x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x256xf32>\n    memref.copy %2, %alloc : memref<512x1024x256xf32> to memref<512x1024x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x256xf32>\n    return %3 : tensor<512x1024x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x256xf32>) -> tensor<512x1024x256xf32>\n  return %ret : tensor<512x1024x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x256xf32>, tensor<1x1x1xf32>, tensor<512x1024x256xf32>) -> (tensor<512x1024x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>_67": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>\n  return %ret : tensor<124x60x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60x1020xf32>\n    memref.copy %2, %alloc : memref<124x60x1020xf32> to memref<124x60x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x60x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x60x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60x1020xf32>\n    return %3 : tensor<124x60x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>\n  return %ret : tensor<124x60x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x60x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x1024xf32>, tensor<5x5x5xf32>, tensor<124x60x1020xf32>) -> (tensor<124x60x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>_68": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>\n  return %ret : tensor<64x64x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x512xf32>\n    memref.copy %2, %alloc : memref<64x64x512xf32> to memref<64x64x512xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x512xf32>\n    return %3 : tensor<64x64x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>\n  return %ret : tensor<64x64x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x512xf32>, tensor<1x1x1xf32>, tensor<64x64x512xf32>) -> (tensor<64x64x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>_69": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>\n  return %ret : tensor<122x1018x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x58xf32>\n    memref.copy %2, %alloc : memref<122x1018x58xf32> to memref<122x1018x58xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x58xf32>\n    return %3 : tensor<122x1018x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>\n  return %ret : tensor<122x1018x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x64xf32>, tensor<7x7x7xf32>, tensor<122x1018x58xf32>) -> (tensor<122x1018x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>_70": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>\n  return %ret : tensor<1020x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x28xf32>\n    memref.copy %2, %alloc : memref<1020x28x28xf32> to memref<1020x28x28xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x28xf32>\n    return %3 : tensor<1020x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>\n  return %ret : tensor<1020x28x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x32xf32>, tensor<5x5x5xf32>, tensor<1020x28x28xf32>) -> (tensor<1020x28x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32>_71": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32>\n  return %ret : tensor<58x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x506xf32>) -> tensor<58x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x506xf32>\n    memref.copy %2, %alloc : memref<58x122x506xf32> to memref<58x122x506xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x506xf32>\n    return %3 : tensor<58x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x506xf32>) -> tensor<58x122x506xf32>\n  return %ret : tensor<58x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x512xf32>, tensor<7x7x7xf32>, tensor<58x122x506xf32>) -> (tensor<58x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32>_72": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32>\n  return %ret : tensor<510x62x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x62xf32>) -> tensor<510x62x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x62xf32>\n    memref.copy %2, %alloc : memref<510x62x62xf32> to memref<510x62x62xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x62xf32>\n    return %3 : tensor<510x62x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x62xf32>) -> tensor<510x62x62xf32>\n  return %ret : tensor<510x62x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x64xf32>, tensor<3x3x3xf32>, tensor<510x62x62xf32>) -> (tensor<510x62x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>_73": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x64xf32>\n    memref.copy %2, %alloc : memref<128x256x64xf32> to memref<128x256x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x64xf32>\n    return %3 : tensor<128x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x64xf32>, tensor<1x1x1xf32>, tensor<128x256x64xf32>) -> (tensor<128x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32>_74": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32>\n  return %ret : tensor<250x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018x506xf32>\n    memref.copy %2, %alloc : memref<250x1018x506xf32> to memref<250x1018x506xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018x506xf32>\n    return %3 : tensor<250x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x506xf32>) -> tensor<250x1018x506xf32>\n  return %ret : tensor<250x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x512xf32>, tensor<7x7x7xf32>, tensor<250x1018x506xf32>) -> (tensor<250x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32>_75": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32>\n  return %ret : tensor<122x58x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x58x122xf32>) -> tensor<122x58x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x58x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x58x122xf32>\n    memref.copy %2, %alloc : memref<122x58x122xf32> to memref<122x58x122xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x58x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x58x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x58x122xf32>\n    return %3 : tensor<122x58x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x122xf32>) -> tensor<122x58x122xf32>\n  return %ret : tensor<122x58x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c58, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x58x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x128xf32>, tensor<7x7x7xf32>, tensor<122x58x122xf32>) -> (tensor<122x58x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32>_76": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32>\n  return %ret : tensor<64x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x1024xf32>\n    memref.copy %2, %alloc : memref<64x256x1024xf32> to memref<64x256x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x256x1024xf32>\n    return %3 : tensor<64x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x256x1024xf32>) -> tensor<64x256x1024xf32>\n  return %ret : tensor<64x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x1024xf32>, tensor<1x1x1xf32>, tensor<64x256x1024xf32>) -> (tensor<64x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32>_77": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32>\n  return %ret : tensor<1018x58x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58x58xf32>\n    memref.copy %2, %alloc : memref<1018x58x58xf32> to memref<1018x58x58xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x58x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x58x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58x58xf32>\n    return %3 : tensor<1018x58x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x58xf32>) -> tensor<1018x58x58xf32>\n  return %ret : tensor<1018x58x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x58x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x64xf32>, tensor<7x7x7xf32>, tensor<1018x58x58xf32>) -> (tensor<1018x58x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>_78": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>\n  return %ret : tensor<250x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x250xf32>\n    memref.copy %2, %alloc : memref<250x122x250xf32> to memref<250x122x250xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x250xf32>\n    return %3 : tensor<250x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>\n  return %ret : tensor<250x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x256xf32>, tensor<7x7x7xf32>, tensor<250x122x250xf32>) -> (tensor<250x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32>_79": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32>\n  return %ret : tensor<1024x512x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x512x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x512x64xf32>\n    memref.copy %2, %alloc : memref<1024x512x64xf32> to memref<1024x512x64xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x512x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x512x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x512x64xf32>\n    return %3 : tensor<1024x512x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x64xf32>) -> tensor<1024x512x64xf32>\n  return %ret : tensor<1024x512x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x64xf32>, tensor<1x1x1xf32>, tensor<1024x512x64xf32>) -> (tensor<1024x512x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>_80": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256x32xf32>\n    memref.copy %2, %alloc : memref<512x256x32xf32> to memref<512x256x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256x32xf32>\n    return %3 : tensor<512x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<1x1x1xf32>, tensor<512x256x32xf32>) -> (tensor<512x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>_81": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x1024xf32>\n    memref.copy %2, %alloc : memref<256x256x1024xf32> to memref<256x256x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x1024xf32>\n    return %3 : tensor<256x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<1x1x1xf32>, tensor<256x256x1024xf32>) -> (tensor<256x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>_82": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>\n  return %ret : tensor<126x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254x510xf32>\n    memref.copy %2, %alloc : memref<126x254x510xf32> to memref<126x254x510xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254x510xf32>\n    return %3 : tensor<126x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>\n  return %ret : tensor<126x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x512xf32>, tensor<3x3x3xf32>, tensor<126x254x510xf32>) -> (tensor<126x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32>_83": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32>\n  return %ret : tensor<512x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x32x256xf32>) -> tensor<512x32x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x32x256xf32>\n    memref.copy %2, %alloc : memref<512x32x256xf32> to memref<512x32x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x32x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x32x256xf32>\n    return %3 : tensor<512x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x256xf32>) -> tensor<512x32x256xf32>\n  return %ret : tensor<512x32x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c32, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x32x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x256xf32>, tensor<1x1x1xf32>, tensor<512x32x256xf32>) -> (tensor<512x32x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32>_84": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32>\n  return %ret : tensor<128x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x32x64xf32>) -> tensor<128x32x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x64xf32>\n    memref.copy %2, %alloc : memref<128x32x64xf32> to memref<128x32x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x32x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x64xf32>\n    return %3 : tensor<128x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x64xf32>) -> tensor<128x32x64xf32>\n  return %ret : tensor<128x32x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c32, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x32x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x64xf32>, tensor<1x1x1xf32>, tensor<128x32x64xf32>) -> (tensor<128x32x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>_85": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>\n  return %ret : tensor<126x30x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x510xf32>\n    memref.copy %2, %alloc : memref<126x30x510xf32> to memref<126x30x510xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x510xf32>\n    return %3 : tensor<126x30x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>\n  return %ret : tensor<126x30x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<3x3x3xf32>, tensor<126x30x510xf32>) -> (tensor<126x30x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32>_86": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32>\n  return %ret : tensor<128x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x128x64xf32>) -> tensor<128x128x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x64xf32>\n    memref.copy %2, %alloc : memref<128x128x64xf32> to memref<128x128x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x128x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x64xf32>\n    return %3 : tensor<128x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x64xf32>) -> tensor<128x128x64xf32>\n  return %ret : tensor<128x128x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x128x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x64xf32>, tensor<1x1x1xf32>, tensor<128x128x64xf32>) -> (tensor<128x128x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32>_87": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32>\n  return %ret : tensor<30x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x126x62xf32>) -> tensor<30x126x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126x62xf32>\n    memref.copy %2, %alloc : memref<30x126x62xf32> to memref<30x126x62xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x126x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126x62xf32>\n    return %3 : tensor<30x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x62xf32>) -> tensor<30x126x62xf32>\n  return %ret : tensor<30x126x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x126x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x64xf32>, tensor<3x3x3xf32>, tensor<30x126x62xf32>) -> (tensor<30x126x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>_88": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250x26xf32>\n    memref.copy %2, %alloc : memref<1018x250x26xf32> to memref<1018x250x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250x26xf32>\n    return %3 : tensor<1018x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<7x7x7xf32>, tensor<1018x250x26xf32>) -> (tensor<1018x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>_89": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256x32xf32>\n    memref.copy %2, %alloc : memref<512x256x32xf32> to memref<512x256x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256x32xf32>\n    return %3 : tensor<512x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<1x1x1xf32>, tensor<512x256x32xf32>) -> (tensor<512x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>_90": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>\n  return %ret : tensor<124x1020x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020x508xf32>\n    memref.copy %2, %alloc : memref<124x1020x508xf32> to memref<124x1020x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x1020x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x1020x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020x508xf32>\n    return %3 : tensor<124x1020x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>\n  return %ret : tensor<124x1020x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x1020x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x512xf32>, tensor<5x5x5xf32>, tensor<124x1020x508xf32>) -> (tensor<124x1020x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>_91": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>\n  return %ret : tensor<126x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254x510xf32>\n    memref.copy %2, %alloc : memref<126x254x510xf32> to memref<126x254x510xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254x510xf32>\n    return %3 : tensor<126x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x510xf32>) -> tensor<126x254x510xf32>\n  return %ret : tensor<126x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x512xf32>, tensor<3x3x3xf32>, tensor<126x254x510xf32>) -> (tensor<126x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>_92": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>\n  return %ret : tensor<1020x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508x60xf32>\n    memref.copy %2, %alloc : memref<1020x508x60xf32> to memref<1020x508x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508x60xf32>\n    return %3 : tensor<1020x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>\n  return %ret : tensor<1020x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x64xf32>, tensor<5x5x5xf32>, tensor<1020x508x60xf32>) -> (tensor<1020x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>_93": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>\n  return %ret : tensor<124x1020x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020x124xf32>\n    memref.copy %2, %alloc : memref<124x1020x124xf32> to memref<124x1020x124xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x1020x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x1020x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020x124xf32>\n    return %3 : tensor<124x1020x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>\n  return %ret : tensor<124x1020x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x1020x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x128xf32>, tensor<5x5x5xf32>, tensor<124x1020x124xf32>) -> (tensor<124x1020x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>_94": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>\n  return %ret : tensor<28x60x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60x28xf32>\n    memref.copy %2, %alloc : memref<28x60x28xf32> to memref<28x60x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x60x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x60x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60x28xf32>\n    return %3 : tensor<28x60x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>\n  return %ret : tensor<28x60x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x60x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x32xf32>, tensor<5x5x5xf32>, tensor<28x60x28xf32>) -> (tensor<28x60x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>_95": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>\n  return %ret : tensor<1022x510x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510x1022xf32>\n    memref.copy %2, %alloc : memref<1022x510x1022xf32> to memref<1022x510x1022xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x510x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x510x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510x1022xf32>\n    return %3 : tensor<1022x510x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>\n  return %ret : tensor<1022x510x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x510x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x1024xf32>, tensor<3x3x3xf32>, tensor<1022x510x1022xf32>) -> (tensor<1022x510x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>_96": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>\n  return %ret : tensor<122x250x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x1018xf32>\n    memref.copy %2, %alloc : memref<122x250x1018xf32> to memref<122x250x1018xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x1018xf32>\n    return %3 : tensor<122x250x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>\n  return %ret : tensor<122x250x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x1024xf32>, tensor<7x7x7xf32>, tensor<122x250x1018xf32>) -> (tensor<122x250x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32>_97": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32>\n  return %ret : tensor<1022x30x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30x1022xf32>\n    memref.copy %2, %alloc : memref<1022x30x1022xf32> to memref<1022x30x1022xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x30x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x30x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30x1022xf32>\n    return %3 : tensor<1022x30x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x1022xf32>) -> tensor<1022x30x1022xf32>\n  return %ret : tensor<1022x30x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x30x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x1024xf32>, tensor<3x3x3xf32>, tensor<1022x30x1022xf32>) -> (tensor<1022x30x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32>_98": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32>\n  return %ret : tensor<1018x58x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58x122xf32>\n    memref.copy %2, %alloc : memref<1018x58x122xf32> to memref<1018x58x122xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x58x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x58x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58x122xf32>\n    return %3 : tensor<1018x58x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x122xf32>) -> tensor<1018x58x122xf32>\n  return %ret : tensor<1018x58x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x58x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x128xf32>, tensor<7x7x7xf32>, tensor<1018x58x122xf32>) -> (tensor<1018x58x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32>_99": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32>\n  return %ret : tensor<58x26x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x26x58xf32>) -> tensor<58x26x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x26x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x26x58xf32>\n    memref.copy %2, %alloc : memref<58x26x58xf32> to memref<58x26x58xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x26x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x26x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x26x58xf32>\n    return %3 : tensor<58x26x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x58xf32>) -> tensor<58x26x58xf32>\n  return %ret : tensor<58x26x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c26, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x26x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x64xf32>, tensor<7x7x7xf32>, tensor<58x26x58xf32>) -> (tensor<58x26x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>_100": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>\n  return %ret : tensor<26x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x26xf32>\n    memref.copy %2, %alloc : memref<26x250x26xf32> to memref<26x250x26xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x26xf32>\n    return %3 : tensor<26x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>\n  return %ret : tensor<26x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x32xf32>, tensor<7x7x7xf32>, tensor<26x250x26xf32>) -> (tensor<26x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32>_101": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32>\n  return %ret : tensor<128x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x1024xf32>\n    memref.copy %2, %alloc : memref<128x32x1024xf32> to memref<128x32x1024xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x1024xf32>\n    return %3 : tensor<128x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x1024xf32>) -> tensor<128x32x1024xf32>\n  return %ret : tensor<128x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x1024xf32>, tensor<1x1x1xf32>, tensor<128x32x1024xf32>) -> (tensor<128x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32>_102": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32>\n  return %ret : tensor<122x1018x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x250xf32>\n    memref.copy %2, %alloc : memref<122x1018x250xf32> to memref<122x1018x250xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x250xf32>\n    return %3 : tensor<122x1018x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x250xf32>) -> tensor<122x1018x250xf32>\n  return %ret : tensor<122x1018x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x256xf32>, tensor<7x7x7xf32>, tensor<122x1018x250xf32>) -> (tensor<122x1018x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32>_103": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32>\n  return %ret : tensor<58x26x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x26x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x26x1018xf32>\n    memref.copy %2, %alloc : memref<58x26x1018xf32> to memref<58x26x1018xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x26x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x26x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x26x1018xf32>\n    return %3 : tensor<58x26x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x26x1018xf32>) -> tensor<58x26x1018xf32>\n  return %ret : tensor<58x26x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c26, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x26x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x1024xf32>, tensor<7x7x7xf32>, tensor<58x26x1018xf32>) -> (tensor<58x26x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>_104": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>\n  return %ret : tensor<62x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x254xf32>\n    memref.copy %2, %alloc : memref<62x126x254xf32> to memref<62x126x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x254xf32>\n    return %3 : tensor<62x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x254xf32>) -> tensor<62x126x254xf32>\n  return %ret : tensor<62x126x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<3x3x3xf32>, tensor<62x126x254xf32>) -> (tensor<62x126x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32>_105": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32>\n  return %ret : tensor<64x128x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x128x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128x1024xf32>\n    memref.copy %2, %alloc : memref<64x128x1024xf32> to memref<64x128x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x128x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x128x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x128x1024xf32>\n    return %3 : tensor<64x128x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x128x1024xf32>) -> tensor<64x128x1024xf32>\n  return %ret : tensor<64x128x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c128, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x128x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x1024xf32>, tensor<1x1x1xf32>, tensor<64x128x1024xf32>) -> (tensor<64x128x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>_106": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x64xf32>\n    memref.copy %2, %alloc : memref<128x256x64xf32> to memref<128x256x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x64xf32>\n    return %3 : tensor<128x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x64xf32>, tensor<1x1x1xf32>, tensor<128x256x64xf32>) -> (tensor<128x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>_107": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>\n  return %ret : tensor<26x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x58xf32>\n    memref.copy %2, %alloc : memref<26x250x58xf32> to memref<26x250x58xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x58xf32>\n    return %3 : tensor<26x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x58xf32>) -> tensor<26x250x58xf32>\n  return %ret : tensor<26x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x64xf32>, tensor<7x7x7xf32>, tensor<26x250x58xf32>) -> (tensor<26x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>_108": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>\n  return %ret : tensor<1020x124x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x124x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x124x124xf32>\n    memref.copy %2, %alloc : memref<1020x124x124xf32> to memref<1020x124x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x124x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x124x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x124x124xf32>\n    return %3 : tensor<1020x124x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>\n  return %ret : tensor<1020x124x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c124, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x124x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<5x5x5xf32>, tensor<1020x124x124xf32>) -> (tensor<1020x124x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>_109": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x28xf32>\n    memref.copy %2, %alloc : memref<508x252x28xf32> to memref<508x252x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x28xf32>\n    return %3 : tensor<508x252x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x28xf32>) -> tensor<508x252x28xf32>\n  return %ret : tensor<508x252x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<5x5x5xf32>, tensor<508x252x28xf32>) -> (tensor<508x252x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32>_110": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32>\n  return %ret : tensor<122x250x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x506xf32>) -> tensor<122x250x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x506xf32>\n    memref.copy %2, %alloc : memref<122x250x506xf32> to memref<122x250x506xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x506xf32>\n    return %3 : tensor<122x250x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x506xf32>) -> tensor<122x250x506xf32>\n  return %ret : tensor<122x250x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x512xf32>, tensor<7x7x7xf32>, tensor<122x250x506xf32>) -> (tensor<122x250x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32>_111": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32>\n  return %ret : tensor<1018x58x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58x506xf32>\n    memref.copy %2, %alloc : memref<1018x58x506xf32> to memref<1018x58x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x58x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x58x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58x506xf32>\n    return %3 : tensor<1018x58x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x506xf32>) -> tensor<1018x58x506xf32>\n  return %ret : tensor<1018x58x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x58x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x512xf32>, tensor<7x7x7xf32>, tensor<1018x58x506xf32>) -> (tensor<1018x58x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32>_112": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32>\n  return %ret : tensor<62x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254x1022xf32>\n    memref.copy %2, %alloc : memref<62x254x1022xf32> to memref<62x254x1022xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254x1022xf32>\n    return %3 : tensor<62x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x1022xf32>) -> tensor<62x254x1022xf32>\n  return %ret : tensor<62x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x1024xf32>, tensor<3x3x3xf32>, tensor<62x254x1022xf32>) -> (tensor<62x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>_113": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>\n  return %ret : tensor<28x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x124x28xf32>\n    memref.copy %2, %alloc : memref<28x124x28xf32> to memref<28x124x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x124x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x124x28xf32>\n    return %3 : tensor<28x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x28xf32>) -> tensor<28x124x28xf32>\n  return %ret : tensor<28x124x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c124, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x124x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x32xf32>, tensor<5x5x5xf32>, tensor<28x124x28xf32>) -> (tensor<28x124x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>_114": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>\n  return %ret : tensor<28x60x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60x28xf32>\n    memref.copy %2, %alloc : memref<28x60x28xf32> to memref<28x60x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x60x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x60x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60x28xf32>\n    return %3 : tensor<28x60x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x28xf32>) -> tensor<28x60x28xf32>\n  return %ret : tensor<28x60x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x60x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x32xf32>, tensor<5x5x5xf32>, tensor<28x60x28xf32>) -> (tensor<28x60x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>_115": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>\n  return %ret : tensor<126x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x30xf32>\n    memref.copy %2, %alloc : memref<126x30x30xf32> to memref<126x30x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x30xf32>\n    return %3 : tensor<126x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>\n  return %ret : tensor<126x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x32xf32>, tensor<3x3x3xf32>, tensor<126x30x30xf32>) -> (tensor<126x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>_116": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>\n  return %ret : tensor<1020x28x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x252xf32>\n    memref.copy %2, %alloc : memref<1020x28x252xf32> to memref<1020x28x252xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x252xf32>\n    return %3 : tensor<1020x28x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>\n  return %ret : tensor<1020x28x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x256xf32>, tensor<5x5x5xf32>, tensor<1020x28x252xf32>) -> (tensor<1020x28x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>_117": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>\n  return %ret : tensor<254x1022x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022x30xf32>\n    memref.copy %2, %alloc : memref<254x1022x30xf32> to memref<254x1022x30xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x1022x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x1022x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022x30xf32>\n    return %3 : tensor<254x1022x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>\n  return %ret : tensor<254x1022x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x1022x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x32xf32>, tensor<3x3x3xf32>, tensor<254x1022x30xf32>) -> (tensor<254x1022x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32>_118": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32>\n  return %ret : tensor<62x62x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x62xf32>) -> tensor<62x62x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x62xf32>\n    memref.copy %2, %alloc : memref<62x62x62xf32> to memref<62x62x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x62xf32>\n    return %3 : tensor<62x62x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x62xf32>) -> tensor<62x62x62xf32>\n  return %ret : tensor<62x62x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x64xf32>, tensor<3x3x3xf32>, tensor<62x62x62xf32>) -> (tensor<62x62x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>_119": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>\n  return %ret : tensor<512x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256x64xf32>\n    memref.copy %2, %alloc : memref<512x256x64xf32> to memref<512x256x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256x64xf32>\n    return %3 : tensor<512x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>\n  return %ret : tensor<512x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x64xf32>, tensor<1x1x1xf32>, tensor<512x256x64xf32>) -> (tensor<512x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32>_120": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32>\n  return %ret : tensor<512x512x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x512x128xf32>) -> tensor<512x512x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x512x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x512x128xf32>\n    memref.copy %2, %alloc : memref<512x512x128xf32> to memref<512x512x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x512x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x512x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x512x128xf32>\n    return %3 : tensor<512x512x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x128xf32>) -> tensor<512x512x128xf32>\n  return %ret : tensor<512x512x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c512, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x512x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x128xf32>, tensor<1x1x1xf32>, tensor<512x512x128xf32>) -> (tensor<512x512x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>_121": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>\n  return %ret : tensor<124x124x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x124x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x124x1020xf32>\n    memref.copy %2, %alloc : memref<124x124x1020xf32> to memref<124x124x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x124x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x124x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x124x1020xf32>\n    return %3 : tensor<124x124x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x1020xf32>) -> tensor<124x124x1020xf32>\n  return %ret : tensor<124x124x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c124, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x124x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x1024xf32>, tensor<5x5x5xf32>, tensor<124x124x1020xf32>) -> (tensor<124x124x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32>_122": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32>\n  return %ret : tensor<250x58x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x58x122xf32>) -> tensor<250x58x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x58x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x58x122xf32>\n    memref.copy %2, %alloc : memref<250x58x122xf32> to memref<250x58x122xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x58x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x58x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x58x122xf32>\n    return %3 : tensor<250x58x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x122xf32>) -> tensor<250x58x122xf32>\n  return %ret : tensor<250x58x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c58, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x58x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x128xf32>, tensor<7x7x7xf32>, tensor<250x58x122xf32>) -> (tensor<250x58x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>_123": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>\n  return %ret : tensor<26x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x1018x506xf32>\n    memref.copy %2, %alloc : memref<26x1018x506xf32> to memref<26x1018x506xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x1018x506xf32>\n    return %3 : tensor<26x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x506xf32>) -> tensor<26x1018x506xf32>\n  return %ret : tensor<26x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x512xf32>, tensor<7x7x7xf32>, tensor<26x1018x506xf32>) -> (tensor<26x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32>_124": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32>\n  return %ret : tensor<128x512x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x1024xf32>\n    memref.copy %2, %alloc : memref<128x512x1024xf32> to memref<128x512x1024xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x512x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x512x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x1024xf32>\n    return %3 : tensor<128x512x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x1024xf32>) -> tensor<128x512x1024xf32>\n  return %ret : tensor<128x512x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x512x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x1024xf32>, tensor<1x1x1xf32>, tensor<128x512x1024xf32>) -> (tensor<128x512x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>_125": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>\n  return %ret : tensor<1020x60x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60x252xf32>\n    memref.copy %2, %alloc : memref<1020x60x252xf32> to memref<1020x60x252xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x60x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x60x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60x252xf32>\n    return %3 : tensor<1020x60x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>\n  return %ret : tensor<1020x60x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x60x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x256xf32>, tensor<5x5x5xf32>, tensor<1020x60x252xf32>) -> (tensor<1020x60x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>_126": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>\n  return %ret : tensor<510x62x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x1022xf32>\n    memref.copy %2, %alloc : memref<510x62x1022xf32> to memref<510x62x1022xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x1022xf32>\n    return %3 : tensor<510x62x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>\n  return %ret : tensor<510x62x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x1024xf32>, tensor<3x3x3xf32>, tensor<510x62x1022xf32>) -> (tensor<510x62x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>_127": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>\n  return %ret : tensor<128x32x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x512xf32>\n    memref.copy %2, %alloc : memref<128x32x512xf32> to memref<128x32x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x32x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x32x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x512xf32>\n    return %3 : tensor<128x32x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>\n  return %ret : tensor<128x32x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<1x1x1xf32>, tensor<128x32x512xf32>) -> (tensor<128x32x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32>_128": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32>\n  return %ret : tensor<30x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x30x30xf32>) -> tensor<30x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30x30xf32>\n    memref.copy %2, %alloc : memref<30x30x30xf32> to memref<30x30x30xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30x30xf32>\n    return %3 : tensor<30x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x30xf32>) -> tensor<30x30x30xf32>\n  return %ret : tensor<30x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x32xf32>, tensor<3x3x3xf32>, tensor<30x30x30xf32>) -> (tensor<30x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32>_129": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32>\n  return %ret : tensor<256x64x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x64x256xf32>) -> tensor<256x64x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x256xf32>\n    memref.copy %2, %alloc : memref<256x64x256xf32> to memref<256x64x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x64x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x64x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x256xf32>\n    return %3 : tensor<256x64x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x256xf32>) -> tensor<256x64x256xf32>\n  return %ret : tensor<256x64x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c64, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x64x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x256xf32>, tensor<1x1x1xf32>, tensor<256x64x256xf32>) -> (tensor<256x64x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>_130": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x128xf32>\n    memref.copy %2, %alloc : memref<64x64x128xf32> to memref<64x64x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x128xf32>\n    return %3 : tensor<64x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<1x1x1xf32>, tensor<64x64x128xf32>) -> (tensor<64x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32>_131": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32>\n  return %ret : tensor<508x508x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x508x28xf32>) -> tensor<508x508x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508x28xf32>\n    memref.copy %2, %alloc : memref<508x508x28xf32> to memref<508x508x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x508x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x508x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508x28xf32>\n    return %3 : tensor<508x508x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x28xf32>) -> tensor<508x508x28xf32>\n  return %ret : tensor<508x508x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x508x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x32xf32>, tensor<5x5x5xf32>, tensor<508x508x28xf32>) -> (tensor<508x508x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32>_132": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32>\n  return %ret : tensor<1018x26x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26x26xf32>\n    memref.copy %2, %alloc : memref<1018x26x26xf32> to memref<1018x26x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x26x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x26x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26x26xf32>\n    return %3 : tensor<1018x26x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x26xf32>) -> tensor<1018x26x26xf32>\n  return %ret : tensor<1018x26x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x26x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x32xf32>, tensor<7x7x7xf32>, tensor<1018x26x26xf32>) -> (tensor<1018x26x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>_133": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>\n  return %ret : tensor<506x506x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506x26xf32>\n    memref.copy %2, %alloc : memref<506x506x26xf32> to memref<506x506x26xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x506x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x506x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506x26xf32>\n    return %3 : tensor<506x506x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>\n  return %ret : tensor<506x506x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x506x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x32xf32>, tensor<7x7x7xf32>, tensor<506x506x26xf32>) -> (tensor<506x506x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32>_134": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32>\n  return %ret : tensor<1024x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x64x128xf32>\n    memref.copy %2, %alloc : memref<1024x64x128xf32> to memref<1024x64x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x64x128xf32>\n    return %3 : tensor<1024x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x128xf32>) -> tensor<1024x64x128xf32>\n  return %ret : tensor<1024x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x128xf32>, tensor<1x1x1xf32>, tensor<1024x64x128xf32>) -> (tensor<1024x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>_135": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>\n  return %ret : tensor<1020x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x28xf32>\n    memref.copy %2, %alloc : memref<1020x28x28xf32> to memref<1020x28x28xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x28xf32>\n    return %3 : tensor<1020x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x28xf32>) -> tensor<1020x28x28xf32>\n  return %ret : tensor<1020x28x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x32xf32>, tensor<5x5x5xf32>, tensor<1020x28x28xf32>) -> (tensor<1020x28x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32>_136": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32>\n  return %ret : tensor<58x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018x122xf32>\n    memref.copy %2, %alloc : memref<58x1018x122xf32> to memref<58x1018x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018x122xf32>\n    return %3 : tensor<58x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x122xf32>) -> tensor<58x1018x122xf32>\n  return %ret : tensor<58x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x128xf32>, tensor<7x7x7xf32>, tensor<58x1018x122xf32>) -> (tensor<58x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32>_137": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32>\n  return %ret : tensor<58x506x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x506x122xf32>) -> tensor<58x506x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506x122xf32>\n    memref.copy %2, %alloc : memref<58x506x122xf32> to memref<58x506x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x506x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x506x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506x122xf32>\n    return %3 : tensor<58x506x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x122xf32>) -> tensor<58x506x122xf32>\n  return %ret : tensor<58x506x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x506x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x128xf32>, tensor<7x7x7xf32>, tensor<58x506x122xf32>) -> (tensor<58x506x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32>_138": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32>\n  return %ret : tensor<128x128x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x128x256xf32>) -> tensor<128x128x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x128x256xf32>\n    memref.copy %2, %alloc : memref<128x128x256xf32> to memref<128x128x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x128x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x128x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x128x256xf32>\n    return %3 : tensor<128x128x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x128x256xf32>) -> tensor<128x128x256xf32>\n  return %ret : tensor<128x128x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c128, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x128x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x256xf32>, tensor<1x1x1xf32>, tensor<128x128x256xf32>) -> (tensor<128x128x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32>_139": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32>\n  return %ret : tensor<506x1018x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018x250xf32>\n    memref.copy %2, %alloc : memref<506x1018x250xf32> to memref<506x1018x250xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x1018x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x1018x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018x250xf32>\n    return %3 : tensor<506x1018x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x250xf32>) -> tensor<506x1018x250xf32>\n  return %ret : tensor<506x1018x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x1018x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x256xf32>, tensor<7x7x7xf32>, tensor<506x1018x250xf32>) -> (tensor<506x1018x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>_140": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>\n  return %ret : tensor<252x60x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x60x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x60x1020xf32>\n    memref.copy %2, %alloc : memref<252x60x1020xf32> to memref<252x60x1020xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x60x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x60x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x60x1020xf32>\n    return %3 : tensor<252x60x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>\n  return %ret : tensor<252x60x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c60, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x60x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x1024xf32>, tensor<5x5x5xf32>, tensor<252x60x1020xf32>) -> (tensor<252x60x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>_141": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x30xf32>\n    memref.copy %2, %alloc : memref<1022x254x30xf32> to memref<1022x254x30xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x30xf32>\n    return %3 : tensor<1022x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<3x3x3xf32>, tensor<1022x254x30xf32>) -> (tensor<1022x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32>_142": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32>\n  return %ret : tensor<26x506x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x506x250xf32>) -> tensor<26x506x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x506x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x506x250xf32>\n    memref.copy %2, %alloc : memref<26x506x250xf32> to memref<26x506x250xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x506x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x506x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x506x250xf32>\n    return %3 : tensor<26x506x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x250xf32>) -> tensor<26x506x250xf32>\n  return %ret : tensor<26x506x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c506, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x506x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x256xf32>, tensor<7x7x7xf32>, tensor<26x506x250xf32>) -> (tensor<26x506x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>_143": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x64xf32>\n    memref.copy %2, %alloc : memref<512x1024x64xf32> to memref<512x1024x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x64xf32>\n    return %3 : tensor<512x1024x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x64xf32>, tensor<1x1x1xf32>, tensor<512x1024x64xf32>) -> (tensor<512x1024x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>_144": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>\n  return %ret : tensor<122x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x26xf32>\n    memref.copy %2, %alloc : memref<122x250x26xf32> to memref<122x250x26xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x26xf32>\n    return %3 : tensor<122x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x26xf32>) -> tensor<122x250x26xf32>\n  return %ret : tensor<122x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<7x7x7xf32>, tensor<122x250x26xf32>) -> (tensor<122x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32>_145": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32>\n  return %ret : tensor<250x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x506xf32>) -> tensor<250x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x506xf32>\n    memref.copy %2, %alloc : memref<250x122x506xf32> to memref<250x122x506xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x506xf32>\n    return %3 : tensor<250x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x506xf32>) -> tensor<250x122x506xf32>\n  return %ret : tensor<250x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x512xf32>, tensor<7x7x7xf32>, tensor<250x122x506xf32>) -> (tensor<250x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>_146": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>\n  return %ret : tensor<252x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28x508xf32>\n    memref.copy %2, %alloc : memref<252x28x508xf32> to memref<252x28x508xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28x508xf32>\n    return %3 : tensor<252x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>\n  return %ret : tensor<252x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x512xf32>, tensor<5x5x5xf32>, tensor<252x28x508xf32>) -> (tensor<252x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>_147": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>\n  return %ret : tensor<62x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30x30xf32>\n    memref.copy %2, %alloc : memref<62x30x30xf32> to memref<62x30x30xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30x30xf32>\n    return %3 : tensor<62x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>\n  return %ret : tensor<62x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x32xf32>, tensor<3x3x3xf32>, tensor<62x30x30xf32>) -> (tensor<62x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>_148": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>\n  return %ret : tensor<1020x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x508xf32>\n    memref.copy %2, %alloc : memref<1020x28x508xf32> to memref<1020x28x508xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x508xf32>\n    return %3 : tensor<1020x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>\n  return %ret : tensor<1020x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x512xf32>, tensor<5x5x5xf32>, tensor<1020x28x508xf32>) -> (tensor<1020x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32>_149": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32>\n  return %ret : tensor<250x58x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x58x58xf32>) -> tensor<250x58x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x58x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x58x58xf32>\n    memref.copy %2, %alloc : memref<250x58x58xf32> to memref<250x58x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x58x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x58x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x58x58xf32>\n    return %3 : tensor<250x58x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x58xf32>) -> tensor<250x58x58xf32>\n  return %ret : tensor<250x58x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c58, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x58x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x64xf32>, tensor<7x7x7xf32>, tensor<250x58x58xf32>) -> (tensor<250x58x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32>_150": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32>\n  return %ret : tensor<30x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x126x254xf32>) -> tensor<30x126x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126x254xf32>\n    memref.copy %2, %alloc : memref<30x126x254xf32> to memref<30x126x254xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x126x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126x254xf32>\n    return %3 : tensor<30x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x254xf32>) -> tensor<30x126x254xf32>\n  return %ret : tensor<30x126x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x126x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x256xf32>, tensor<3x3x3xf32>, tensor<30x126x254xf32>) -> (tensor<30x126x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32>_151": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32>\n  return %ret : tensor<124x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x28x28xf32>) -> tensor<124x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28x28xf32>\n    memref.copy %2, %alloc : memref<124x28x28xf32> to memref<124x28x28xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x28x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28x28xf32>\n    return %3 : tensor<124x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x28xf32>) -> tensor<124x28x28xf32>\n  return %ret : tensor<124x28x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x28x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x32xf32>, tensor<5x5x5xf32>, tensor<124x28x28xf32>) -> (tensor<124x28x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>_152": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x250xf32>\n    memref.copy %2, %alloc : memref<58x122x250xf32> to memref<58x122x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x250xf32>\n    return %3 : tensor<58x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<7x7x7xf32>, tensor<58x122x250xf32>) -> (tensor<58x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>_153": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>\n  return %ret : tensor<250x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018x122xf32>\n    memref.copy %2, %alloc : memref<250x1018x122xf32> to memref<250x1018x122xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018x122xf32>\n    return %3 : tensor<250x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x122xf32>) -> tensor<250x1018x122xf32>\n  return %ret : tensor<250x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x128xf32>, tensor<7x7x7xf32>, tensor<250x1018x122xf32>) -> (tensor<250x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32>_154": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32>\n  return %ret : tensor<128x64x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x64x32xf32>) -> tensor<128x64x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x64x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64x32xf32>\n    memref.copy %2, %alloc : memref<128x64x32xf32> to memref<128x64x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x64x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x64x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x64x32xf32>\n    return %3 : tensor<128x64x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x64x32xf32>) -> tensor<128x64x32xf32>\n  return %ret : tensor<128x64x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c64, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x64x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x32xf32>, tensor<1x1x1xf32>, tensor<128x64x32xf32>) -> (tensor<128x64x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>_155": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>\n  return %ret : tensor<1020x124x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x124x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x124x124xf32>\n    memref.copy %2, %alloc : memref<1020x124x124xf32> to memref<1020x124x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x124x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x124x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x124x124xf32>\n    return %3 : tensor<1020x124x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x124x124xf32>) -> tensor<1020x124x124xf32>\n  return %ret : tensor<1020x124x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c124, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x124x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<5x5x5xf32>, tensor<1020x124x124xf32>) -> (tensor<1020x124x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>_156": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>\n  return %ret : tensor<60x124x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124x508xf32>\n    memref.copy %2, %alloc : memref<60x124x508xf32> to memref<60x124x508xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x124x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x124x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124x508xf32>\n    return %3 : tensor<60x124x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>\n  return %ret : tensor<60x124x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x124x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x512xf32>, tensor<5x5x5xf32>, tensor<60x124x508xf32>) -> (tensor<60x124x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>_157": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>\n  return %ret : tensor<252x60x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x60x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x60x1020xf32>\n    memref.copy %2, %alloc : memref<252x60x1020xf32> to memref<252x60x1020xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x60x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x60x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x60x1020xf32>\n    return %3 : tensor<252x60x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x60x1020xf32>) -> tensor<252x60x1020xf32>\n  return %ret : tensor<252x60x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c60, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x60x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x1024xf32>, tensor<5x5x5xf32>, tensor<252x60x1020xf32>) -> (tensor<252x60x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32>_158": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32>\n  return %ret : tensor<506x506x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x506x250xf32>) -> tensor<506x506x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506x250xf32>\n    memref.copy %2, %alloc : memref<506x506x250xf32> to memref<506x506x250xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x506x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x506x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506x250xf32>\n    return %3 : tensor<506x506x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x250xf32>) -> tensor<506x506x250xf32>\n  return %ret : tensor<506x506x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x506x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x256xf32>, tensor<7x7x7xf32>, tensor<506x506x250xf32>) -> (tensor<506x506x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32>_159": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32>\n  return %ret : tensor<1018x1018x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018x58xf32>\n    memref.copy %2, %alloc : memref<1018x1018x58xf32> to memref<1018x1018x58xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018x58xf32>\n    return %3 : tensor<1018x1018x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x58xf32>) -> tensor<1018x1018x58xf32>\n  return %ret : tensor<1018x1018x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x1018x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x64xf32>, tensor<7x7x7xf32>, tensor<1018x1018x58xf32>) -> (tensor<1018x1018x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32>_160": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32>\n  return %ret : tensor<512x512x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x512x512xf32>) -> tensor<512x512x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x512x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x512x512xf32>\n    memref.copy %2, %alloc : memref<512x512x512xf32> to memref<512x512x512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x512x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x512x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x512x512xf32>\n    return %3 : tensor<512x512x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x512x512xf32>) -> tensor<512x512x512xf32>\n  return %ret : tensor<512x512x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c512, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x512x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x512xf32>, tensor<1x1x1xf32>, tensor<512x512x512xf32>) -> (tensor<512x512x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32>_161": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32>\n  return %ret : tensor<124x28x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x28x252xf32>) -> tensor<124x28x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28x252xf32>\n    memref.copy %2, %alloc : memref<124x28x252xf32> to memref<124x28x252xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x28x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x28x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28x252xf32>\n    return %3 : tensor<124x28x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x252xf32>) -> tensor<124x28x252xf32>\n  return %ret : tensor<124x28x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x28x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x256xf32>, tensor<5x5x5xf32>, tensor<124x28x252xf32>) -> (tensor<124x28x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>_162": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x128xf32>\n    memref.copy %2, %alloc : memref<64x64x128xf32> to memref<64x64x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x128xf32>\n    return %3 : tensor<64x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<1x1x1xf32>, tensor<64x64x128xf32>) -> (tensor<64x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32>_163": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32>\n  return %ret : tensor<508x60x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x60x60xf32>) -> tensor<508x60x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60x60xf32>\n    memref.copy %2, %alloc : memref<508x60x60xf32> to memref<508x60x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x60x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x60x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60x60xf32>\n    return %3 : tensor<508x60x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x60xf32>) -> tensor<508x60x60xf32>\n  return %ret : tensor<508x60x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x60x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x64xf32>, tensor<5x5x5xf32>, tensor<508x60x60xf32>) -> (tensor<508x60x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32>_164": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32>\n  return %ret : tensor<1024x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256x32xf32>\n    memref.copy %2, %alloc : memref<1024x256x32xf32> to memref<1024x256x32xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256x32xf32>\n    return %3 : tensor<1024x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x32xf32>) -> tensor<1024x256x32xf32>\n  return %ret : tensor<1024x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<1x1x1xf32>, tensor<1024x256x32xf32>) -> (tensor<1024x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>_165": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>\n  return %ret : tensor<128x1024x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024x32xf32>\n    memref.copy %2, %alloc : memref<128x1024x32xf32> to memref<128x1024x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x1024x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x1024x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024x32xf32>\n    return %3 : tensor<128x1024x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>\n  return %ret : tensor<128x1024x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<1x1x1xf32>, tensor<128x1024x32xf32>) -> (tensor<128x1024x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32>_166": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32>\n  return %ret : tensor<1018x58x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58x1018xf32>\n    memref.copy %2, %alloc : memref<1018x58x1018xf32> to memref<1018x58x1018xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x58x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x58x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58x1018xf32>\n    return %3 : tensor<1018x58x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x1018xf32>) -> tensor<1018x58x1018xf32>\n  return %ret : tensor<1018x58x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x58x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x1024xf32>, tensor<7x7x7xf32>, tensor<1018x58x1018xf32>) -> (tensor<1018x58x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>_167": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>\n  return %ret : tensor<60x124x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124x508xf32>\n    memref.copy %2, %alloc : memref<60x124x508xf32> to memref<60x124x508xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x124x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x124x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124x508xf32>\n    return %3 : tensor<60x124x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x508xf32>) -> tensor<60x124x508xf32>\n  return %ret : tensor<60x124x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x124x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x512xf32>, tensor<5x5x5xf32>, tensor<60x124x508xf32>) -> (tensor<60x124x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32>_168": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32>\n  return %ret : tensor<30x1022x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022x30xf32>\n    memref.copy %2, %alloc : memref<30x1022x30xf32> to memref<30x1022x30xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x1022x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x1022x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022x30xf32>\n    return %3 : tensor<30x1022x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x30xf32>) -> tensor<30x1022x30xf32>\n  return %ret : tensor<30x1022x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x1022x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x32xf32>, tensor<3x3x3xf32>, tensor<30x1022x30xf32>) -> (tensor<30x1022x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>_169": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x256x64xf32>\n    memref.copy %2, %alloc : memref<128x256x64xf32> to memref<128x256x64xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x256x64xf32>\n    return %3 : tensor<128x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x256x64xf32>) -> tensor<128x256x64xf32>\n  return %ret : tensor<128x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x64xf32>, tensor<1x1x1xf32>, tensor<128x256x64xf32>) -> (tensor<128x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32>_170": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32>\n  return %ret : tensor<1024x128x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128x1024xf32>\n    memref.copy %2, %alloc : memref<1024x128x1024xf32> to memref<1024x128x1024xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x128x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x128x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128x1024xf32>\n    return %3 : tensor<1024x128x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x1024xf32>) -> tensor<1024x128x1024xf32>\n  return %ret : tensor<1024x128x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x128x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x1024xf32>, tensor<1x1x1xf32>, tensor<1024x128x1024xf32>) -> (tensor<1024x128x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>_171": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>\n  return %ret : tensor<32x1024x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024x256xf32>\n    memref.copy %2, %alloc : memref<32x1024x256xf32> to memref<32x1024x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x1024x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x1024x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024x256xf32>\n    return %3 : tensor<32x1024x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x256xf32>) -> tensor<32x1024x256xf32>\n  return %ret : tensor<32x1024x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x1024x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x256xf32>, tensor<1x1x1xf32>, tensor<32x1024x256xf32>) -> (tensor<32x1024x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32>_172": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32>\n  return %ret : tensor<60x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x28x508xf32>) -> tensor<60x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28x508xf32>\n    memref.copy %2, %alloc : memref<60x28x508xf32> to memref<60x28x508xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28x508xf32>\n    return %3 : tensor<60x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x508xf32>) -> tensor<60x28x508xf32>\n  return %ret : tensor<60x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x512xf32>, tensor<5x5x5xf32>, tensor<60x28x508xf32>) -> (tensor<60x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32>_173": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32>\n  return %ret : tensor<252x252x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x252x60xf32>) -> tensor<252x252x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x252x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x252x60xf32>\n    memref.copy %2, %alloc : memref<252x252x60xf32> to memref<252x252x60xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x252x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x252x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x252x60xf32>\n    return %3 : tensor<252x252x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x252x60xf32>) -> tensor<252x252x60xf32>\n  return %ret : tensor<252x252x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c252, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x252x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x64xf32>, tensor<5x5x5xf32>, tensor<252x252x60xf32>) -> (tensor<252x252x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32>_174": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32>\n  return %ret : tensor<26x250x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x1018xf32>\n    memref.copy %2, %alloc : memref<26x250x1018xf32> to memref<26x250x1018xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x1018xf32>\n    return %3 : tensor<26x250x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x1018xf32>) -> tensor<26x250x1018xf32>\n  return %ret : tensor<26x250x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x1024xf32>, tensor<7x7x7xf32>, tensor<26x250x1018xf32>) -> (tensor<26x250x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32>_175": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32>\n  return %ret : tensor<128x1024x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024x128xf32>\n    memref.copy %2, %alloc : memref<128x1024x128xf32> to memref<128x1024x128xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x1024x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x1024x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024x128xf32>\n    return %3 : tensor<128x1024x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x128xf32>) -> tensor<128x1024x128xf32>\n  return %ret : tensor<128x1024x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x1024x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x128xf32>, tensor<1x1x1xf32>, tensor<128x1024x128xf32>) -> (tensor<128x1024x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32>_176": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32>\n  return %ret : tensor<124x124x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x124x252xf32>) -> tensor<124x124x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x124x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x124x252xf32>\n    memref.copy %2, %alloc : memref<124x124x252xf32> to memref<124x124x252xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x124x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x124x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x124x252xf32>\n    return %3 : tensor<124x124x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x124x252xf32>) -> tensor<124x124x252xf32>\n  return %ret : tensor<124x124x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c124, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x124x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x256xf32>, tensor<5x5x5xf32>, tensor<124x124x252xf32>) -> (tensor<124x124x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32>_177": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32>\n  return %ret : tensor<1022x510x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510x30xf32>\n    memref.copy %2, %alloc : memref<1022x510x30xf32> to memref<1022x510x30xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x510x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x510x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510x30xf32>\n    return %3 : tensor<1022x510x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x30xf32>) -> tensor<1022x510x30xf32>\n  return %ret : tensor<1022x510x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x510x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x32xf32>, tensor<3x3x3xf32>, tensor<1022x510x30xf32>) -> (tensor<1022x510x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32>_178": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32>\n  return %ret : tensor<508x28x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x28x124xf32>) -> tensor<508x28x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28x124xf32>\n    memref.copy %2, %alloc : memref<508x28x124xf32> to memref<508x28x124xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x28x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x28x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28x124xf32>\n    return %3 : tensor<508x28x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x124xf32>) -> tensor<508x28x124xf32>\n  return %ret : tensor<508x28x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x28x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x128xf32>, tensor<5x5x5xf32>, tensor<508x28x124xf32>) -> (tensor<508x28x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>_179": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x250xf32>\n    memref.copy %2, %alloc : memref<58x122x250xf32> to memref<58x122x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x250xf32>\n    return %3 : tensor<58x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<7x7x7xf32>, tensor<58x122x250xf32>) -> (tensor<58x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>_180": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x30xf32>\n    memref.copy %2, %alloc : memref<1022x254x30xf32> to memref<1022x254x30xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x30xf32>\n    return %3 : tensor<1022x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<3x3x3xf32>, tensor<1022x254x30xf32>) -> (tensor<1022x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32>_181": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32>\n  return %ret : tensor<122x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x506x58xf32>) -> tensor<122x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506x58xf32>\n    memref.copy %2, %alloc : memref<122x506x58xf32> to memref<122x506x58xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506x58xf32>\n    return %3 : tensor<122x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x58xf32>) -> tensor<122x506x58xf32>\n  return %ret : tensor<122x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x64xf32>, tensor<7x7x7xf32>, tensor<122x506x58xf32>) -> (tensor<122x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32>_182": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32>\n  return %ret : tensor<28x1020x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020x28xf32>\n    memref.copy %2, %alloc : memref<28x1020x28xf32> to memref<28x1020x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x1020x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x1020x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020x28xf32>\n    return %3 : tensor<28x1020x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x28xf32>) -> tensor<28x1020x28xf32>\n  return %ret : tensor<28x1020x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x1020x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x32xf32>, tensor<5x5x5xf32>, tensor<28x1020x28xf32>) -> (tensor<28x1020x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32>_183": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32>\n  return %ret : tensor<250x26x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x26x506xf32>) -> tensor<250x26x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26x506xf32>\n    memref.copy %2, %alloc : memref<250x26x506xf32> to memref<250x26x506xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x26x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x26x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26x506xf32>\n    return %3 : tensor<250x26x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x506xf32>) -> tensor<250x26x506xf32>\n  return %ret : tensor<250x26x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x26x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x512xf32>, tensor<7x7x7xf32>, tensor<250x26x506xf32>) -> (tensor<250x26x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32>_184": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32>\n  return %ret : tensor<126x30x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x254xf32>) -> tensor<126x30x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x254xf32>\n    memref.copy %2, %alloc : memref<126x30x254xf32> to memref<126x30x254xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x254xf32>\n    return %3 : tensor<126x30x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x254xf32>) -> tensor<126x30x254xf32>\n  return %ret : tensor<126x30x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x256xf32>, tensor<3x3x3xf32>, tensor<126x30x254xf32>) -> (tensor<126x30x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32>_185": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32>\n  return %ret : tensor<1024x128x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128x32xf32>\n    memref.copy %2, %alloc : memref<1024x128x32xf32> to memref<1024x128x32xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x128x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x128x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128x32xf32>\n    return %3 : tensor<1024x128x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x32xf32>) -> tensor<1024x128x32xf32>\n  return %ret : tensor<1024x128x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x128x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x32xf32>, tensor<1x1x1xf32>, tensor<1024x128x32xf32>) -> (tensor<1024x128x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>_186": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>\n  return %ret : tensor<250x506x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506x506xf32>\n    memref.copy %2, %alloc : memref<250x506x506xf32> to memref<250x506x506xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x506x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x506x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506x506xf32>\n    return %3 : tensor<250x506x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x506xf32>) -> tensor<250x506x506xf32>\n  return %ret : tensor<250x506x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x506x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x512xf32>, tensor<7x7x7xf32>, tensor<250x506x506xf32>) -> (tensor<250x506x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>_187": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>\n  return %ret : tensor<60x1020x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x1020x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x1020x1020xf32>\n    memref.copy %2, %alloc : memref<60x1020x1020xf32> to memref<60x1020x1020xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x1020x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x1020x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x1020x1020xf32>\n    return %3 : tensor<60x1020x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>\n  return %ret : tensor<60x1020x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c1020, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x1020x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x1024xf32>, tensor<5x5x5xf32>, tensor<60x1020x1020xf32>) -> (tensor<60x1020x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32>_188": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32>\n  return %ret : tensor<1024x64x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x64x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x64x1024xf32>\n    memref.copy %2, %alloc : memref<1024x64x1024xf32> to memref<1024x64x1024xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x64x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x64x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x64x1024xf32>\n    return %3 : tensor<1024x64x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x64x1024xf32>) -> tensor<1024x64x1024xf32>\n  return %ret : tensor<1024x64x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c64, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x64x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x1024xf32>, tensor<1x1x1xf32>, tensor<1024x64x1024xf32>) -> (tensor<1024x64x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>_189": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>\n  return %ret : tensor<26x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x122x506xf32>\n    memref.copy %2, %alloc : memref<26x122x506xf32> to memref<26x122x506xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x122x506xf32>\n    return %3 : tensor<26x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>\n  return %ret : tensor<26x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x512xf32>, tensor<7x7x7xf32>, tensor<26x122x506xf32>) -> (tensor<26x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>_190": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>\n  return %ret : tensor<62x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x254xf32>\n    memref.copy %2, %alloc : memref<62x62x254xf32> to memref<62x62x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x254xf32>\n    return %3 : tensor<62x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>\n  return %ret : tensor<62x62x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x256xf32>, tensor<3x3x3xf32>, tensor<62x62x254xf32>) -> (tensor<62x62x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32>_191": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32>\n  return %ret : tensor<1018x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122x506xf32>\n    memref.copy %2, %alloc : memref<1018x122x506xf32> to memref<1018x122x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122x506xf32>\n    return %3 : tensor<1018x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x506xf32>) -> tensor<1018x122x506xf32>\n  return %ret : tensor<1018x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x512xf32>, tensor<7x7x7xf32>, tensor<1018x122x506xf32>) -> (tensor<1018x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32>_192": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32>\n  return %ret : tensor<1020x1020x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020x1020xf32>\n    memref.copy %2, %alloc : memref<1020x1020x1020xf32> to memref<1020x1020x1020xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020x1020xf32>\n    return %3 : tensor<1020x1020x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x1020xf32>) -> tensor<1020x1020x1020xf32>\n  return %ret : tensor<1020x1020x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x1020x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x1024xf32>, tensor<5x5x5xf32>, tensor<1020x1020x1020xf32>) -> (tensor<1020x1020x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>_193": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x250xf32>\n    memref.copy %2, %alloc : memref<26x250x250xf32> to memref<26x250x250xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x250xf32>\n    return %3 : tensor<26x250x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x256xf32>, tensor<7x7x7xf32>, tensor<26x250x250xf32>) -> (tensor<26x250x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>_194": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>\n  return %ret : tensor<256x64x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x1024xf32>\n    memref.copy %2, %alloc : memref<256x64x1024xf32> to memref<256x64x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x64x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x64x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x1024xf32>\n    return %3 : tensor<256x64x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>\n  return %ret : tensor<256x64x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x1024xf32>, tensor<1x1x1xf32>, tensor<256x64x1024xf32>) -> (tensor<256x64x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32>_195": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32>\n  return %ret : tensor<510x1022x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x1022x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x1022x62xf32>\n    memref.copy %2, %alloc : memref<510x1022x62xf32> to memref<510x1022x62xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x1022x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x1022x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x1022x62xf32>\n    return %3 : tensor<510x1022x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x62xf32>) -> tensor<510x1022x62xf32>\n  return %ret : tensor<510x1022x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c1022, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x1022x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x64xf32>, tensor<3x3x3xf32>, tensor<510x1022x62xf32>) -> (tensor<510x1022x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>_196": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>\n  return %ret : tensor<1020x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60x124xf32>\n    memref.copy %2, %alloc : memref<1020x60x124xf32> to memref<1020x60x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60x124xf32>\n    return %3 : tensor<1020x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>\n  return %ret : tensor<1020x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x128xf32>, tensor<5x5x5xf32>, tensor<1020x60x124xf32>) -> (tensor<1020x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32>_197": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32>\n  return %ret : tensor<1024x512x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x512x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x512x128xf32>\n    memref.copy %2, %alloc : memref<1024x512x128xf32> to memref<1024x512x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x512x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x512x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x512x128xf32>\n    return %3 : tensor<1024x512x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x512x128xf32>) -> tensor<1024x512x128xf32>\n  return %ret : tensor<1024x512x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c512, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x512x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x128xf32>, tensor<1x1x1xf32>, tensor<1024x512x128xf32>) -> (tensor<1024x512x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32>_198": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32>\n  return %ret : tensor<128x32x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x32x32xf32>) -> tensor<128x32x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x32xf32>\n    memref.copy %2, %alloc : memref<128x32x32xf32> to memref<128x32x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x32x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x32x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x32xf32>\n    return %3 : tensor<128x32x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x32xf32>) -> tensor<128x32x32xf32>\n  return %ret : tensor<128x32x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c32, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x32x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x32xf32>, tensor<1x1x1xf32>, tensor<128x32x32xf32>) -> (tensor<128x32x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>_199": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>\n  return %ret : tensor<1018x1018x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018x26xf32>\n    memref.copy %2, %alloc : memref<1018x1018x26xf32> to memref<1018x1018x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018x26xf32>\n    return %3 : tensor<1018x1018x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>\n  return %ret : tensor<1018x1018x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x1018x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x32xf32>, tensor<7x7x7xf32>, tensor<1018x1018x26xf32>) -> (tensor<1018x1018x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>_200": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>\n  return %ret : tensor<26x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x26xf32>\n    memref.copy %2, %alloc : memref<26x250x26xf32> to memref<26x250x26xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x26xf32>\n    return %3 : tensor<26x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x26xf32>) -> tensor<26x250x26xf32>\n  return %ret : tensor<26x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x32xf32>, tensor<7x7x7xf32>, tensor<26x250x26xf32>) -> (tensor<26x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32>_201": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32>\n  return %ret : tensor<30x510x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x510x510xf32>) -> tensor<30x510x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x510x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x510x510xf32>\n    memref.copy %2, %alloc : memref<30x510x510xf32> to memref<30x510x510xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x510x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x510x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x510x510xf32>\n    return %3 : tensor<30x510x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x510xf32>) -> tensor<30x510x510xf32>\n  return %ret : tensor<30x510x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c510, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x510x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x512xf32>, tensor<3x3x3xf32>, tensor<30x510x510xf32>) -> (tensor<30x510x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32>_202": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32>\n  return %ret : tensor<508x1020x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x1020x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x1020x1020xf32>\n    memref.copy %2, %alloc : memref<508x1020x1020xf32> to memref<508x1020x1020xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x1020x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x1020x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x1020x1020xf32>\n    return %3 : tensor<508x1020x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x1020x1020xf32>) -> tensor<508x1020x1020xf32>\n  return %ret : tensor<508x1020x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c1020, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x1020x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x1024xf32>, tensor<5x5x5xf32>, tensor<508x1020x1020xf32>) -> (tensor<508x1020x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>_203": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>\n  return %ret : tensor<26x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x122x506xf32>\n    memref.copy %2, %alloc : memref<26x122x506xf32> to memref<26x122x506xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x122x506xf32>\n    return %3 : tensor<26x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x506xf32>) -> tensor<26x122x506xf32>\n  return %ret : tensor<26x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x512xf32>, tensor<7x7x7xf32>, tensor<26x122x506xf32>) -> (tensor<26x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32>_204": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32>\n  return %ret : tensor<30x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x30x126xf32>) -> tensor<30x30x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30x126xf32>\n    memref.copy %2, %alloc : memref<30x30x126xf32> to memref<30x30x126xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x30x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30x126xf32>\n    return %3 : tensor<30x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x126xf32>) -> tensor<30x30x126xf32>\n  return %ret : tensor<30x30x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x30x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x128xf32>, tensor<3x3x3xf32>, tensor<30x30x126xf32>) -> (tensor<30x30x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>_205": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x128xf32>\n    memref.copy %2, %alloc : memref<64x64x128xf32> to memref<64x64x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x128xf32>\n    return %3 : tensor<64x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<1x1x1xf32>, tensor<64x64x128xf32>) -> (tensor<64x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32>_206": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32>\n  return %ret : tensor<512x1024x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x128xf32>\n    memref.copy %2, %alloc : memref<512x1024x128xf32> to memref<512x1024x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x128xf32>\n    return %3 : tensor<512x1024x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x128xf32>) -> tensor<512x1024x128xf32>\n  return %ret : tensor<512x1024x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x128xf32>, tensor<1x1x1xf32>, tensor<512x1024x128xf32>) -> (tensor<512x1024x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32>_207": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32>\n  return %ret : tensor<122x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x122xf32>) -> tensor<122x250x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x122xf32>\n    memref.copy %2, %alloc : memref<122x250x122xf32> to memref<122x250x122xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x122xf32>\n    return %3 : tensor<122x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x122xf32>) -> tensor<122x250x122xf32>\n  return %ret : tensor<122x250x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x128xf32>, tensor<7x7x7xf32>, tensor<122x250x122xf32>) -> (tensor<122x250x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>_208": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>\n  return %ret : tensor<126x1022x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x1022x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x1022x30xf32>\n    memref.copy %2, %alloc : memref<126x1022x30xf32> to memref<126x1022x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x1022x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x1022x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x1022x30xf32>\n    return %3 : tensor<126x1022x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x1022x30xf32>) -> tensor<126x1022x30xf32>\n  return %ret : tensor<126x1022x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c1022, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x1022x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<3x3x3xf32>, tensor<126x1022x30xf32>) -> (tensor<126x1022x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32>_209": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32>\n  return %ret : tensor<128x1024x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024x256xf32>\n    memref.copy %2, %alloc : memref<128x1024x256xf32> to memref<128x1024x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x1024x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x1024x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024x256xf32>\n    return %3 : tensor<128x1024x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x256xf32>) -> tensor<128x1024x256xf32>\n  return %ret : tensor<128x1024x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x1024x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x256xf32>, tensor<1x1x1xf32>, tensor<128x1024x256xf32>) -> (tensor<128x1024x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>_210": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>\n  return %ret : tensor<1024x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128x128xf32>\n    memref.copy %2, %alloc : memref<1024x128x128xf32> to memref<1024x128x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128x128xf32>\n    return %3 : tensor<1024x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>\n  return %ret : tensor<1024x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<1x1x1xf32>, tensor<1024x128x128xf32>) -> (tensor<1024x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>_211": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>\n  return %ret : tensor<122x1018x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x58xf32>\n    memref.copy %2, %alloc : memref<122x1018x58xf32> to memref<122x1018x58xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x58xf32>\n    return %3 : tensor<122x1018x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x58xf32>) -> tensor<122x1018x58xf32>\n  return %ret : tensor<122x1018x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x64xf32>, tensor<7x7x7xf32>, tensor<122x1018x58xf32>) -> (tensor<122x1018x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>_212": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>\n  return %ret : tensor<1022x510x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x510x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x510x1022xf32>\n    memref.copy %2, %alloc : memref<1022x510x1022xf32> to memref<1022x510x1022xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x510x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x510x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x510x1022xf32>\n    return %3 : tensor<1022x510x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x510x1022xf32>) -> tensor<1022x510x1022xf32>\n  return %ret : tensor<1022x510x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c510, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x510x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x1024xf32>, tensor<3x3x3xf32>, tensor<1022x510x1022xf32>) -> (tensor<1022x510x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>_213": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>\n  return %ret : tensor<508x252x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x252xf32>\n    memref.copy %2, %alloc : memref<508x252x252xf32> to memref<508x252x252xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x252xf32>\n    return %3 : tensor<508x252x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x252xf32>) -> tensor<508x252x252xf32>\n  return %ret : tensor<508x252x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x256xf32>, tensor<5x5x5xf32>, tensor<508x252x252xf32>) -> (tensor<508x252x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32>_214": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32>\n  return %ret : tensor<28x252x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252x1020xf32>\n    memref.copy %2, %alloc : memref<28x252x1020xf32> to memref<28x252x1020xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x252x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x252x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252x1020xf32>\n    return %3 : tensor<28x252x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x1020xf32>) -> tensor<28x252x1020xf32>\n  return %ret : tensor<28x252x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x252x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x1024xf32>, tensor<5x5x5xf32>, tensor<28x252x1020xf32>) -> (tensor<28x252x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>_215": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>\n  return %ret : tensor<124x60x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x60x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x60x1020xf32>\n    memref.copy %2, %alloc : memref<124x60x1020xf32> to memref<124x60x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x60x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x60x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x60x1020xf32>\n    return %3 : tensor<124x60x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x60x1020xf32>) -> tensor<124x60x1020xf32>\n  return %ret : tensor<124x60x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c60, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x60x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x1024xf32>, tensor<5x5x5xf32>, tensor<124x60x1020xf32>) -> (tensor<124x60x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>_216": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x60xf32>\n    memref.copy %2, %alloc : memref<1020x28x60xf32> to memref<1020x28x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x60xf32>\n    return %3 : tensor<1020x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x64xf32>, tensor<5x5x5xf32>, tensor<1020x28x60xf32>) -> (tensor<1020x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32>_217": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32>\n  return %ret : tensor<122x506x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x506x26xf32>) -> tensor<122x506x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x506x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x506x26xf32>\n    memref.copy %2, %alloc : memref<122x506x26xf32> to memref<122x506x26xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x506x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x506x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x506x26xf32>\n    return %3 : tensor<122x506x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x506x26xf32>) -> tensor<122x506x26xf32>\n  return %ret : tensor<122x506x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c506, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x506x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x32xf32>, tensor<7x7x7xf32>, tensor<122x506x26xf32>) -> (tensor<122x506x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32>_218": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32>\n  return %ret : tensor<1018x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122x1018xf32>\n    memref.copy %2, %alloc : memref<1018x122x1018xf32> to memref<1018x122x1018xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122x1018xf32>\n    return %3 : tensor<1018x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x1018xf32>) -> tensor<1018x122x1018xf32>\n  return %ret : tensor<1018x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x1024xf32>, tensor<7x7x7xf32>, tensor<1018x122x1018xf32>) -> (tensor<1018x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>_219": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>\n  return %ret : tensor<128x32x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x32x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x32x512xf32>\n    memref.copy %2, %alloc : memref<128x32x512xf32> to memref<128x32x512xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x32x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x32x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x32x512xf32>\n    return %3 : tensor<128x32x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x32x512xf32>) -> tensor<128x32x512xf32>\n  return %ret : tensor<128x32x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<1x1x1xf32>, tensor<128x32x512xf32>) -> (tensor<128x32x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32>_220": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32>\n  return %ret : tensor<64x512x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x512x64xf32>) -> tensor<64x512x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x64xf32>\n    memref.copy %2, %alloc : memref<64x512x64xf32> to memref<64x512x64xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x512x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x512x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512x64xf32>\n    return %3 : tensor<64x512x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x64xf32>) -> tensor<64x512x64xf32>\n  return %ret : tensor<64x512x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x512x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x64xf32>, tensor<1x1x1xf32>, tensor<64x512x64xf32>) -> (tensor<64x512x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>_221": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>\n  return %ret : tensor<1018x1018x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018x26xf32>\n    memref.copy %2, %alloc : memref<1018x1018x26xf32> to memref<1018x1018x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018x26xf32>\n    return %3 : tensor<1018x1018x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x26xf32>) -> tensor<1018x1018x26xf32>\n  return %ret : tensor<1018x1018x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x1018x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x32xf32>, tensor<7x7x7xf32>, tensor<1018x1018x26xf32>) -> (tensor<1018x1018x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32>_222": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32>\n  return %ret : tensor<58x58x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x58x122xf32>) -> tensor<58x58x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58x122xf32>\n    memref.copy %2, %alloc : memref<58x58x122xf32> to memref<58x58x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x58x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x58x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58x122xf32>\n    return %3 : tensor<58x58x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x122xf32>) -> tensor<58x58x122xf32>\n  return %ret : tensor<58x58x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x58x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<7x7x7xf32>, tensor<58x58x122xf32>) -> (tensor<58x58x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32>_223": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32>\n  return %ret : tensor<62x1022x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x1022x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x1022x62xf32>\n    memref.copy %2, %alloc : memref<62x1022x62xf32> to memref<62x1022x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x1022x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x1022x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x1022x62xf32>\n    return %3 : tensor<62x1022x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x62xf32>) -> tensor<62x1022x62xf32>\n  return %ret : tensor<62x1022x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c1022, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x1022x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x64xf32>, tensor<3x3x3xf32>, tensor<62x1022x62xf32>) -> (tensor<62x1022x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>_224": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x60xf32>\n    memref.copy %2, %alloc : memref<508x124x60xf32> to memref<508x124x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x60xf32>\n    return %3 : tensor<508x124x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x64xf32>, tensor<5x5x5xf32>, tensor<508x124x60xf32>) -> (tensor<508x124x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>_225": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x60xf32>\n    memref.copy %2, %alloc : memref<508x124x60xf32> to memref<508x124x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x60xf32>\n    return %3 : tensor<508x124x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x64xf32>, tensor<5x5x5xf32>, tensor<508x124x60xf32>) -> (tensor<508x124x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32>_226": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32>\n  return %ret : tensor<58x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018x506xf32>\n    memref.copy %2, %alloc : memref<58x1018x506xf32> to memref<58x1018x506xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018x506xf32>\n    return %3 : tensor<58x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x506xf32>) -> tensor<58x1018x506xf32>\n  return %ret : tensor<58x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x512xf32>, tensor<7x7x7xf32>, tensor<58x1018x506xf32>) -> (tensor<58x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>_227": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>\n  return %ret : tensor<254x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126x126xf32>\n    memref.copy %2, %alloc : memref<254x126x126xf32> to memref<254x126x126xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126x126xf32>\n    return %3 : tensor<254x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x126xf32>) -> tensor<254x126x126xf32>\n  return %ret : tensor<254x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x128xf32>, tensor<3x3x3xf32>, tensor<254x126x126xf32>) -> (tensor<254x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32>_228": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32>\n  return %ret : tensor<506x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x122x250xf32>) -> tensor<506x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x122x250xf32>\n    memref.copy %2, %alloc : memref<506x122x250xf32> to memref<506x122x250xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x122x250xf32>\n    return %3 : tensor<506x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x250xf32>) -> tensor<506x122x250xf32>\n  return %ret : tensor<506x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x256xf32>, tensor<7x7x7xf32>, tensor<506x122x250xf32>) -> (tensor<506x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>_229": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>\n  return %ret : tensor<1018x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506x58xf32>\n    memref.copy %2, %alloc : memref<1018x506x58xf32> to memref<1018x506x58xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506x58xf32>\n    return %3 : tensor<1018x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>\n  return %ret : tensor<1018x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x64xf32>, tensor<7x7x7xf32>, tensor<1018x506x58xf32>) -> (tensor<1018x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>_230": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>\n  return %ret : tensor<1022x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x510xf32>\n    memref.copy %2, %alloc : memref<1022x254x510xf32> to memref<1022x254x510xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x510xf32>\n    return %3 : tensor<1022x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x510xf32>) -> tensor<1022x254x510xf32>\n  return %ret : tensor<1022x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x512xf32>, tensor<3x3x3xf32>, tensor<1022x254x510xf32>) -> (tensor<1022x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32>_231": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32>\n  return %ret : tensor<122x58x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x58x506xf32>) -> tensor<122x58x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x58x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x58x506xf32>\n    memref.copy %2, %alloc : memref<122x58x506xf32> to memref<122x58x506xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x58x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x58x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x58x506xf32>\n    return %3 : tensor<122x58x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x506xf32>) -> tensor<122x58x506xf32>\n  return %ret : tensor<122x58x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c58, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x58x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x512xf32>, tensor<7x7x7xf32>, tensor<122x58x506xf32>) -> (tensor<122x58x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32>_232": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32>\n  return %ret : tensor<254x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x30x30xf32>) -> tensor<254x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x30x30xf32>\n    memref.copy %2, %alloc : memref<254x30x30xf32> to memref<254x30x30xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x30x30xf32>\n    return %3 : tensor<254x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x30xf32>) -> tensor<254x30x30xf32>\n  return %ret : tensor<254x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x32xf32>, tensor<3x3x3xf32>, tensor<254x30x30xf32>) -> (tensor<254x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32>_233": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32>\n  return %ret : tensor<64x512x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x512x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x512xf32>\n    memref.copy %2, %alloc : memref<64x512x512xf32> to memref<64x512x512xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x512x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x512x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512x512xf32>\n    return %3 : tensor<64x512x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x512xf32>) -> tensor<64x512x512xf32>\n  return %ret : tensor<64x512x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x512x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x512xf32>, tensor<1x1x1xf32>, tensor<64x512x512xf32>) -> (tensor<64x512x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>_234": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>\n  return %ret : tensor<62x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x254xf32>\n    memref.copy %2, %alloc : memref<62x62x254xf32> to memref<62x62x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x254xf32>\n    return %3 : tensor<62x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x254xf32>) -> tensor<62x62x254xf32>\n  return %ret : tensor<62x62x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x256xf32>, tensor<3x3x3xf32>, tensor<62x62x254xf32>) -> (tensor<62x62x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>_235": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>\n  return %ret : tensor<1022x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126x126xf32>\n    memref.copy %2, %alloc : memref<1022x126x126xf32> to memref<1022x126x126xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126x126xf32>\n    return %3 : tensor<1022x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>\n  return %ret : tensor<1022x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<3x3x3xf32>, tensor<1022x126x126xf32>) -> (tensor<1022x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32>_236": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32>\n  return %ret : tensor<506x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x506x58xf32>) -> tensor<506x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506x58xf32>\n    memref.copy %2, %alloc : memref<506x506x58xf32> to memref<506x506x58xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506x58xf32>\n    return %3 : tensor<506x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x58xf32>) -> tensor<506x506x58xf32>\n  return %ret : tensor<506x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x64xf32>, tensor<7x7x7xf32>, tensor<506x506x58xf32>) -> (tensor<506x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32>_237": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32>\n  return %ret : tensor<124x252x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x252x508xf32>) -> tensor<124x252x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252x508xf32>\n    memref.copy %2, %alloc : memref<124x252x508xf32> to memref<124x252x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x252x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x252x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252x508xf32>\n    return %3 : tensor<124x252x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x508xf32>) -> tensor<124x252x508xf32>\n  return %ret : tensor<124x252x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x252x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x512xf32>, tensor<5x5x5xf32>, tensor<124x252x508xf32>) -> (tensor<124x252x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>_238": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>\n  return %ret : tensor<512x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256x64xf32>\n    memref.copy %2, %alloc : memref<512x256x64xf32> to memref<512x256x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x256x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256x64xf32>\n    return %3 : tensor<512x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x64xf32>) -> tensor<512x256x64xf32>\n  return %ret : tensor<512x256x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x64xf32>, tensor<1x1x1xf32>, tensor<512x256x64xf32>) -> (tensor<512x256x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32>_239": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32>\n  return %ret : tensor<58x506x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x506x250xf32>) -> tensor<58x506x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506x250xf32>\n    memref.copy %2, %alloc : memref<58x506x250xf32> to memref<58x506x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x506x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x506x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506x250xf32>\n    return %3 : tensor<58x506x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x250xf32>) -> tensor<58x506x250xf32>\n  return %ret : tensor<58x506x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x506x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x256xf32>, tensor<7x7x7xf32>, tensor<58x506x250xf32>) -> (tensor<58x506x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32>_240": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32>\n  return %ret : tensor<126x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x126xf32>) -> tensor<126x30x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x126xf32>\n    memref.copy %2, %alloc : memref<126x30x126xf32> to memref<126x30x126xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x126xf32>\n    return %3 : tensor<126x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x126xf32>) -> tensor<126x30x126xf32>\n  return %ret : tensor<126x30x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x128xf32>, tensor<3x3x3xf32>, tensor<126x30x126xf32>) -> (tensor<126x30x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>_241": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x512xf32>\n    memref.copy %2, %alloc : memref<256x1024x512xf32> to memref<256x1024x512xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024x512xf32>\n    return %3 : tensor<256x1024x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x512xf32>, tensor<1x1x1xf32>, tensor<256x1024x512xf32>) -> (tensor<256x1024x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>_242": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>\n  return %ret : tensor<58x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250x58xf32>\n    memref.copy %2, %alloc : memref<58x250x58xf32> to memref<58x250x58xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250x58xf32>\n    return %3 : tensor<58x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x58xf32>) -> tensor<58x250x58xf32>\n  return %ret : tensor<58x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x64xf32>, tensor<7x7x7xf32>, tensor<58x250x58xf32>) -> (tensor<58x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32>_243": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32>\n  return %ret : tensor<506x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x122x58xf32>) -> tensor<506x122x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x122x58xf32>\n    memref.copy %2, %alloc : memref<506x122x58xf32> to memref<506x122x58xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x122x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x122x58xf32>\n    return %3 : tensor<506x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x122x58xf32>) -> tensor<506x122x58xf32>\n  return %ret : tensor<506x122x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c122, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x122x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x64xf32>, tensor<7x7x7xf32>, tensor<506x122x58xf32>) -> (tensor<506x122x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>_244": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>\n  return %ret : tensor<32x1024x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024x512xf32>\n    memref.copy %2, %alloc : memref<32x1024x512xf32> to memref<32x1024x512xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x1024x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x1024x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024x512xf32>\n    return %3 : tensor<32x1024x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>\n  return %ret : tensor<32x1024x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x512xf32>, tensor<1x1x1xf32>, tensor<32x1024x512xf32>) -> (tensor<32x1024x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32>_245": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32>\n  return %ret : tensor<510x510x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x510x30xf32>) -> tensor<510x510x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510x30xf32>\n    memref.copy %2, %alloc : memref<510x510x30xf32> to memref<510x510x30xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x510x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x510x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510x30xf32>\n    return %3 : tensor<510x510x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x30xf32>) -> tensor<510x510x30xf32>\n  return %ret : tensor<510x510x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x510x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x32xf32>, tensor<3x3x3xf32>, tensor<510x510x30xf32>) -> (tensor<510x510x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>_246": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>\n  return %ret : tensor<1024x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x128x128xf32>\n    memref.copy %2, %alloc : memref<1024x128x128xf32> to memref<1024x128x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x128x128xf32>\n    return %3 : tensor<1024x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x128x128xf32>) -> tensor<1024x128x128xf32>\n  return %ret : tensor<1024x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<1x1x1xf32>, tensor<1024x128x128xf32>) -> (tensor<1024x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32>_247": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32>\n  return %ret : tensor<510x510x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x510x254xf32>) -> tensor<510x510x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510x254xf32>\n    memref.copy %2, %alloc : memref<510x510x254xf32> to memref<510x510x254xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x510x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x510x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510x254xf32>\n    return %3 : tensor<510x510x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x254xf32>) -> tensor<510x510x254xf32>\n  return %ret : tensor<510x510x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x510x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x256xf32>, tensor<3x3x3xf32>, tensor<510x510x254xf32>) -> (tensor<510x510x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>_248": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>\n  return %ret : tensor<506x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018x122xf32>\n    memref.copy %2, %alloc : memref<506x1018x122xf32> to memref<506x1018x122xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018x122xf32>\n    return %3 : tensor<506x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>\n  return %ret : tensor<506x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x128xf32>, tensor<7x7x7xf32>, tensor<506x1018x122xf32>) -> (tensor<506x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>_249": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>\n  return %ret : tensor<508x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60x124xf32>\n    memref.copy %2, %alloc : memref<508x60x124xf32> to memref<508x60x124xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60x124xf32>\n    return %3 : tensor<508x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>\n  return %ret : tensor<508x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x128xf32>, tensor<5x5x5xf32>, tensor<508x60x124xf32>) -> (tensor<508x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>_250": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>\n  return %ret : tensor<254x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x62x30xf32>\n    memref.copy %2, %alloc : memref<254x62x30xf32> to memref<254x62x30xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x62x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x62x30xf32>\n    return %3 : tensor<254x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>\n  return %ret : tensor<254x62x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c62, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x62x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x32xf32>, tensor<3x3x3xf32>, tensor<254x62x30xf32>) -> (tensor<254x62x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32>_251": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32>\n  return %ret : tensor<28x1020x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x1020x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x1020x1020xf32>\n    memref.copy %2, %alloc : memref<28x1020x1020xf32> to memref<28x1020x1020xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x1020x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x1020x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x1020x1020xf32>\n    return %3 : tensor<28x1020x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x1020x1020xf32>) -> tensor<28x1020x1020xf32>\n  return %ret : tensor<28x1020x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c1020, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x1020x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x1024xf32>, tensor<5x5x5xf32>, tensor<28x1020x1020xf32>) -> (tensor<28x1020x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>_252": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>\n  return %ret : tensor<126x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126x62xf32>\n    memref.copy %2, %alloc : memref<126x126x62xf32> to memref<126x126x62xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x126x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126x62xf32>\n    return %3 : tensor<126x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>\n  return %ret : tensor<126x126x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x126x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x64xf32>, tensor<3x3x3xf32>, tensor<126x126x62xf32>) -> (tensor<126x126x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>_253": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x1022xf32>\n    memref.copy %2, %alloc : memref<254x254x1022xf32> to memref<254x254x1022xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x1022xf32>\n    return %3 : tensor<254x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<3x3x3xf32>, tensor<254x254x1022xf32>) -> (tensor<254x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32>_254": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32>\n  return %ret : tensor<122x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x122x250xf32>) -> tensor<122x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122x250xf32>\n    memref.copy %2, %alloc : memref<122x122x250xf32> to memref<122x122x250xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122x250xf32>\n    return %3 : tensor<122x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x250xf32>) -> tensor<122x122x250xf32>\n  return %ret : tensor<122x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x256xf32>, tensor<7x7x7xf32>, tensor<122x122x250xf32>) -> (tensor<122x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32>_255": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32>\n  return %ret : tensor<122x1018x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x26xf32>\n    memref.copy %2, %alloc : memref<122x1018x26xf32> to memref<122x1018x26xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x26xf32>\n    return %3 : tensor<122x1018x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x26xf32>) -> tensor<122x1018x26xf32>\n  return %ret : tensor<122x1018x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<7x7x7xf32>, tensor<122x1018x26xf32>) -> (tensor<122x1018x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>_256": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x1024xf32>\n    memref.copy %2, %alloc : memref<256x256x1024xf32> to memref<256x256x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x1024xf32>\n    return %3 : tensor<256x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x1024xf32>) -> tensor<256x256x1024xf32>\n  return %ret : tensor<256x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<1x1x1xf32>, tensor<256x256x1024xf32>) -> (tensor<256x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>_257": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>\n  return %ret : tensor<126x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x254x30xf32>\n    memref.copy %2, %alloc : memref<126x254x30xf32> to memref<126x254x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x254x30xf32>\n    return %3 : tensor<126x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x254x30xf32>) -> tensor<126x254x30xf32>\n  return %ret : tensor<126x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<3x3x3xf32>, tensor<126x254x30xf32>) -> (tensor<126x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32>_258": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32>\n  return %ret : tensor<506x506x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x506x506xf32>) -> tensor<506x506x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506x506xf32>\n    memref.copy %2, %alloc : memref<506x506x506xf32> to memref<506x506x506xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x506x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x506x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506x506xf32>\n    return %3 : tensor<506x506x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x506xf32>) -> tensor<506x506x506xf32>\n  return %ret : tensor<506x506x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x506x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x512xf32>, tensor<7x7x7xf32>, tensor<506x506x506xf32>) -> (tensor<506x506x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32>_259": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32>\n  return %ret : tensor<254x1022x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022x62xf32>\n    memref.copy %2, %alloc : memref<254x1022x62xf32> to memref<254x1022x62xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x1022x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x1022x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022x62xf32>\n    return %3 : tensor<254x1022x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x62xf32>) -> tensor<254x1022x62xf32>\n  return %ret : tensor<254x1022x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x1022x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x64xf32>, tensor<3x3x3xf32>, tensor<254x1022x62xf32>) -> (tensor<254x1022x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32>_260": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32>\n  return %ret : tensor<126x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x126x126xf32>) -> tensor<126x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126x126xf32>\n    memref.copy %2, %alloc : memref<126x126x126xf32> to memref<126x126x126xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126x126xf32>\n    return %3 : tensor<126x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x126xf32>) -> tensor<126x126x126xf32>\n  return %ret : tensor<126x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x128xf32>, tensor<3x3x3xf32>, tensor<126x126x126xf32>) -> (tensor<126x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32>_261": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32>\n  return %ret : tensor<30x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x126x126xf32>) -> tensor<30x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126x126xf32>\n    memref.copy %2, %alloc : memref<30x126x126xf32> to memref<30x126x126xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126x126xf32>\n    return %3 : tensor<30x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x126xf32>) -> tensor<30x126x126xf32>\n  return %ret : tensor<30x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x128xf32>, tensor<3x3x3xf32>, tensor<30x126x126xf32>) -> (tensor<30x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32>_262": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32>\n  return %ret : tensor<30x30x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x30x510xf32>) -> tensor<30x30x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30x510xf32>\n    memref.copy %2, %alloc : memref<30x30x510xf32> to memref<30x30x510xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x30x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x30x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30x510xf32>\n    return %3 : tensor<30x30x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x510xf32>) -> tensor<30x30x510xf32>\n  return %ret : tensor<30x30x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x30x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x512xf32>, tensor<3x3x3xf32>, tensor<30x30x510xf32>) -> (tensor<30x30x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32>_263": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32>\n  return %ret : tensor<32x64x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x64x512xf32>) -> tensor<32x64x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x64x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x512xf32>\n    memref.copy %2, %alloc : memref<32x64x512xf32> to memref<32x64x512xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x64x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x64x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x64x512xf32>\n    return %3 : tensor<32x64x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x64x512xf32>) -> tensor<32x64x512xf32>\n  return %ret : tensor<32x64x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c64, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x64x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x512xf32>, tensor<1x1x1xf32>, tensor<32x64x512xf32>) -> (tensor<32x64x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32>_264": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32>\n  return %ret : tensor<510x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x254xf32>) -> tensor<510x62x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x254xf32>\n    memref.copy %2, %alloc : memref<510x62x254xf32> to memref<510x62x254xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x254xf32>\n    return %3 : tensor<510x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x254xf32>) -> tensor<510x62x254xf32>\n  return %ret : tensor<510x62x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x256xf32>, tensor<3x3x3xf32>, tensor<510x62x254xf32>) -> (tensor<510x62x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32>_265": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32>\n  return %ret : tensor<30x126x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x126x510xf32>) -> tensor<30x126x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x126x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x126x510xf32>\n    memref.copy %2, %alloc : memref<30x126x510xf32> to memref<30x126x510xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x126x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x126x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x126x510xf32>\n    return %3 : tensor<30x126x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x126x510xf32>) -> tensor<30x126x510xf32>\n  return %ret : tensor<30x126x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c126, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x126x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x512xf32>, tensor<3x3x3xf32>, tensor<30x126x510xf32>) -> (tensor<30x126x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>_266": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x512xf32>\n    memref.copy %2, %alloc : memref<256x1024x512xf32> to memref<256x1024x512xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024x512xf32>\n    return %3 : tensor<256x1024x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x512xf32>, tensor<1x1x1xf32>, tensor<256x1024x512xf32>) -> (tensor<256x1024x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>_267": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>\n  return %ret : tensor<508x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x28x60xf32>\n    memref.copy %2, %alloc : memref<508x28x60xf32> to memref<508x28x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x28x60xf32>\n    return %3 : tensor<508x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x28x60xf32>) -> tensor<508x28x60xf32>\n  return %ret : tensor<508x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x64xf32>, tensor<5x5x5xf32>, tensor<508x28x60xf32>) -> (tensor<508x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32>_268": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32>\n  return %ret : tensor<1022x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62x254xf32>\n    memref.copy %2, %alloc : memref<1022x62x254xf32> to memref<1022x62x254xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x62x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62x254xf32>\n    return %3 : tensor<1022x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x254xf32>) -> tensor<1022x62x254xf32>\n  return %ret : tensor<1022x62x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x62x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x256xf32>, tensor<3x3x3xf32>, tensor<1022x62x254xf32>) -> (tensor<1022x62x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32>_269": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32>\n  return %ret : tensor<26x506x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x506x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x506x1018xf32>\n    memref.copy %2, %alloc : memref<26x506x1018xf32> to memref<26x506x1018xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x506x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x506x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x506x1018xf32>\n    return %3 : tensor<26x506x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x506x1018xf32>) -> tensor<26x506x1018xf32>\n  return %ret : tensor<26x506x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c506, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x506x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x1024xf32>, tensor<7x7x7xf32>, tensor<26x506x1018xf32>) -> (tensor<26x506x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>_270": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x60xf32>\n    memref.copy %2, %alloc : memref<1020x28x60xf32> to memref<1020x28x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x60xf32>\n    return %3 : tensor<1020x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x64xf32>, tensor<5x5x5xf32>, tensor<1020x28x60xf32>) -> (tensor<1020x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32>_271": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32>\n  return %ret : tensor<1022x30x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30x254xf32>\n    memref.copy %2, %alloc : memref<1022x30x254xf32> to memref<1022x30x254xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x30x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x30x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30x254xf32>\n    return %3 : tensor<1022x30x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x254xf32>) -> tensor<1022x30x254xf32>\n  return %ret : tensor<1022x30x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x30x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x256xf32>, tensor<3x3x3xf32>, tensor<1022x30x254xf32>) -> (tensor<1022x30x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32>_272": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32>\n  return %ret : tensor<122x122x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x122x122xf32>) -> tensor<122x122x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122x122xf32>\n    memref.copy %2, %alloc : memref<122x122x122xf32> to memref<122x122x122xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x122x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x122x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122x122xf32>\n    return %3 : tensor<122x122x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x122xf32>) -> tensor<122x122x122xf32>\n  return %ret : tensor<122x122x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x122x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x128xf32>, tensor<7x7x7xf32>, tensor<122x122x122xf32>) -> (tensor<122x122x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32>_273": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32>\n  return %ret : tensor<510x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x254x254xf32>) -> tensor<510x254x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254x254xf32>\n    memref.copy %2, %alloc : memref<510x254x254xf32> to memref<510x254x254xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x254x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254x254xf32>\n    return %3 : tensor<510x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x254xf32>) -> tensor<510x254x254xf32>\n  return %ret : tensor<510x254x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x254x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x256xf32>, tensor<3x3x3xf32>, tensor<510x254x254xf32>) -> (tensor<510x254x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32>_274": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32>\n  return %ret : tensor<26x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x1018x122xf32>\n    memref.copy %2, %alloc : memref<26x1018x122xf32> to memref<26x1018x122xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x1018x122xf32>\n    return %3 : tensor<26x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x1018x122xf32>) -> tensor<26x1018x122xf32>\n  return %ret : tensor<26x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x128xf32>, tensor<7x7x7xf32>, tensor<26x1018x122xf32>) -> (tensor<26x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>_275": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>\n  return %ret : tensor<250x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250x58xf32>\n    memref.copy %2, %alloc : memref<250x250x58xf32> to memref<250x250x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250x58xf32>\n    return %3 : tensor<250x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>\n  return %ret : tensor<250x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x64xf32>, tensor<7x7x7xf32>, tensor<250x250x58xf32>) -> (tensor<250x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32>_276": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32>\n  return %ret : tensor<1020x60x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60x28xf32>\n    memref.copy %2, %alloc : memref<1020x60x28xf32> to memref<1020x60x28xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x60x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x60x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60x28xf32>\n    return %3 : tensor<1020x60x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x28xf32>) -> tensor<1020x60x28xf32>\n  return %ret : tensor<1020x60x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x60x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x32xf32>, tensor<5x5x5xf32>, tensor<1020x60x28xf32>) -> (tensor<1020x60x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32>_277": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32>\n  return %ret : tensor<60x252x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x252x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x252x1020xf32>\n    memref.copy %2, %alloc : memref<60x252x1020xf32> to memref<60x252x1020xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x252x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x252x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x252x1020xf32>\n    return %3 : tensor<60x252x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x252x1020xf32>) -> tensor<60x252x1020xf32>\n  return %ret : tensor<60x252x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c252, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x252x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x1024xf32>, tensor<5x5x5xf32>, tensor<60x252x1020xf32>) -> (tensor<60x252x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>_278": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x254x1022xf32>\n    memref.copy %2, %alloc : memref<254x254x1022xf32> to memref<254x254x1022xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x254x1022xf32>\n    return %3 : tensor<254x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x254x1022xf32>) -> tensor<254x254x1022xf32>\n  return %ret : tensor<254x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<3x3x3xf32>, tensor<254x254x1022xf32>) -> (tensor<254x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32>_279": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32>\n  return %ret : tensor<1018x58x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x58x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x58x26xf32>\n    memref.copy %2, %alloc : memref<1018x58x26xf32> to memref<1018x58x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x58x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x58x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x58x26xf32>\n    return %3 : tensor<1018x58x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x58x26xf32>) -> tensor<1018x58x26xf32>\n  return %ret : tensor<1018x58x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c58, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x58x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x32xf32>, tensor<7x7x7xf32>, tensor<1018x58x26xf32>) -> (tensor<1018x58x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>_280": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>\n  return %ret : tensor<508x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x28xf32>\n    memref.copy %2, %alloc : memref<508x124x28xf32> to memref<508x124x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x28xf32>\n    return %3 : tensor<508x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>\n  return %ret : tensor<508x124x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x32xf32>, tensor<5x5x5xf32>, tensor<508x124x28xf32>) -> (tensor<508x124x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32>_281": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32>\n  return %ret : tensor<32x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x32x128xf32>) -> tensor<32x32x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x128xf32>\n    memref.copy %2, %alloc : memref<32x32x128xf32> to memref<32x32x128xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x32x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32x128xf32>\n    return %3 : tensor<32x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x128xf32>) -> tensor<32x32x128xf32>\n  return %ret : tensor<32x32x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x32x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x128xf32>, tensor<1x1x1xf32>, tensor<32x32x128xf32>) -> (tensor<32x32x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>_282": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>\n  return %ret : tensor<510x62x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x1022xf32>\n    memref.copy %2, %alloc : memref<510x62x1022xf32> to memref<510x62x1022xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x1022xf32>\n    return %3 : tensor<510x62x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x1022xf32>) -> tensor<510x62x1022xf32>\n  return %ret : tensor<510x62x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x1024xf32>, tensor<3x3x3xf32>, tensor<510x62x1022xf32>) -> (tensor<510x62x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>_283": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>\n  return %ret : tensor<62x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x126xf32>\n    memref.copy %2, %alloc : memref<62x62x126xf32> to memref<62x62x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x126xf32>\n    return %3 : tensor<62x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x126xf32>) -> tensor<62x62x126xf32>\n  return %ret : tensor<62x62x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<3x3x3xf32>, tensor<62x62x126xf32>) -> (tensor<62x62x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>_284": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>\n  return %ret : tensor<60x1020x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x1020x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x1020x1020xf32>\n    memref.copy %2, %alloc : memref<60x1020x1020xf32> to memref<60x1020x1020xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x1020x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x1020x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x1020x1020xf32>\n    return %3 : tensor<60x1020x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x1020x1020xf32>) -> tensor<60x1020x1020xf32>\n  return %ret : tensor<60x1020x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c1020, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x1020x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x1024xf32>, tensor<5x5x5xf32>, tensor<60x1020x1020xf32>) -> (tensor<60x1020x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32>_285": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32>\n  return %ret : tensor<124x252x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x252x28xf32>) -> tensor<124x252x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252x28xf32>\n    memref.copy %2, %alloc : memref<124x252x28xf32> to memref<124x252x28xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x252x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x252x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252x28xf32>\n    return %3 : tensor<124x252x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x28xf32>) -> tensor<124x252x28xf32>\n  return %ret : tensor<124x252x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x252x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x32xf32>, tensor<5x5x5xf32>, tensor<124x252x28xf32>) -> (tensor<124x252x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32>_286": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32>\n  return %ret : tensor<62x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x254x510xf32>) -> tensor<62x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254x510xf32>\n    memref.copy %2, %alloc : memref<62x254x510xf32> to memref<62x254x510xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254x510xf32>\n    return %3 : tensor<62x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x510xf32>) -> tensor<62x254x510xf32>\n  return %ret : tensor<62x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x512xf32>, tensor<3x3x3xf32>, tensor<62x254x510xf32>) -> (tensor<62x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32>_287": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32>\n  return %ret : tensor<26x26x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x26x250xf32>) -> tensor<26x26x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x26x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x26x250xf32>\n    memref.copy %2, %alloc : memref<26x26x250xf32> to memref<26x26x250xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x26x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x26x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x26x250xf32>\n    return %3 : tensor<26x26x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x26x250xf32>) -> tensor<26x26x250xf32>\n  return %ret : tensor<26x26x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c26, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x26x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x256xf32>, tensor<7x7x7xf32>, tensor<26x26x250xf32>) -> (tensor<26x26x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>_288": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>\n  return %ret : tensor<64x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x1024xf32>\n    memref.copy %2, %alloc : memref<64x32x1024xf32> to memref<64x32x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32x1024xf32>\n    return %3 : tensor<64x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>\n  return %ret : tensor<64x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x1024xf32>, tensor<1x1x1xf32>, tensor<64x32x1024xf32>) -> (tensor<64x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>_289": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>\n  return %ret : tensor<254x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x62x30xf32>\n    memref.copy %2, %alloc : memref<254x62x30xf32> to memref<254x62x30xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x62x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x62x30xf32>\n    return %3 : tensor<254x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x62x30xf32>) -> tensor<254x62x30xf32>\n  return %ret : tensor<254x62x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c62, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x62x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x32xf32>, tensor<3x3x3xf32>, tensor<254x62x30xf32>) -> (tensor<254x62x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32>_290": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32>\n  return %ret : tensor<1024x1024x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x1024x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x1024x128xf32>\n    memref.copy %2, %alloc : memref<1024x1024x128xf32> to memref<1024x1024x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x1024x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x1024x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x1024x128xf32>\n    return %3 : tensor<1024x1024x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x128xf32>) -> tensor<1024x1024x128xf32>\n  return %ret : tensor<1024x1024x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c1024, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x1024x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x128xf32>, tensor<1x1x1xf32>, tensor<1024x1024x128xf32>) -> (tensor<1024x1024x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32>_291": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32>\n  return %ret : tensor<250x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x58xf32>) -> tensor<250x122x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x58xf32>\n    memref.copy %2, %alloc : memref<250x122x58xf32> to memref<250x122x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x58xf32>\n    return %3 : tensor<250x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x58xf32>) -> tensor<250x122x58xf32>\n  return %ret : tensor<250x122x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x64xf32>, tensor<7x7x7xf32>, tensor<250x122x58xf32>) -> (tensor<250x122x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32>_292": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32>\n  return %ret : tensor<510x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x126xf32>) -> tensor<510x62x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x126xf32>\n    memref.copy %2, %alloc : memref<510x62x126xf32> to memref<510x62x126xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x126xf32>\n    return %3 : tensor<510x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x126xf32>) -> tensor<510x62x126xf32>\n  return %ret : tensor<510x62x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x128xf32>, tensor<3x3x3xf32>, tensor<510x62x126xf32>) -> (tensor<510x62x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32>_293": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32>\n  return %ret : tensor<124x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x28x508xf32>) -> tensor<124x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28x508xf32>\n    memref.copy %2, %alloc : memref<124x28x508xf32> to memref<124x28x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28x508xf32>\n    return %3 : tensor<124x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x508xf32>) -> tensor<124x28x508xf32>\n  return %ret : tensor<124x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<5x5x5xf32>, tensor<124x28x508xf32>) -> (tensor<124x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32>_294": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32>\n  return %ret : tensor<256x128x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x128x512xf32>) -> tensor<256x128x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x128x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x512xf32>\n    memref.copy %2, %alloc : memref<256x128x512xf32> to memref<256x128x512xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x128x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x128x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x128x512xf32>\n    return %3 : tensor<256x128x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x128x512xf32>) -> tensor<256x128x512xf32>\n  return %ret : tensor<256x128x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c128, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x128x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x512xf32>, tensor<1x1x1xf32>, tensor<256x128x512xf32>) -> (tensor<256x128x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>_295": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>\n  return %ret : tensor<32x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x128xf32>\n    memref.copy %2, %alloc : memref<32x128x128xf32> to memref<32x128x128xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128x128xf32>\n    return %3 : tensor<32x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>\n  return %ret : tensor<32x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x128xf32>, tensor<1x1x1xf32>, tensor<32x128x128xf32>) -> (tensor<32x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>_296": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>\n  return %ret : tensor<1020x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x508xf32>\n    memref.copy %2, %alloc : memref<1020x28x508xf32> to memref<1020x28x508xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x508xf32>\n    return %3 : tensor<1020x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x508xf32>) -> tensor<1020x28x508xf32>\n  return %ret : tensor<1020x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x512xf32>, tensor<5x5x5xf32>, tensor<1020x28x508xf32>) -> (tensor<1020x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>_297": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>\n  return %ret : tensor<28x252x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252x508xf32>\n    memref.copy %2, %alloc : memref<28x252x508xf32> to memref<28x252x508xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x252x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x252x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252x508xf32>\n    return %3 : tensor<28x252x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x508xf32>) -> tensor<28x252x508xf32>\n  return %ret : tensor<28x252x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x252x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x512xf32>, tensor<5x5x5xf32>, tensor<28x252x508xf32>) -> (tensor<28x252x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32>_298": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32>\n  return %ret : tensor<512x1024x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x32xf32>\n    memref.copy %2, %alloc : memref<512x1024x32xf32> to memref<512x1024x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x32xf32>\n    return %3 : tensor<512x1024x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x32xf32>) -> tensor<512x1024x32xf32>\n  return %ret : tensor<512x1024x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x32xf32>, tensor<1x1x1xf32>, tensor<512x1024x32xf32>) -> (tensor<512x1024x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>_299": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>\n  return %ret : tensor<250x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x250xf32>\n    memref.copy %2, %alloc : memref<250x122x250xf32> to memref<250x122x250xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x250xf32>\n    return %3 : tensor<250x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x250xf32>) -> tensor<250x122x250xf32>\n  return %ret : tensor<250x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x256xf32>, tensor<7x7x7xf32>, tensor<250x122x250xf32>) -> (tensor<250x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32>_300": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32>\n  return %ret : tensor<512x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x32x128xf32>) -> tensor<512x32x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x32x128xf32>\n    memref.copy %2, %alloc : memref<512x32x128xf32> to memref<512x32x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x32x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x32x128xf32>\n    return %3 : tensor<512x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x128xf32>) -> tensor<512x32x128xf32>\n  return %ret : tensor<512x32x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c32, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x32x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x128xf32>, tensor<1x1x1xf32>, tensor<512x32x128xf32>) -> (tensor<512x32x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>_301": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>\n  return %ret : tensor<510x1022x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x1022x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x1022x126xf32>\n    memref.copy %2, %alloc : memref<510x1022x126xf32> to memref<510x1022x126xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x1022x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x1022x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x1022x126xf32>\n    return %3 : tensor<510x1022x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>\n  return %ret : tensor<510x1022x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c1022, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x1022x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x128xf32>, tensor<3x3x3xf32>, tensor<510x1022x126xf32>) -> (tensor<510x1022x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32>_302": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32>\n  return %ret : tensor<254x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x30x126xf32>) -> tensor<254x30x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x30x126xf32>\n    memref.copy %2, %alloc : memref<254x30x126xf32> to memref<254x30x126xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x30x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x30x126xf32>\n    return %3 : tensor<254x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x126xf32>) -> tensor<254x30x126xf32>\n  return %ret : tensor<254x30x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c30, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x30x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x128xf32>, tensor<3x3x3xf32>, tensor<254x30x126xf32>) -> (tensor<254x30x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32>_303": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32>\n  return %ret : tensor<122x58x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x58x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x58x1018xf32>\n    memref.copy %2, %alloc : memref<122x58x1018xf32> to memref<122x58x1018xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x58x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x58x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x58x1018xf32>\n    return %3 : tensor<122x58x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x1018xf32>) -> tensor<122x58x1018xf32>\n  return %ret : tensor<122x58x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c58, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x58x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x1024xf32>, tensor<7x7x7xf32>, tensor<122x58x1018xf32>) -> (tensor<122x58x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>_304": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>\n  return %ret : tensor<60x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508x252xf32>\n    memref.copy %2, %alloc : memref<60x508x252xf32> to memref<60x508x252xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x508x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508x252xf32>\n    return %3 : tensor<60x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>\n  return %ret : tensor<60x508x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x508x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x256xf32>, tensor<5x5x5xf32>, tensor<60x508x252xf32>) -> (tensor<60x508x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32>_305": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32>\n  return %ret : tensor<512x64x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x64x32xf32>) -> tensor<512x64x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x64x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x64x32xf32>\n    memref.copy %2, %alloc : memref<512x64x32xf32> to memref<512x64x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x64x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x64x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x64x32xf32>\n    return %3 : tensor<512x64x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x32xf32>) -> tensor<512x64x32xf32>\n  return %ret : tensor<512x64x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c64, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x64x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x32xf32>, tensor<1x1x1xf32>, tensor<512x64x32xf32>) -> (tensor<512x64x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>_306": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>\n  return %ret : tensor<1018x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506x58xf32>\n    memref.copy %2, %alloc : memref<1018x506x58xf32> to memref<1018x506x58xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506x58xf32>\n    return %3 : tensor<1018x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x58xf32>) -> tensor<1018x506x58xf32>\n  return %ret : tensor<1018x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x64xf32>, tensor<7x7x7xf32>, tensor<1018x506x58xf32>) -> (tensor<1018x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32>_307": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32>\n  return %ret : tensor<122x58x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x58x26xf32>) -> tensor<122x58x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x58x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x58x26xf32>\n    memref.copy %2, %alloc : memref<122x58x26xf32> to memref<122x58x26xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x58x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x58x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x58x26xf32>\n    return %3 : tensor<122x58x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x58x26xf32>) -> tensor<122x58x26xf32>\n  return %ret : tensor<122x58x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c58, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x58x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x32xf32>, tensor<7x7x7xf32>, tensor<122x58x26xf32>) -> (tensor<122x58x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32>_308": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32>\n  return %ret : tensor<254x1022x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022x1022xf32>\n    memref.copy %2, %alloc : memref<254x1022x1022xf32> to memref<254x1022x1022xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x1022x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x1022x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022x1022xf32>\n    return %3 : tensor<254x1022x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x1022xf32>) -> tensor<254x1022x1022xf32>\n  return %ret : tensor<254x1022x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x1022x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x1024xf32>, tensor<3x3x3xf32>, tensor<254x1022x1022xf32>) -> (tensor<254x1022x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32>_309": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32>\n  return %ret : tensor<62x126x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x30xf32>) -> tensor<62x126x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x30xf32>\n    memref.copy %2, %alloc : memref<62x126x30xf32> to memref<62x126x30xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x30xf32>\n    return %3 : tensor<62x126x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x30xf32>) -> tensor<62x126x30xf32>\n  return %ret : tensor<62x126x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x32xf32>, tensor<3x3x3xf32>, tensor<62x126x30xf32>) -> (tensor<62x126x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>_310": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>\n  return %ret : tensor<62x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x126xf32>\n    memref.copy %2, %alloc : memref<62x126x126xf32> to memref<62x126x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x126xf32>\n    return %3 : tensor<62x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>\n  return %ret : tensor<62x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x128xf32>, tensor<3x3x3xf32>, tensor<62x126x126xf32>) -> (tensor<62x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32>_311": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32>\n  return %ret : tensor<252x28x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28x1020xf32>\n    memref.copy %2, %alloc : memref<252x28x1020xf32> to memref<252x28x1020xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x28x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x28x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28x1020xf32>\n    return %3 : tensor<252x28x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x1020xf32>) -> tensor<252x28x1020xf32>\n  return %ret : tensor<252x28x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x28x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x1024xf32>, tensor<5x5x5xf32>, tensor<252x28x1020xf32>) -> (tensor<252x28x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32>_312": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32>\n  return %ret : tensor<28x28x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28x1020xf32>\n    memref.copy %2, %alloc : memref<28x28x1020xf32> to memref<28x28x1020xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x28x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x28x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28x1020xf32>\n    return %3 : tensor<28x28x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x1020xf32>) -> tensor<28x28x1020xf32>\n  return %ret : tensor<28x28x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x28x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x1024xf32>, tensor<5x5x5xf32>, tensor<28x28x1020xf32>) -> (tensor<28x28x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32>_313": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32>\n  return %ret : tensor<26x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x122x1018xf32>\n    memref.copy %2, %alloc : memref<26x122x1018xf32> to memref<26x122x1018xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x122x1018xf32>\n    return %3 : tensor<26x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x1018xf32>) -> tensor<26x122x1018xf32>\n  return %ret : tensor<26x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x1024xf32>, tensor<7x7x7xf32>, tensor<26x122x1018xf32>) -> (tensor<26x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32>_314": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32>\n  return %ret : tensor<1018x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018x506xf32>\n    memref.copy %2, %alloc : memref<1018x1018x506xf32> to memref<1018x1018x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018x506xf32>\n    return %3 : tensor<1018x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x506xf32>) -> tensor<1018x1018x506xf32>\n  return %ret : tensor<1018x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x512xf32>, tensor<7x7x7xf32>, tensor<1018x1018x506xf32>) -> (tensor<1018x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32>_315": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32>\n  return %ret : tensor<62x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x254x30xf32>) -> tensor<62x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254x30xf32>\n    memref.copy %2, %alloc : memref<62x254x30xf32> to memref<62x254x30xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254x30xf32>\n    return %3 : tensor<62x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x30xf32>) -> tensor<62x254x30xf32>\n  return %ret : tensor<62x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x32xf32>, tensor<3x3x3xf32>, tensor<62x254x30xf32>) -> (tensor<62x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32>_316": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32>\n  return %ret : tensor<126x30x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x1022xf32>\n    memref.copy %2, %alloc : memref<126x30x1022xf32> to memref<126x30x1022xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x1022xf32>\n    return %3 : tensor<126x30x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x1022xf32>) -> tensor<126x30x1022xf32>\n  return %ret : tensor<126x30x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x1024xf32>, tensor<3x3x3xf32>, tensor<126x30x1022xf32>) -> (tensor<126x30x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32>_317": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32>\n  return %ret : tensor<58x506x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x506x506xf32>) -> tensor<58x506x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x506x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x506x506xf32>\n    memref.copy %2, %alloc : memref<58x506x506xf32> to memref<58x506x506xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x506x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x506x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x506x506xf32>\n    return %3 : tensor<58x506x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x506x506xf32>) -> tensor<58x506x506xf32>\n  return %ret : tensor<58x506x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c506, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x506x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x512xf32>, tensor<7x7x7xf32>, tensor<58x506x506xf32>) -> (tensor<58x506x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32>_318": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32>\n  return %ret : tensor<1018x506x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506x506xf32>\n    memref.copy %2, %alloc : memref<1018x506x506xf32> to memref<1018x506x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x506x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x506x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506x506xf32>\n    return %3 : tensor<1018x506x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x506xf32>) -> tensor<1018x506x506xf32>\n  return %ret : tensor<1018x506x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x506x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x512xf32>, tensor<7x7x7xf32>, tensor<1018x506x506xf32>) -> (tensor<1018x506x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32>_319": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32>\n  return %ret : tensor<60x124x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x124x124xf32>) -> tensor<60x124x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124x124xf32>\n    memref.copy %2, %alloc : memref<60x124x124xf32> to memref<60x124x124xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x124x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x124x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124x124xf32>\n    return %3 : tensor<60x124x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x124xf32>) -> tensor<60x124x124xf32>\n  return %ret : tensor<60x124x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x124x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x128xf32>, tensor<5x5x5xf32>, tensor<60x124x124xf32>) -> (tensor<60x124x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32>_320": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32>\n  return %ret : tensor<1024x256x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256x1024xf32>\n    memref.copy %2, %alloc : memref<1024x256x1024xf32> to memref<1024x256x1024xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x256x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x256x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256x1024xf32>\n    return %3 : tensor<1024x256x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x1024xf32>) -> tensor<1024x256x1024xf32>\n  return %ret : tensor<1024x256x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x256x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x1024xf32>, tensor<1x1x1xf32>, tensor<1024x256x1024xf32>) -> (tensor<1024x256x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>_321": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>\n  return %ret : tensor<62x510x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510x62xf32>\n    memref.copy %2, %alloc : memref<62x510x62xf32> to memref<62x510x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x510x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x510x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510x62xf32>\n    return %3 : tensor<62x510x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>\n  return %ret : tensor<62x510x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x510x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x64xf32>, tensor<3x3x3xf32>, tensor<62x510x62xf32>) -> (tensor<62x510x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32>_322": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32>\n  return %ret : tensor<28x252x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x252x124xf32>) -> tensor<28x252x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252x124xf32>\n    memref.copy %2, %alloc : memref<28x252x124xf32> to memref<28x252x124xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x252x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x252x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252x124xf32>\n    return %3 : tensor<28x252x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x124xf32>) -> tensor<28x252x124xf32>\n  return %ret : tensor<28x252x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x252x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x128xf32>, tensor<5x5x5xf32>, tensor<28x252x124xf32>) -> (tensor<28x252x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32>_323": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32>\n  return %ret : tensor<124x252x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x252x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x252x1020xf32>\n    memref.copy %2, %alloc : memref<124x252x1020xf32> to memref<124x252x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x252x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x252x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x252x1020xf32>\n    return %3 : tensor<124x252x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x252x1020xf32>) -> tensor<124x252x1020xf32>\n  return %ret : tensor<124x252x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c252, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x252x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x1024xf32>, tensor<5x5x5xf32>, tensor<124x252x1020xf32>) -> (tensor<124x252x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>_324": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x128xf32>\n    memref.copy %2, %alloc : memref<64x64x128xf32> to memref<64x64x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x128xf32>\n    return %3 : tensor<64x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x128xf32>) -> tensor<64x64x128xf32>\n  return %ret : tensor<64x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x128xf32>, tensor<1x1x1xf32>, tensor<64x64x128xf32>) -> (tensor<64x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>_325": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250x26xf32>\n    memref.copy %2, %alloc : memref<1018x250x26xf32> to memref<1018x250x26xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250x26xf32>\n    return %3 : tensor<1018x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x26xf32>) -> tensor<1018x250x26xf32>\n  return %ret : tensor<1018x250x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x250x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<7x7x7xf32>, tensor<1018x250x26xf32>) -> (tensor<1018x250x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>_326": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x250xf32>\n    memref.copy %2, %alloc : memref<58x122x250xf32> to memref<58x122x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x250xf32>\n    return %3 : tensor<58x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<7x7x7xf32>, tensor<58x122x250xf32>) -> (tensor<58x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32>_327": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32>\n  return %ret : tensor<30x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x254x30xf32>) -> tensor<30x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254x30xf32>\n    memref.copy %2, %alloc : memref<30x254x30xf32> to memref<30x254x30xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254x30xf32>\n    return %3 : tensor<30x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x30xf32>) -> tensor<30x254x30xf32>\n  return %ret : tensor<30x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x32xf32>, tensor<3x3x3xf32>, tensor<30x254x30xf32>) -> (tensor<30x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32>_328": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32>\n  return %ret : tensor<510x510x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x510x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x510x1022xf32>\n    memref.copy %2, %alloc : memref<510x510x1022xf32> to memref<510x510x1022xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x510x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x510x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x510x1022xf32>\n    return %3 : tensor<510x510x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x510x1022xf32>) -> tensor<510x510x1022xf32>\n  return %ret : tensor<510x510x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c510, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x510x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x1024xf32>, tensor<3x3x3xf32>, tensor<510x510x1022xf32>) -> (tensor<510x510x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32>_329": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32>\n  return %ret : tensor<30x30x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30x1022xf32>\n    memref.copy %2, %alloc : memref<30x30x1022xf32> to memref<30x30x1022xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x30x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x30x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30x1022xf32>\n    return %3 : tensor<30x30x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x1022xf32>) -> tensor<30x30x1022xf32>\n  return %ret : tensor<30x30x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x30x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x1024xf32>, tensor<3x3x3xf32>, tensor<30x30x1022xf32>) -> (tensor<30x30x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32>_330": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32>\n  return %ret : tensor<30x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x254x254xf32>) -> tensor<30x254x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254x254xf32>\n    memref.copy %2, %alloc : memref<30x254x254xf32> to memref<30x254x254xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x254x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254x254xf32>\n    return %3 : tensor<30x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x254xf32>) -> tensor<30x254x254xf32>\n  return %ret : tensor<30x254x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x254x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x256xf32>, tensor<3x3x3xf32>, tensor<30x254x254xf32>) -> (tensor<30x254x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32>_331": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32>\n  return %ret : tensor<1018x506x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x506x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x506x250xf32>\n    memref.copy %2, %alloc : memref<1018x506x250xf32> to memref<1018x506x250xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x506x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x506x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x506x250xf32>\n    return %3 : tensor<1018x506x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x506x250xf32>) -> tensor<1018x506x250xf32>\n  return %ret : tensor<1018x506x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c506, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x506x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x256xf32>, tensor<7x7x7xf32>, tensor<1018x506x250xf32>) -> (tensor<1018x506x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>_332": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>\n  return %ret : tensor<250x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x1018xf32>\n    memref.copy %2, %alloc : memref<250x122x1018xf32> to memref<250x122x1018xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x1018xf32>\n    return %3 : tensor<250x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>\n  return %ret : tensor<250x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x1024xf32>, tensor<7x7x7xf32>, tensor<250x122x1018xf32>) -> (tensor<250x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>_333": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>\n  return %ret : tensor<512x64x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x64x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x64x128xf32>\n    memref.copy %2, %alloc : memref<512x64x128xf32> to memref<512x64x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x64x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x64x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x64x128xf32>\n    return %3 : tensor<512x64x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x64x128xf32>) -> tensor<512x64x128xf32>\n  return %ret : tensor<512x64x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x128xf32>, tensor<1x1x1xf32>, tensor<512x64x128xf32>) -> (tensor<512x64x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>_334": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x256x32xf32>\n    memref.copy %2, %alloc : memref<512x256x32xf32> to memref<512x256x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x256x32xf32>\n    return %3 : tensor<512x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x256x32xf32>) -> tensor<512x256x32xf32>\n  return %ret : tensor<512x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x32xf32>, tensor<1x1x1xf32>, tensor<512x256x32xf32>) -> (tensor<512x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32>_335": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32>\n  return %ret : tensor<58x1018x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018x1018xf32>\n    memref.copy %2, %alloc : memref<58x1018x1018xf32> to memref<58x1018x1018xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x1018x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x1018x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018x1018xf32>\n    return %3 : tensor<58x1018x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x1018xf32>) -> tensor<58x1018x1018xf32>\n  return %ret : tensor<58x1018x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x1018x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x1024xf32>, tensor<7x7x7xf32>, tensor<58x1018x1018xf32>) -> (tensor<58x1018x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>_336": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>\n  return %ret : tensor<250x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x122x1018xf32>\n    memref.copy %2, %alloc : memref<250x122x1018xf32> to memref<250x122x1018xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x122x1018xf32>\n    return %3 : tensor<250x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x122x1018xf32>) -> tensor<250x122x1018xf32>\n  return %ret : tensor<250x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x1024xf32>, tensor<7x7x7xf32>, tensor<250x122x1018xf32>) -> (tensor<250x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>_337": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>\n  return %ret : tensor<1020x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508x60xf32>\n    memref.copy %2, %alloc : memref<1020x508x60xf32> to memref<1020x508x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508x60xf32>\n    return %3 : tensor<1020x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x60xf32>) -> tensor<1020x508x60xf32>\n  return %ret : tensor<1020x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x64xf32>, tensor<5x5x5xf32>, tensor<1020x508x60xf32>) -> (tensor<1020x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>_338": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>\n  return %ret : tensor<1020x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508x124xf32>\n    memref.copy %2, %alloc : memref<1020x508x124xf32> to memref<1020x508x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x508x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508x124xf32>\n    return %3 : tensor<1020x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>\n  return %ret : tensor<1020x508x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x508x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x128xf32>, tensor<5x5x5xf32>, tensor<1020x508x124xf32>) -> (tensor<1020x508x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>_339": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>\n  return %ret : tensor<60x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x508x252xf32>\n    memref.copy %2, %alloc : memref<60x508x252xf32> to memref<60x508x252xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x508x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x508x252xf32>\n    return %3 : tensor<60x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x508x252xf32>) -> tensor<60x508x252xf32>\n  return %ret : tensor<60x508x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c508, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x508x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x256xf32>, tensor<5x5x5xf32>, tensor<60x508x252xf32>) -> (tensor<60x508x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>_340": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>\n  return %ret : tensor<58x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250x122xf32>\n    memref.copy %2, %alloc : memref<58x250x122xf32> to memref<58x250x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x250x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250x122xf32>\n    return %3 : tensor<58x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>\n  return %ret : tensor<58x250x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x250x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x128xf32>, tensor<7x7x7xf32>, tensor<58x250x122xf32>) -> (tensor<58x250x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32>_341": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32>\n  return %ret : tensor<30x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x62x254xf32>) -> tensor<30x62x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62x254xf32>\n    memref.copy %2, %alloc : memref<30x62x254xf32> to memref<30x62x254xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x62x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62x254xf32>\n    return %3 : tensor<30x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x254xf32>) -> tensor<30x62x254xf32>\n  return %ret : tensor<30x62x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x62x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x256xf32>, tensor<3x3x3xf32>, tensor<30x62x254xf32>) -> (tensor<30x62x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32>_342": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32>\n  return %ret : tensor<250x250x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250x1018xf32>\n    memref.copy %2, %alloc : memref<250x250x1018xf32> to memref<250x250x1018xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x250x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x250x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250x1018xf32>\n    return %3 : tensor<250x250x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x1018xf32>) -> tensor<250x250x1018xf32>\n  return %ret : tensor<250x250x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x250x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x1024xf32>, tensor<7x7x7xf32>, tensor<250x250x1018xf32>) -> (tensor<250x250x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>_343": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>\n  return %ret : tensor<62x510x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510x62xf32>\n    memref.copy %2, %alloc : memref<62x510x62xf32> to memref<62x510x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x510x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x510x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510x62xf32>\n    return %3 : tensor<62x510x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x62xf32>) -> tensor<62x510x62xf32>\n  return %ret : tensor<62x510x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x510x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x64xf32>, tensor<3x3x3xf32>, tensor<62x510x62xf32>) -> (tensor<62x510x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>_344": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>\n  return %ret : tensor<256x64x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x1024xf32>\n    memref.copy %2, %alloc : memref<256x64x1024xf32> to memref<256x64x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x64x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x64x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x1024xf32>\n    return %3 : tensor<256x64x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x1024xf32>) -> tensor<256x64x1024xf32>\n  return %ret : tensor<256x64x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c64, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x64x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x1024xf32>, tensor<1x1x1xf32>, tensor<256x64x1024xf32>) -> (tensor<256x64x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>_345": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>\n  return %ret : tensor<126x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x30xf32>\n    memref.copy %2, %alloc : memref<126x30x30xf32> to memref<126x30x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x30xf32>\n    return %3 : tensor<126x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x30xf32>) -> tensor<126x30x30xf32>\n  return %ret : tensor<126x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x32xf32>, tensor<3x3x3xf32>, tensor<126x30x30xf32>) -> (tensor<126x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>_346": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x1024x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x512xf32>\n    memref.copy %2, %alloc : memref<256x1024x512xf32> to memref<256x1024x512xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x1024x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x1024x512xf32>\n    return %3 : tensor<256x1024x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x1024x512xf32>) -> tensor<256x1024x512xf32>\n  return %ret : tensor<256x1024x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c1024, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x1024x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x512xf32>, tensor<1x1x1xf32>, tensor<256x1024x512xf32>) -> (tensor<256x1024x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>_347": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>\n  return %ret : tensor<1020x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60x124xf32>\n    memref.copy %2, %alloc : memref<1020x60x124xf32> to memref<1020x60x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60x124xf32>\n    return %3 : tensor<1020x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x124xf32>) -> tensor<1020x60x124xf32>\n  return %ret : tensor<1020x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x128xf32>, tensor<5x5x5xf32>, tensor<1020x60x124xf32>) -> (tensor<1020x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>_348": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>\n  return %ret : tensor<256x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x64xf32>\n    memref.copy %2, %alloc : memref<256x32x64xf32> to memref<256x32x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x64xf32>\n    return %3 : tensor<256x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>\n  return %ret : tensor<256x32x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x64xf32>, tensor<1x1x1xf32>, tensor<256x32x64xf32>) -> (tensor<256x32x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32>_349": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32>\n  return %ret : tensor<124x28x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x28x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x28x1020xf32>\n    memref.copy %2, %alloc : memref<124x28x1020xf32> to memref<124x28x1020xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x28x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x28x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x28x1020xf32>\n    return %3 : tensor<124x28x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x28x1020xf32>) -> tensor<124x28x1020xf32>\n  return %ret : tensor<124x28x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c28, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x28x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x1024xf32>, tensor<5x5x5xf32>, tensor<124x28x1020xf32>) -> (tensor<124x28x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32>_350": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32>\n  return %ret : tensor<250x506x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x506x58xf32>) -> tensor<250x506x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x506x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x506x58xf32>\n    memref.copy %2, %alloc : memref<250x506x58xf32> to memref<250x506x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x506x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x506x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x506x58xf32>\n    return %3 : tensor<250x506x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x506x58xf32>) -> tensor<250x506x58xf32>\n  return %ret : tensor<250x506x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c506, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x506x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x64xf32>, tensor<7x7x7xf32>, tensor<250x506x58xf32>) -> (tensor<250x506x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>_351": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>\n  return %ret : tensor<64x512x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x1024xf32>\n    memref.copy %2, %alloc : memref<64x512x1024xf32> to memref<64x512x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x512x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x512x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512x1024xf32>\n    return %3 : tensor<64x512x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>\n  return %ret : tensor<64x512x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x512x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x1024xf32>, tensor<1x1x1xf32>, tensor<64x512x1024xf32>) -> (tensor<64x512x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32>_352": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32>\n  return %ret : tensor<1018x1018x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x1018x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x1018x250xf32>\n    memref.copy %2, %alloc : memref<1018x1018x250xf32> to memref<1018x1018x250xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x1018x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x1018x250xf32>\n    return %3 : tensor<1018x1018x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x1018x250xf32>) -> tensor<1018x1018x250xf32>\n  return %ret : tensor<1018x1018x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c1018, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x1018x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x256xf32>, tensor<7x7x7xf32>, tensor<1018x1018x250xf32>) -> (tensor<1018x1018x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32>_353": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32>\n  return %ret : tensor<1024x256x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256x128xf32>\n    memref.copy %2, %alloc : memref<1024x256x128xf32> to memref<1024x256x128xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x256x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x256x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256x128xf32>\n    return %3 : tensor<1024x256x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x128xf32>) -> tensor<1024x256x128xf32>\n  return %ret : tensor<1024x256x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x256x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x128xf32>, tensor<1x1x1xf32>, tensor<1024x256x128xf32>) -> (tensor<1024x256x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>_354": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>\n  return %ret : tensor<124x508x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508x508xf32>\n    memref.copy %2, %alloc : memref<124x508x508xf32> to memref<124x508x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x508x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x508x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508x508xf32>\n    return %3 : tensor<124x508x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>\n  return %ret : tensor<124x508x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x508x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x512xf32>, tensor<5x5x5xf32>, tensor<124x508x508xf32>) -> (tensor<124x508x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32>_355": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32>\n  return %ret : tensor<26x58x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58x1018xf32>\n    memref.copy %2, %alloc : memref<26x58x1018xf32> to memref<26x58x1018xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x58x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x58x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58x1018xf32>\n    return %3 : tensor<26x58x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x1018xf32>) -> tensor<26x58x1018xf32>\n  return %ret : tensor<26x58x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x58x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x1024xf32>, tensor<7x7x7xf32>, tensor<26x58x1018xf32>) -> (tensor<26x58x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32>_356": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32>\n  return %ret : tensor<1024x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x256x256xf32>\n    memref.copy %2, %alloc : memref<1024x256x256xf32> to memref<1024x256x256xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x256x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x256x256xf32>\n    return %3 : tensor<1024x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x256x256xf32>) -> tensor<1024x256x256xf32>\n  return %ret : tensor<1024x256x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c256, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x256x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x256xf32>, tensor<1x1x1xf32>, tensor<1024x256x256xf32>) -> (tensor<1024x256x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32>_357": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32>\n  return %ret : tensor<30x510x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x510x62xf32>) -> tensor<30x510x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x510x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x510x62xf32>\n    memref.copy %2, %alloc : memref<30x510x62xf32> to memref<30x510x62xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x510x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x510x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x510x62xf32>\n    return %3 : tensor<30x510x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x510x62xf32>) -> tensor<30x510x62xf32>\n  return %ret : tensor<30x510x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c510, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x510x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x64xf32>, tensor<3x3x3xf32>, tensor<30x510x62xf32>) -> (tensor<30x510x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>_358": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x60xf32>\n    memref.copy %2, %alloc : memref<508x124x60xf32> to memref<508x124x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x60xf32>\n    return %3 : tensor<508x124x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x60xf32>) -> tensor<508x124x60xf32>\n  return %ret : tensor<508x124x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x64xf32>, tensor<5x5x5xf32>, tensor<508x124x60xf32>) -> (tensor<508x124x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32>_359": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32>\n  return %ret : tensor<1020x28x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x124xf32>\n    memref.copy %2, %alloc : memref<1020x28x124xf32> to memref<1020x28x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x124xf32>\n    return %3 : tensor<1020x28x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x124xf32>) -> tensor<1020x28x124xf32>\n  return %ret : tensor<1020x28x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x128xf32>, tensor<5x5x5xf32>, tensor<1020x28x124xf32>) -> (tensor<1020x28x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32>_360": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32>\n  return %ret : tensor<1022x1022x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x1022x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x1022x1022xf32>\n    memref.copy %2, %alloc : memref<1022x1022x1022xf32> to memref<1022x1022x1022xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x1022x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x1022x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x1022x1022xf32>\n    return %3 : tensor<1022x1022x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x1022x1022xf32>) -> tensor<1022x1022x1022xf32>\n  return %ret : tensor<1022x1022x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c1022, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x1022x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x1024xf32>, tensor<3x3x3xf32>, tensor<1022x1022x1022xf32>) -> (tensor<1022x1022x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32>_361": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32>\n  return %ret : tensor<510x126x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x126x510xf32>) -> tensor<510x126x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x126x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x126x510xf32>\n    memref.copy %2, %alloc : memref<510x126x510xf32> to memref<510x126x510xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x126x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x126x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x126x510xf32>\n    return %3 : tensor<510x126x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x126x510xf32>) -> tensor<510x126x510xf32>\n  return %ret : tensor<510x126x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c126, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x126x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x512xf32>, tensor<3x3x3xf32>, tensor<510x126x510xf32>) -> (tensor<510x126x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32>_362": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32>\n  return %ret : tensor<60x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x28x60xf32>) -> tensor<60x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x28x60xf32>\n    memref.copy %2, %alloc : memref<60x28x60xf32> to memref<60x28x60xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x28x60xf32>\n    return %3 : tensor<60x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x28x60xf32>) -> tensor<60x28x60xf32>\n  return %ret : tensor<60x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x64xf32>, tensor<5x5x5xf32>, tensor<60x28x60xf32>) -> (tensor<60x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>_363": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x250xf32>\n    memref.copy %2, %alloc : memref<26x250x250xf32> to memref<26x250x250xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x250xf32>\n    return %3 : tensor<26x250x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x256xf32>, tensor<7x7x7xf32>, tensor<26x250x250xf32>) -> (tensor<26x250x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32>_364": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32>\n  return %ret : tensor<62x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x62x30xf32>) -> tensor<62x62x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x62x30xf32>\n    memref.copy %2, %alloc : memref<62x62x30xf32> to memref<62x62x30xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x62x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x62x30xf32>\n    return %3 : tensor<62x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x62x30xf32>) -> tensor<62x62x30xf32>\n  return %ret : tensor<62x62x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c62, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x62x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x32xf32>, tensor<3x3x3xf32>, tensor<62x62x30xf32>) -> (tensor<62x62x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32>_365": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32>\n  return %ret : tensor<1024x1024x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x1024x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x1024x64xf32>\n    memref.copy %2, %alloc : memref<1024x1024x64xf32> to memref<1024x1024x64xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x1024x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x1024x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x1024x64xf32>\n    return %3 : tensor<1024x1024x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x1024x64xf32>) -> tensor<1024x1024x64xf32>\n  return %ret : tensor<1024x1024x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c1024, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x1024x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x64xf32>, tensor<1x1x1xf32>, tensor<1024x1024x64xf32>) -> (tensor<1024x1024x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32>_366": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32>\n  return %ret : tensor<122x26x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x26x506xf32>) -> tensor<122x26x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26x506xf32>\n    memref.copy %2, %alloc : memref<122x26x506xf32> to memref<122x26x506xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x26x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x26x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26x506xf32>\n    return %3 : tensor<122x26x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x506xf32>) -> tensor<122x26x506xf32>\n  return %ret : tensor<122x26x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x26x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<7x7x7xf32>, tensor<122x26x506xf32>) -> (tensor<122x26x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32>_367": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32>\n  return %ret : tensor<508x252x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x508xf32>) -> tensor<508x252x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x508xf32>\n    memref.copy %2, %alloc : memref<508x252x508xf32> to memref<508x252x508xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x508xf32>\n    return %3 : tensor<508x252x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x508xf32>) -> tensor<508x252x508xf32>\n  return %ret : tensor<508x252x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x512xf32>, tensor<5x5x5xf32>, tensor<508x252x508xf32>) -> (tensor<508x252x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32>_368": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32>\n  return %ret : tensor<126x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x64x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x62x30xf32>) -> tensor<126x62x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x62x30xf32>\n    memref.copy %2, %alloc : memref<126x62x30xf32> to memref<126x62x30xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x62x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x62x30xf32>\n    return %3 : tensor<126x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x62x30xf32>) -> tensor<126x62x30xf32>\n  return %ret : tensor<126x62x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c62, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x62x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x64x32xf32>, tensor<3x3x3xf32>, tensor<126x62x30xf32>) -> (tensor<126x62x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32>_369": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32>\n  return %ret : tensor<62x126x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x510xf32>) -> tensor<62x126x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x510xf32>\n    memref.copy %2, %alloc : memref<62x126x510xf32> to memref<62x126x510xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x510xf32>\n    return %3 : tensor<62x126x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x510xf32>) -> tensor<62x126x510xf32>\n  return %ret : tensor<62x126x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x512xf32>, tensor<3x3x3xf32>, tensor<62x126x510xf32>) -> (tensor<62x126x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32>_370": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32>\n  return %ret : tensor<62x1022x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x1022x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x1022x126xf32>\n    memref.copy %2, %alloc : memref<62x1022x126xf32> to memref<62x1022x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x1022x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x1022x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x1022x126xf32>\n    return %3 : tensor<62x1022x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x1022x126xf32>) -> tensor<62x1022x126xf32>\n  return %ret : tensor<62x1022x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c1022, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x1022x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x128xf32>, tensor<3x3x3xf32>, tensor<62x1022x126xf32>) -> (tensor<62x1022x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32>_371": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32>\n  return %ret : tensor<256x512x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x512x128xf32>) -> tensor<256x512x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x128xf32>\n    memref.copy %2, %alloc : memref<256x512x128xf32> to memref<256x512x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x512x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x512x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x128xf32>\n    return %3 : tensor<256x512x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x128xf32>) -> tensor<256x512x128xf32>\n  return %ret : tensor<256x512x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c512, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x512x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x128xf32>, tensor<1x1x1xf32>, tensor<256x512x128xf32>) -> (tensor<256x512x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32>_372": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32>\n  return %ret : tensor<32x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x32x256xf32>) -> tensor<32x32x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x256xf32>\n    memref.copy %2, %alloc : memref<32x32x256xf32> to memref<32x32x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x32x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32x256xf32>\n    return %3 : tensor<32x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x256xf32>) -> tensor<32x32x256xf32>\n  return %ret : tensor<32x32x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x32x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x256xf32>, tensor<1x1x1xf32>, tensor<32x32x256xf32>) -> (tensor<32x32x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32>_373": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32>\n  return %ret : tensor<506x58x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x58x506xf32>) -> tensor<506x58x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x58x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x58x506xf32>\n    memref.copy %2, %alloc : memref<506x58x506xf32> to memref<506x58x506xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x58x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x58x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x58x506xf32>\n    return %3 : tensor<506x58x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x506xf32>) -> tensor<506x58x506xf32>\n  return %ret : tensor<506x58x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c58, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x58x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x512xf32>, tensor<7x7x7xf32>, tensor<506x58x506xf32>) -> (tensor<506x58x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32>_374": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32>\n  return %ret : tensor<1022x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x254xf32>\n    memref.copy %2, %alloc : memref<1022x254x254xf32> to memref<1022x254x254xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x254xf32>\n    return %3 : tensor<1022x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x254xf32>) -> tensor<1022x254x254xf32>\n  return %ret : tensor<1022x254x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x256xf32>, tensor<3x3x3xf32>, tensor<1022x254x254xf32>) -> (tensor<1022x254x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>_375": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>\n  return %ret : tensor<128x1024x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024x32xf32>\n    memref.copy %2, %alloc : memref<128x1024x32xf32> to memref<128x1024x32xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x1024x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x1024x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024x32xf32>\n    return %3 : tensor<128x1024x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x32xf32>) -> tensor<128x1024x32xf32>\n  return %ret : tensor<128x1024x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<1x1x1xf32>, tensor<128x1024x32xf32>) -> (tensor<128x1024x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32>_376": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32>\n  return %ret : tensor<256x64x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x64x64xf32>) -> tensor<256x64x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x64x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x64xf32>\n    memref.copy %2, %alloc : memref<256x64x64xf32> to memref<256x64x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x64x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x64x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x64x64xf32>\n    return %3 : tensor<256x64x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x64x64xf32>) -> tensor<256x64x64xf32>\n  return %ret : tensor<256x64x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c64, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x64x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x64xf32>, tensor<1x1x1xf32>, tensor<256x64x64xf32>) -> (tensor<256x64x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32>_377": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32>\n  return %ret : tensor<28x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x508x124xf32>) -> tensor<28x508x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508x124xf32>\n    memref.copy %2, %alloc : memref<28x508x124xf32> to memref<28x508x124xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x508x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508x124xf32>\n    return %3 : tensor<28x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x124xf32>) -> tensor<28x508x124xf32>\n  return %ret : tensor<28x508x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x508x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x128xf32>, tensor<5x5x5xf32>, tensor<28x508x124xf32>) -> (tensor<28x508x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32>_378": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32>\n  return %ret : tensor<256x256x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x128xf32>) -> tensor<256x256x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x128xf32>\n    memref.copy %2, %alloc : memref<256x256x128xf32> to memref<256x256x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x128xf32>\n    return %3 : tensor<256x256x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x128xf32>) -> tensor<256x256x128xf32>\n  return %ret : tensor<256x256x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x128xf32>, tensor<1x1x1xf32>, tensor<256x256x128xf32>) -> (tensor<256x256x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32>_379": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32>\n  return %ret : tensor<508x124x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x1020xf32>\n    memref.copy %2, %alloc : memref<508x124x1020xf32> to memref<508x124x1020xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x1020xf32>\n    return %3 : tensor<508x124x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x1020xf32>) -> tensor<508x124x1020xf32>\n  return %ret : tensor<508x124x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x1024xf32>, tensor<5x5x5xf32>, tensor<508x124x1020xf32>) -> (tensor<508x124x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>_380": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x30xf32>\n    memref.copy %2, %alloc : memref<1022x254x30xf32> to memref<1022x254x30xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x30xf32>\n    return %3 : tensor<1022x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x30xf32>) -> tensor<1022x254x30xf32>\n  return %ret : tensor<1022x254x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x32xf32>, tensor<3x3x3xf32>, tensor<1022x254x30xf32>) -> (tensor<1022x254x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>_381": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>\n  return %ret : tensor<508x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508x60xf32>\n    memref.copy %2, %alloc : memref<508x508x60xf32> to memref<508x508x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508x60xf32>\n    return %3 : tensor<508x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>\n  return %ret : tensor<508x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x64xf32>, tensor<5x5x5xf32>, tensor<508x508x60xf32>) -> (tensor<508x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>_382": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>\n  return %ret : tensor<510x1022x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x1022x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x1022x126xf32>\n    memref.copy %2, %alloc : memref<510x1022x126xf32> to memref<510x1022x126xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x1022x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x1022x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x1022x126xf32>\n    return %3 : tensor<510x1022x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x126xf32>) -> tensor<510x1022x126xf32>\n  return %ret : tensor<510x1022x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c1022, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x1022x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x128xf32>, tensor<3x3x3xf32>, tensor<510x1022x126xf32>) -> (tensor<510x1022x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>_383": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>\n  return %ret : tensor<32x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x128xf32>\n    memref.copy %2, %alloc : memref<32x128x128xf32> to memref<32x128x128xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128x128xf32>\n    return %3 : tensor<32x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x128xf32>) -> tensor<32x128x128xf32>\n  return %ret : tensor<32x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x128xf32>, tensor<1x1x1xf32>, tensor<32x128x128xf32>) -> (tensor<32x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32>_384": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32>\n  return %ret : tensor<1020x1020x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020x508xf32>\n    memref.copy %2, %alloc : memref<1020x1020x508xf32> to memref<1020x1020x508xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020x508xf32>\n    return %3 : tensor<1020x1020x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x508xf32>) -> tensor<1020x1020x508xf32>\n  return %ret : tensor<1020x1020x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x1020x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x512xf32>, tensor<5x5x5xf32>, tensor<1020x1020x508xf32>) -> (tensor<1020x1020x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32>_385": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32>\n  return %ret : tensor<506x26x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x26x250xf32>) -> tensor<506x26x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x26x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x26x250xf32>\n    memref.copy %2, %alloc : memref<506x26x250xf32> to memref<506x26x250xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x26x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x26x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x26x250xf32>\n    return %3 : tensor<506x26x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x26x250xf32>) -> tensor<506x26x250xf32>\n  return %ret : tensor<506x26x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c26, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x26x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x256xf32>, tensor<7x7x7xf32>, tensor<506x26x250xf32>) -> (tensor<506x26x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32>_386": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32>\n  return %ret : tensor<510x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x254x62xf32>) -> tensor<510x254x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254x62xf32>\n    memref.copy %2, %alloc : memref<510x254x62xf32> to memref<510x254x62xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x254x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254x62xf32>\n    return %3 : tensor<510x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x62xf32>) -> tensor<510x254x62xf32>\n  return %ret : tensor<510x254x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x254x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x64xf32>, tensor<3x3x3xf32>, tensor<510x254x62xf32>) -> (tensor<510x254x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32>_387": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32>\n  return %ret : tensor<254x30x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x30x254xf32>) -> tensor<254x30x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x30x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x30x254xf32>\n    memref.copy %2, %alloc : memref<254x30x254xf32> to memref<254x30x254xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x30x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x30x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x30x254xf32>\n    return %3 : tensor<254x30x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x30x254xf32>) -> tensor<254x30x254xf32>\n  return %ret : tensor<254x30x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c30, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x30x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x256xf32>, tensor<3x3x3xf32>, tensor<254x30x254xf32>) -> (tensor<254x30x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>_388": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>\n  return %ret : tensor<508x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508x60xf32>\n    memref.copy %2, %alloc : memref<508x508x60xf32> to memref<508x508x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508x60xf32>\n    return %3 : tensor<508x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x60xf32>) -> tensor<508x508x60xf32>\n  return %ret : tensor<508x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x64xf32>, tensor<5x5x5xf32>, tensor<508x508x60xf32>) -> (tensor<508x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>_389": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>\n  return %ret : tensor<64x512x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x512x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x1024xf32>\n    memref.copy %2, %alloc : memref<64x512x1024xf32> to memref<64x512x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x512x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x512x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x512x1024xf32>\n    return %3 : tensor<64x512x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x512x1024xf32>) -> tensor<64x512x1024xf32>\n  return %ret : tensor<64x512x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c512, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x512x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x1024xf32>, tensor<1x1x1xf32>, tensor<64x512x1024xf32>) -> (tensor<64x512x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32>_390": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32>\n  return %ret : tensor<256x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x256xf32>) -> tensor<256x32x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x256xf32>\n    memref.copy %2, %alloc : memref<256x32x256xf32> to memref<256x32x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x256xf32>\n    return %3 : tensor<256x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x256xf32>) -> tensor<256x32x256xf32>\n  return %ret : tensor<256x32x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x256xf32>, tensor<1x1x1xf32>, tensor<256x32x256xf32>) -> (tensor<256x32x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32>_391": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32>\n  return %ret : tensor<252x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x124x28xf32>) -> tensor<252x124x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x124x28xf32>\n    memref.copy %2, %alloc : memref<252x124x28xf32> to memref<252x124x28xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x124x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x124x28xf32>\n    return %3 : tensor<252x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x28xf32>) -> tensor<252x124x28xf32>\n  return %ret : tensor<252x124x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c124, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x124x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x32xf32>, tensor<5x5x5xf32>, tensor<252x124x28xf32>) -> (tensor<252x124x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>_392": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>\n  return %ret : tensor<510x62x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x510xf32>\n    memref.copy %2, %alloc : memref<510x62x510xf32> to memref<510x62x510xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x510xf32>\n    return %3 : tensor<510x62x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>\n  return %ret : tensor<510x62x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x512xf32>, tensor<3x3x3xf32>, tensor<510x62x510xf32>) -> (tensor<510x62x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>_393": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>\n  return %ret : tensor<126x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x126x62xf32>\n    memref.copy %2, %alloc : memref<126x126x62xf32> to memref<126x126x62xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x126x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x126x62xf32>\n    return %3 : tensor<126x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x126x62xf32>) -> tensor<126x126x62xf32>\n  return %ret : tensor<126x126x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c126, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x126x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x64xf32>, tensor<3x3x3xf32>, tensor<126x126x62xf32>) -> (tensor<126x126x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>_394": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>\n  return %ret : tensor<28x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x512x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x508x60xf32>\n    memref.copy %2, %alloc : memref<28x508x60xf32> to memref<28x508x60xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x512x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x508x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x508x60xf32>\n    return %3 : tensor<28x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x512x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x512x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x508x60xf32>) -> tensor<28x508x60xf32>\n  return %ret : tensor<28x508x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c512, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x512x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c508, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x508x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x512x64xf32>, tensor<5x5x5xf32>, tensor<28x508x60xf32>) -> (tensor<28x508x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>_395": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>\n  return %ret : tensor<254x1022x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x1022x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x1022x30xf32>\n    memref.copy %2, %alloc : memref<254x1022x30xf32> to memref<254x1022x30xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x1022x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x1022x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x1022x30xf32>\n    return %3 : tensor<254x1022x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x1022x30xf32>) -> tensor<254x1022x30xf32>\n  return %ret : tensor<254x1022x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c1022, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x1022x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x32xf32>, tensor<3x3x3xf32>, tensor<254x1022x30xf32>) -> (tensor<254x1022x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>_396": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>\n  return %ret : tensor<256x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x128xf32>\n    memref.copy %2, %alloc : memref<256x32x128xf32> to memref<256x32x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x128xf32>\n    return %3 : tensor<256x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>\n  return %ret : tensor<256x32x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x128xf32>, tensor<1x1x1xf32>, tensor<256x32x128xf32>) -> (tensor<256x32x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>_397": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>\n  return %ret : tensor<64x64x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x512xf32>\n    memref.copy %2, %alloc : memref<64x64x512xf32> to memref<64x64x512xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x512xf32>\n    return %3 : tensor<64x64x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x512xf32>) -> tensor<64x64x512xf32>\n  return %ret : tensor<64x64x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x512xf32>, tensor<1x1x1xf32>, tensor<64x64x512xf32>) -> (tensor<64x64x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32>_398": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32>\n  return %ret : tensor<28x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x28x28xf32>) -> tensor<28x28x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x28x28xf32>\n    memref.copy %2, %alloc : memref<28x28x28xf32> to memref<28x28x28xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x28x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x28x28xf32>\n    return %3 : tensor<28x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x28x28xf32>) -> tensor<28x28x28xf32>\n  return %ret : tensor<28x28x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c28, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x28x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x32xf32>, tensor<5x5x5xf32>, tensor<28x28x28xf32>) -> (tensor<28x28x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32>_399": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32>\n  return %ret : tensor<128x1024x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x1024x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x1024x1024xf32>\n    memref.copy %2, %alloc : memref<128x1024x1024xf32> to memref<128x1024x1024xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x1024x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x1024x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x1024x1024xf32>\n    return %3 : tensor<128x1024x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x1024x1024xf32>) -> tensor<128x1024x1024xf32>\n  return %ret : tensor<128x1024x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c1024, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x1024x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x1024xf32>, tensor<1x1x1xf32>, tensor<128x1024x1024xf32>) -> (tensor<128x1024x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>_400": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>\n  return %ret : tensor<508x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x60x124xf32>\n    memref.copy %2, %alloc : memref<508x60x124xf32> to memref<508x60x124xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x60x124xf32>\n    return %3 : tensor<508x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x60x124xf32>) -> tensor<508x60x124xf32>\n  return %ret : tensor<508x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x128xf32>, tensor<5x5x5xf32>, tensor<508x60x124xf32>) -> (tensor<508x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32>_401": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32>\n  return %ret : tensor<28x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x60x124xf32>) -> tensor<28x60x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x60x124xf32>\n    memref.copy %2, %alloc : memref<28x60x124xf32> to memref<28x60x124xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x60x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x60x124xf32>\n    return %3 : tensor<28x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x60x124xf32>) -> tensor<28x60x124xf32>\n  return %ret : tensor<28x60x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c60, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x60x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x128xf32>, tensor<5x5x5xf32>, tensor<28x60x124xf32>) -> (tensor<28x60x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>_402": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>\n  return %ret : tensor<506x1018x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018x122xf32>\n    memref.copy %2, %alloc : memref<506x1018x122xf32> to memref<506x1018x122xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x1018x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x1018x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018x122xf32>\n    return %3 : tensor<506x1018x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x122xf32>) -> tensor<506x1018x122xf32>\n  return %ret : tensor<506x1018x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x1018x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x128xf32>, tensor<7x7x7xf32>, tensor<506x1018x122xf32>) -> (tensor<506x1018x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>_403": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122x250xf32>\n    memref.copy %2, %alloc : memref<1018x122x250xf32> to memref<1018x122x250xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122x250xf32>\n    return %3 : tensor<1018x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x256xf32>, tensor<7x7x7xf32>, tensor<1018x122x250xf32>) -> (tensor<1018x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>_404": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>\n  return %ret : tensor<1020x60x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x60x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x60x252xf32>\n    memref.copy %2, %alloc : memref<1020x60x252xf32> to memref<1020x60x252xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x60x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x60x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x60x252xf32>\n    return %3 : tensor<1020x60x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x60x252xf32>) -> tensor<1020x60x252xf32>\n  return %ret : tensor<1020x60x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c60, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x60x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x256xf32>, tensor<5x5x5xf32>, tensor<1020x60x252xf32>) -> (tensor<1020x60x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>_405": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>\n  return %ret : tensor<506x506x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x506x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x506x26xf32>\n    memref.copy %2, %alloc : memref<506x506x26xf32> to memref<506x506x26xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 506 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x506x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x506x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x506x26xf32>\n    return %3 : tensor<506x506x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x506x26xf32>) -> tensor<506x506x26xf32>\n  return %ret : tensor<506x506x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c506, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x506x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x32xf32>, tensor<7x7x7xf32>, tensor<506x506x26xf32>) -> (tensor<506x506x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 506, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32>_406": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32>\n  return %ret : tensor<1018x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250x122xf32>\n    memref.copy %2, %alloc : memref<1018x250x122xf32> to memref<1018x250x122xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x250x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250x122xf32>\n    return %3 : tensor<1018x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x122xf32>) -> tensor<1018x250x122xf32>\n  return %ret : tensor<1018x250x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x250x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x128xf32>, tensor<7x7x7xf32>, tensor<1018x250x122xf32>) -> (tensor<1018x250x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>_407": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>\n  return %ret : tensor<254x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x126x254xf32>\n    memref.copy %2, %alloc : memref<254x126x254xf32> to memref<254x126x254xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x126x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x126x254xf32>\n    return %3 : tensor<254x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x126x254xf32>) -> tensor<254x126x254xf32>\n  return %ret : tensor<254x126x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c126, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x126x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x256xf32>, tensor<3x3x3xf32>, tensor<254x126x254xf32>) -> (tensor<254x126x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32>_408": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32>\n  return %ret : tensor<512x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x32x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x32x64xf32>) -> tensor<512x32x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x32x64xf32>\n    memref.copy %2, %alloc : memref<512x32x64xf32> to memref<512x32x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x32x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x32x64xf32>\n    return %3 : tensor<512x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x32x64xf32>) -> tensor<512x32x64xf32>\n  return %ret : tensor<512x32x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c32, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x32x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x32x64xf32>, tensor<1x1x1xf32>, tensor<512x32x64xf32>) -> (tensor<512x32x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32>_409": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32>\n  return %ret : tensor<28x252x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x252x60xf32>) -> tensor<28x252x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x252x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x252x60xf32>\n    memref.copy %2, %alloc : memref<28x252x60xf32> to memref<28x252x60xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x252x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x252x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x252x60xf32>\n    return %3 : tensor<28x252x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x252x60xf32>) -> tensor<28x252x60xf32>\n  return %ret : tensor<28x252x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c252, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x252x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x64xf32>, tensor<5x5x5xf32>, tensor<28x252x60xf32>) -> (tensor<28x252x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32>_410": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32>\n  return %ret : tensor<30x1022x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022x1022xf32>\n    memref.copy %2, %alloc : memref<30x1022x1022xf32> to memref<30x1022x1022xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x1022x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x1022x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022x1022xf32>\n    return %3 : tensor<30x1022x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x1022xf32>) -> tensor<30x1022x1022xf32>\n  return %ret : tensor<30x1022x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x1022x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x1024xf32>, tensor<3x3x3xf32>, tensor<30x1022x1022xf32>) -> (tensor<30x1022x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>_411": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>\n  return %ret : tensor<124x1020x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020x124xf32>\n    memref.copy %2, %alloc : memref<124x1020x124xf32> to memref<124x1020x124xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x1020x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x1020x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020x124xf32>\n    return %3 : tensor<124x1020x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x124xf32>) -> tensor<124x1020x124xf32>\n  return %ret : tensor<124x1020x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x1020x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x128xf32>, tensor<5x5x5xf32>, tensor<124x1020x124xf32>) -> (tensor<124x1020x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32>_412": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32>\n  return %ret : tensor<28x124x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<28x124x508xf32>) -> tensor<28x124x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<28x124x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<28x124x508xf32>\n    memref.copy %2, %alloc : memref<28x124x508xf32> to memref<28x124x508xf32>\n    affine.for %arg3 = 0 to 28 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<28x124x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<28x124x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<28x124x508xf32>\n    return %3 : tensor<28x124x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<28x124x508xf32>) -> tensor<28x124x508xf32>\n  return %ret : tensor<28x124x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c28, %c124, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<28x124x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x512xf32>, tensor<5x5x5xf32>, tensor<28x124x508xf32>) -> (tensor<28x124x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 28, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32>_413": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32>\n  return %ret : tensor<58x58x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x58x250xf32>) -> tensor<58x58x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58x250xf32>\n    memref.copy %2, %alloc : memref<58x58x250xf32> to memref<58x58x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x58x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x58x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58x250xf32>\n    return %3 : tensor<58x58x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x250xf32>) -> tensor<58x58x250xf32>\n  return %ret : tensor<58x58x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x58x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x256xf32>, tensor<7x7x7xf32>, tensor<58x58x250xf32>) -> (tensor<58x58x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32>_414": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32>\n  return %ret : tensor<122x1018x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x1018xf32>\n    memref.copy %2, %alloc : memref<122x1018x1018xf32> to memref<122x1018x1018xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x1018xf32>\n    return %3 : tensor<122x1018x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x1018xf32>) -> tensor<122x1018x1018xf32>\n  return %ret : tensor<122x1018x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x1024xf32>, tensor<7x7x7xf32>, tensor<122x1018x1018xf32>) -> (tensor<122x1018x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>_415": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122x250xf32>\n    memref.copy %2, %alloc : memref<1018x122x250xf32> to memref<1018x122x250xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122x250xf32>\n    return %3 : tensor<1018x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x256xf32>, tensor<7x7x7xf32>, tensor<1018x122x250xf32>) -> (tensor<1018x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32>_416": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32>\n  return %ret : tensor<122x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x122x506xf32>) -> tensor<122x122x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122x506xf32>\n    memref.copy %2, %alloc : memref<122x122x506xf32> to memref<122x122x506xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x122x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122x506xf32>\n    return %3 : tensor<122x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x506xf32>) -> tensor<122x122x506xf32>\n  return %ret : tensor<122x122x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x122x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x512xf32>, tensor<7x7x7xf32>, tensor<122x122x506xf32>) -> (tensor<122x122x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>_417": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>\n  return %ret : tensor<250x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x250x58xf32>\n    memref.copy %2, %alloc : memref<250x250x58xf32> to memref<250x250x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x250x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x250x58xf32>\n    return %3 : tensor<250x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x250x58xf32>) -> tensor<250x250x58xf32>\n  return %ret : tensor<250x250x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c250, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x250x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x64xf32>, tensor<7x7x7xf32>, tensor<250x250x58xf32>) -> (tensor<250x250x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32>_418": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32>\n  return %ret : tensor<32x32x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x32x512xf32>) -> tensor<32x32x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x32x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x512xf32>\n    memref.copy %2, %alloc : memref<32x32x512xf32> to memref<32x32x512xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x32x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x32x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x32x512xf32>\n    return %3 : tensor<32x32x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x32x512xf32>) -> tensor<32x32x512xf32>\n  return %ret : tensor<32x32x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c32, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x32x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x512xf32>, tensor<1x1x1xf32>, tensor<32x32x512xf32>) -> (tensor<32x32x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32>_419": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32>\n  return %ret : tensor<512x128x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x128x512xf32>) -> tensor<512x128x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128x512xf32>\n    memref.copy %2, %alloc : memref<512x128x512xf32> to memref<512x128x512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x128x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x128x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128x512xf32>\n    return %3 : tensor<512x128x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x512xf32>) -> tensor<512x128x512xf32>\n  return %ret : tensor<512x128x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x128x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x512xf32>, tensor<1x1x1xf32>, tensor<512x128x512xf32>) -> (tensor<512x128x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32>_420": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32>\n  return %ret : tensor<1018x26x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x26x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x26x506xf32>\n    memref.copy %2, %alloc : memref<1018x26x506xf32> to memref<1018x26x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x26x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x26x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x26x506xf32>\n    return %3 : tensor<1018x26x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x26x506xf32>) -> tensor<1018x26x506xf32>\n  return %ret : tensor<1018x26x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c26, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x26x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x512xf32>, tensor<7x7x7xf32>, tensor<1018x26x506xf32>) -> (tensor<1018x26x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>_421": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>\n  return %ret : tensor<124x1020x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020x508xf32>\n    memref.copy %2, %alloc : memref<124x1020x508xf32> to memref<124x1020x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x1020x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x1020x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020x508xf32>\n    return %3 : tensor<124x1020x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x508xf32>) -> tensor<124x1020x508xf32>\n  return %ret : tensor<124x1020x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x1020x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x512xf32>, tensor<5x5x5xf32>, tensor<124x1020x508xf32>) -> (tensor<124x1020x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32>_422": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32>\n  return %ret : tensor<1022x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x64x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x62x30xf32>\n    memref.copy %2, %alloc : memref<1022x62x30xf32> to memref<1022x62x30xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x62x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x62x30xf32>\n    return %3 : tensor<1022x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x64x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x64x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x62x30xf32>) -> tensor<1022x62x30xf32>\n  return %ret : tensor<1022x62x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c62, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x62x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x64x32xf32>, tensor<3x3x3xf32>, tensor<1022x62x30xf32>) -> (tensor<1022x62x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>_423": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>\n  return %ret : tensor<1020x28x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x252xf32>\n    memref.copy %2, %alloc : memref<1020x28x252xf32> to memref<1020x28x252xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x252xf32>\n    return %3 : tensor<1020x28x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x252xf32>) -> tensor<1020x28x252xf32>\n  return %ret : tensor<1020x28x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x256xf32>, tensor<5x5x5xf32>, tensor<1020x28x252xf32>) -> (tensor<1020x28x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32>_424": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32>\n  return %ret : tensor<64x32x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x32x512xf32>) -> tensor<64x32x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x512xf32>\n    memref.copy %2, %alloc : memref<64x32x512xf32> to memref<64x32x512xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x32x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x32x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32x512xf32>\n    return %3 : tensor<64x32x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x512xf32>) -> tensor<64x32x512xf32>\n  return %ret : tensor<64x32x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x32x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x512xf32>, tensor<1x1x1xf32>, tensor<64x32x512xf32>) -> (tensor<64x32x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32>_425": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32>\n  return %ret : tensor<64x64x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x1024xf32>\n    memref.copy %2, %alloc : memref<64x64x1024xf32> to memref<64x64x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x1024xf32>\n    return %3 : tensor<64x64x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x1024xf32>) -> tensor<64x64x1024xf32>\n  return %ret : tensor<64x64x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x1024xf32>, tensor<1x1x1xf32>, tensor<64x64x1024xf32>) -> (tensor<64x64x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>_426": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>\n  return %ret : tensor<256x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x64xf32>\n    memref.copy %2, %alloc : memref<256x32x64xf32> to memref<256x32x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x64xf32>\n    return %3 : tensor<256x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x64xf32>) -> tensor<256x32x64xf32>\n  return %ret : tensor<256x32x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x64xf32>, tensor<1x1x1xf32>, tensor<256x32x64xf32>) -> (tensor<256x32x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>_427": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>\n  return %ret : tensor<62x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x32xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30x30xf32>\n    memref.copy %2, %alloc : memref<62x30x30xf32> to memref<62x30x30xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x30x30xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30x30xf32>\n    return %3 : tensor<62x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x32xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x32xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x30xf32>) -> tensor<62x30x30xf32>\n  return %ret : tensor<62x30x30xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30, %c30) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x30x30xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x32xf32>, tensor<3x3x3xf32>, tensor<62x30x30xf32>) -> (tensor<62x30x30xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32>_428": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32>\n  return %ret : tensor<64x1024x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x1024x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024x64xf32>\n    memref.copy %2, %alloc : memref<64x1024x64xf32> to memref<64x1024x64xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x1024x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x1024x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x1024x64xf32>\n    return %3 : tensor<64x1024x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x1024x64xf32>) -> tensor<64x1024x64xf32>\n  return %ret : tensor<64x1024x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c1024, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x1024x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x64xf32>, tensor<1x1x1xf32>, tensor<64x1024x64xf32>) -> (tensor<64x1024x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32>_429": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32>\n  return %ret : tensor<510x1022x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x1022x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x1022x1022xf32>\n    memref.copy %2, %alloc : memref<510x1022x1022xf32> to memref<510x1022x1022xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x1022x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x1022x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x1022x1022xf32>\n    return %3 : tensor<510x1022x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x1022x1022xf32>) -> tensor<510x1022x1022xf32>\n  return %ret : tensor<510x1022x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c1022, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x1022x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x1024xf32>, tensor<3x3x3xf32>, tensor<510x1022x1022xf32>) -> (tensor<510x1022x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32>_430": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32>\n  return %ret : tensor<62x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x254x62xf32>) -> tensor<62x254x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x254x62xf32>\n    memref.copy %2, %alloc : memref<62x254x62xf32> to memref<62x254x62xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x254x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x254x62xf32>\n    return %3 : tensor<62x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x254x62xf32>) -> tensor<62x254x62xf32>\n  return %ret : tensor<62x254x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c254, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x254x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x64xf32>, tensor<3x3x3xf32>, tensor<62x254x62xf32>) -> (tensor<62x254x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32>_431": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32>\n  return %ret : tensor<1022x254x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x126xf32>\n    memref.copy %2, %alloc : memref<1022x254x126xf32> to memref<1022x254x126xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x126xf32>\n    return %3 : tensor<1022x254x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x126xf32>) -> tensor<1022x254x126xf32>\n  return %ret : tensor<1022x254x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x128xf32>, tensor<3x3x3xf32>, tensor<1022x254x126xf32>) -> (tensor<1022x254x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>_432": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>\n  return %ret : tensor<1022x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x126x126xf32>\n    memref.copy %2, %alloc : memref<1022x126x126xf32> to memref<1022x126x126xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x126x126xf32>\n    return %3 : tensor<1022x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x126x126xf32>) -> tensor<1022x126x126xf32>\n  return %ret : tensor<1022x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x128xf32>, tensor<3x3x3xf32>, tensor<1022x126x126xf32>) -> (tensor<1022x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>_433": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>\n  return %ret : tensor<58x122x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x1018xf32>\n    memref.copy %2, %alloc : memref<58x122x1018xf32> to memref<58x122x1018xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x1018xf32>\n    return %3 : tensor<58x122x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x1018xf32>) -> tensor<58x122x1018xf32>\n  return %ret : tensor<58x122x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x1024xf32>, tensor<7x7x7xf32>, tensor<58x122x1018xf32>) -> (tensor<58x122x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32>_434": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32>\n  return %ret : tensor<26x122x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x122x26xf32>) -> tensor<26x122x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x122x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x122x26xf32>\n    memref.copy %2, %alloc : memref<26x122x26xf32> to memref<26x122x26xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x122x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x122x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x122x26xf32>\n    return %3 : tensor<26x122x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x122x26xf32>) -> tensor<26x122x26xf32>\n  return %ret : tensor<26x122x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c122, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x122x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x32xf32>, tensor<7x7x7xf32>, tensor<26x122x26xf32>) -> (tensor<26x122x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>_435": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>\n  return %ret : tensor<64x32x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x1024xf32>\n    memref.copy %2, %alloc : memref<64x32x1024xf32> to memref<64x32x1024xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x32x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x32x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32x1024xf32>\n    return %3 : tensor<64x32x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x1024xf32>) -> tensor<64x32x1024xf32>\n  return %ret : tensor<64x32x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x32x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x1024xf32>, tensor<1x1x1xf32>, tensor<64x32x1024xf32>) -> (tensor<64x32x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32>_436": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32>\n  return %ret : tensor<254x510x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<254x510x126xf32>) -> tensor<254x510x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<254x510x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<254x510x126xf32>\n    memref.copy %2, %alloc : memref<254x510x126xf32> to memref<254x510x126xf32>\n    affine.for %arg3 = 0 to 254 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<254x510x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<254x510x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<254x510x126xf32>\n    return %3 : tensor<254x510x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<254x510x126xf32>) -> tensor<254x510x126xf32>\n  return %ret : tensor<254x510x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c254, %c510, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<254x510x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x128xf32>, tensor<3x3x3xf32>, tensor<254x510x126xf32>) -> (tensor<254x510x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 254, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32>_437": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32>\n  return %ret : tensor<64x64x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x64x32xf32>) -> tensor<64x64x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x64x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x32xf32>\n    memref.copy %2, %alloc : memref<64x64x32xf32> to memref<64x64x32xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x64x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x64x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x64x32xf32>\n    return %3 : tensor<64x64x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x64x32xf32>) -> tensor<64x64x32xf32>\n  return %ret : tensor<64x64x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c64, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x64x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x32xf32>, tensor<1x1x1xf32>, tensor<64x64x32xf32>) -> (tensor<64x64x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32>_438": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32>\n  return %ret : tensor<252x124x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x124x60xf32>) -> tensor<252x124x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x124x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x124x60xf32>\n    memref.copy %2, %alloc : memref<252x124x60xf32> to memref<252x124x60xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x124x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x124x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x124x60xf32>\n    return %3 : tensor<252x124x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x128x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x128x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x124x60xf32>) -> tensor<252x124x60xf32>\n  return %ret : tensor<252x124x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c124, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x124x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x128x64xf32>, tensor<5x5x5xf32>, tensor<252x124x60xf32>) -> (tensor<252x124x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32>_439": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32>\n  return %ret : tensor<124x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x508x252xf32>) -> tensor<124x508x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508x252xf32>\n    memref.copy %2, %alloc : memref<124x508x252xf32> to memref<124x508x252xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x508x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508x252xf32>\n    return %3 : tensor<124x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x252xf32>) -> tensor<124x508x252xf32>\n  return %ret : tensor<124x508x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x508x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x256xf32>, tensor<5x5x5xf32>, tensor<124x508x252xf32>) -> (tensor<124x508x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32>_440": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32>\n  return %ret : tensor<1024x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1024x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1024x32x64xf32>\n    memref.copy %2, %alloc : memref<1024x32x64xf32> to memref<1024x32x64xf32>\n    affine.for %arg3 = 0 to 1024 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1024x32x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1024x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1024x32x64xf32>\n    return %3 : tensor<1024x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<1024x32x64xf32>) -> tensor<1024x32x64xf32>\n  return %ret : tensor<1024x32x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c1024, %c32, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1024x32x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x64xf32>, tensor<1x1x1xf32>, tensor<1024x32x64xf32>) -> (tensor<1024x32x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1024, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32>_441": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32>\n  return %ret : tensor<122x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x1018x506xf32>\n    memref.copy %2, %alloc : memref<122x1018x506xf32> to memref<122x1018x506xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x1018x506xf32>\n    return %3 : tensor<122x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x1018x506xf32>) -> tensor<122x1018x506xf32>\n  return %ret : tensor<122x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x512xf32>, tensor<7x7x7xf32>, tensor<122x1018x506xf32>) -> (tensor<122x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32>_442": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32>\n  return %ret : tensor<122x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x128x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x122x58xf32>) -> tensor<122x122x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x128x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x122x58xf32>\n    memref.copy %2, %alloc : memref<122x122x58xf32> to memref<122x122x58xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x128x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x122x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x122x58xf32>\n    return %3 : tensor<122x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x128x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x128x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x122x58xf32>) -> tensor<122x122x58xf32>\n  return %ret : tensor<122x122x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c128, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x128x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c122, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x122x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x128x64xf32>, tensor<7x7x7xf32>, tensor<122x122x58xf32>) -> (tensor<122x122x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32>_443": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32>\n  return %ret : tensor<1018x250x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x250x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x250x506xf32>\n    memref.copy %2, %alloc : memref<1018x250x506xf32> to memref<1018x250x506xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x250x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x250x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x250x506xf32>\n    return %3 : tensor<1018x250x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x250x506xf32>) -> tensor<1018x250x506xf32>\n  return %ret : tensor<1018x250x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c250, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x250x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x512xf32>, tensor<7x7x7xf32>, tensor<1018x250x506xf32>) -> (tensor<1018x250x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32>_444": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32>\n  return %ret : tensor<122x26x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x26x58xf32>) -> tensor<122x26x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x26x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x26x58xf32>\n    memref.copy %2, %alloc : memref<122x26x58xf32> to memref<122x26x58xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x26x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x26x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x26x58xf32>\n    return %3 : tensor<122x26x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x26x58xf32>) -> tensor<122x26x58xf32>\n  return %ret : tensor<122x26x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c26, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x26x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x64xf32>, tensor<7x7x7xf32>, tensor<122x26x58xf32>) -> (tensor<122x26x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32>_445": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32>\n  return %ret : tensor<30x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x30x62xf32>) -> tensor<30x30x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x30x62xf32>\n    memref.copy %2, %alloc : memref<30x30x62xf32> to memref<30x30x62xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x30x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x30x62xf32>\n    return %3 : tensor<30x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x32x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x32x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x30x62xf32>) -> tensor<30x30x62xf32>\n  return %ret : tensor<30x30x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c30, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x30x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x32x64xf32>, tensor<3x3x3xf32>, tensor<30x30x62xf32>) -> (tensor<30x30x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>_446": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>\n  return %ret : tensor<252x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x28x508xf32>\n    memref.copy %2, %alloc : memref<252x28x508xf32> to memref<252x28x508xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x28x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x28x508xf32>\n    return %3 : tensor<252x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x28x508xf32>) -> tensor<252x28x508xf32>\n  return %ret : tensor<252x28x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c28, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x28x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x512xf32>, tensor<5x5x5xf32>, tensor<252x28x508xf32>) -> (tensor<252x28x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>_447": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>\n  return %ret : tensor<126x30x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x32x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<126x30x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<126x30x510xf32>\n    memref.copy %2, %alloc : memref<126x30x510xf32> to memref<126x30x510xf32>\n    affine.for %arg3 = 0 to 126 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<126x30x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<126x30x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<126x30x510xf32>\n    return %3 : tensor<126x30x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<126x30x510xf32>) -> tensor<126x30x510xf32>\n  return %ret : tensor<126x30x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c126, %c30, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<126x30x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x32x512xf32>, tensor<3x3x3xf32>, tensor<126x30x510xf32>) -> (tensor<126x30x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 126, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32>_448": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32>\n  return %ret : tensor<60x124x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x124x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x124x1020xf32>\n    memref.copy %2, %alloc : memref<60x124x1020xf32> to memref<60x124x1020xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x124x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x124x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x124x1020xf32>\n    return %3 : tensor<60x124x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x124x1020xf32>) -> tensor<60x124x1020xf32>\n  return %ret : tensor<60x124x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c124, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x124x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x1024xf32>, tensor<5x5x5xf32>, tensor<60x124x1020xf32>) -> (tensor<60x124x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32>_449": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32>\n  return %ret : tensor<60x60x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<60x60x508xf32>) -> tensor<60x60x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<60x60x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<60x60x508xf32>\n    memref.copy %2, %alloc : memref<60x60x508xf32> to memref<60x60x508xf32>\n    affine.for %arg3 = 0 to 60 {\n      affine.for %arg4 = 0 to 60 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<60x60x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<60x60x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<60x60x508xf32>\n    return %3 : tensor<60x60x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<60x60x508xf32>) -> tensor<60x60x508xf32>\n  return %ret : tensor<60x60x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c60, %c60, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<60x60x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x512xf32>, tensor<5x5x5xf32>, tensor<60x60x508xf32>) -> (tensor<60x60x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 60, 1], ["%arg4", 0, 60, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32>_450": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32>\n  return %ret : tensor<250x26x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x26x58xf32>) -> tensor<250x26x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26x58xf32>\n    memref.copy %2, %alloc : memref<250x26x58xf32> to memref<250x26x58xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x26x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x26x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26x58xf32>\n    return %3 : tensor<250x26x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x58xf32>) -> tensor<250x26x58xf32>\n  return %ret : tensor<250x26x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x26x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x64xf32>, tensor<7x7x7xf32>, tensor<250x26x58xf32>) -> (tensor<250x26x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32>_451": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32>\n  return %ret : tensor<124x1020x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x1024x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x1020x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x1020x28xf32>\n    memref.copy %2, %alloc : memref<124x1020x28xf32> to memref<124x1020x28xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x1020x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x1020x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x1020x28xf32>\n    return %3 : tensor<124x1020x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x1024x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x1024x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x1020x28xf32>) -> tensor<124x1020x28xf32>\n  return %ret : tensor<124x1020x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c1020, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x1020x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x1024x32xf32>, tensor<5x5x5xf32>, tensor<124x1020x28xf32>) -> (tensor<124x1020x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32>_452": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32>\n  return %ret : tensor<1022x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x254x62xf32>\n    memref.copy %2, %alloc : memref<1022x254x62xf32> to memref<1022x254x62xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x254x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x254x62xf32>\n    return %3 : tensor<1022x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x254x62xf32>) -> tensor<1022x254x62xf32>\n  return %ret : tensor<1022x254x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c254, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x254x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x64xf32>, tensor<3x3x3xf32>, tensor<1022x254x62xf32>) -> (tensor<1022x254x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32>_453": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32>\n  return %ret : tensor<256x512x1024xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x1024xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x512x1024xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x1024xf32>\n    memref.copy %2, %alloc : memref<256x512x1024xf32> to memref<256x512x1024xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 1024 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x512x1024xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x512x1024xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x512x1024xf32>\n    return %3 : tensor<256x512x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x1024xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x512x1024xf32>) -> tensor<256x512x1024xf32>\n  return %ret : tensor<256x512x1024xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c512, %c1024) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x512x1024xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x1024xf32>, tensor<1x1x1xf32>, tensor<256x512x1024xf32>) -> (tensor<256x512x1024xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 1024, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32>_454": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32>\n  return %ret : tensor<64x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<64x32x256xf32>) -> tensor<64x32x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<64x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x256xf32>\n    memref.copy %2, %alloc : memref<64x32x256xf32> to memref<64x32x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<64x32x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<64x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<64x32x256xf32>\n    return %3 : tensor<64x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<64x32x256xf32>) -> tensor<64x32x256xf32>\n  return %ret : tensor<64x32x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c64 = arith.constant 64 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c64, %c32, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<64x32x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x256xf32>, tensor<1x1x1xf32>, tensor<64x32x256xf32>) -> (tensor<64x32x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>_455": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x250x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x250x250xf32>\n    memref.copy %2, %alloc : memref<26x250x250xf32> to memref<26x250x250xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x250x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x250x250xf32>\n    return %3 : tensor<26x250x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x250x250xf32>) -> tensor<26x250x250xf32>\n  return %ret : tensor<26x250x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c250, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x250x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x256xf32>, tensor<7x7x7xf32>, tensor<26x250x250xf32>) -> (tensor<26x250x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>_456": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>\n  return %ret : tensor<124x508x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x512xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508x508xf32>\n    memref.copy %2, %alloc : memref<124x508x508xf32> to memref<124x508x508xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x508x508xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x508x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508x508xf32>\n    return %3 : tensor<124x508x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x512xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x512xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x508xf32>) -> tensor<124x508x508xf32>\n  return %ret : tensor<124x508x508xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508, %c508) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x508x508xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x512xf32>, tensor<5x5x5xf32>, tensor<124x508x508xf32>) -> (tensor<124x508x508xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32>_457": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32>\n  return %ret : tensor<32x128x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x128x512xf32>) -> tensor<32x128x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x128x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x128x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x512xf32>\n    memref.copy %2, %alloc : memref<32x128x512xf32> to memref<32x128x512xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x128x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x128x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x128x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x128x512xf32>\n    return %3 : tensor<32x128x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x128x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x128x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x128x512xf32>) -> tensor<32x128x512xf32>\n  return %ret : tensor<32x128x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c128, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x128x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x128x512xf32>, tensor<1x1x1xf32>, tensor<32x128x512xf32>) -> (tensor<32x128x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>_458": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x122x250xf32>\n    memref.copy %2, %alloc : memref<58x122x250xf32> to memref<58x122x250xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x122x250xf32>\n    return %3 : tensor<58x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x122x250xf32>) -> tensor<58x122x250xf32>\n  return %ret : tensor<58x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x256xf32>, tensor<7x7x7xf32>, tensor<58x122x250xf32>) -> (tensor<58x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32>_459": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32>\n  return %ret : tensor<512x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x128x128xf32>) -> tensor<512x128x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128x128xf32>\n    memref.copy %2, %alloc : memref<512x128x128xf32> to memref<512x128x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x128x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128x128xf32>\n    return %3 : tensor<512x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x128xf32>) -> tensor<512x128x128xf32>\n  return %ret : tensor<512x128x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x128x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x128xf32>, tensor<1x1x1xf32>, tensor<512x128x128xf32>) -> (tensor<512x128x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32>_460": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32>\n  return %ret : tensor<508x252x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x252x60xf32>) -> tensor<508x252x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x252x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x252x60xf32>\n    memref.copy %2, %alloc : memref<508x252x60xf32> to memref<508x252x60xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x252x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x252x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x252x60xf32>\n    return %3 : tensor<508x252x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x252x60xf32>) -> tensor<508x252x60xf32>\n  return %ret : tensor<508x252x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c252, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x252x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x64xf32>, tensor<5x5x5xf32>, tensor<508x252x60xf32>) -> (tensor<508x252x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32>_461": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32>\n  return %ret : tensor<512x128x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x128x256xf32>) -> tensor<512x128x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x128x256xf32>\n    memref.copy %2, %alloc : memref<512x128x256xf32> to memref<512x128x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x128x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x128x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x128x256xf32>\n    return %3 : tensor<512x128x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x128x256xf32>) -> tensor<512x128x256xf32>\n  return %ret : tensor<512x128x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c128, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x128x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x256xf32>, tensor<1x1x1xf32>, tensor<512x128x256xf32>) -> (tensor<512x128x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>_462": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>\n  return %ret : tensor<62x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x128x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x126x126xf32>\n    memref.copy %2, %alloc : memref<62x126x126xf32> to memref<62x126x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 126 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x128x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x126x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x126x126xf32>\n    return %3 : tensor<62x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x128x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x128x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x126x126xf32>) -> tensor<62x126x126xf32>\n  return %ret : tensor<62x126x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c128, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x128x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c126, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x126x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x128x128xf32>, tensor<3x3x3xf32>, tensor<62x126x126xf32>) -> (tensor<62x126x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 126, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>_463": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x128x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x128x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1018x122x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1018x122x250xf32>\n    memref.copy %2, %alloc : memref<1018x122x250xf32> to memref<1018x122x250xf32>\n    affine.for %arg3 = 0 to 1018 {\n      affine.for %arg4 = 0 to 122 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x128x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1018x122x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1018x122x250xf32>\n    return %3 : tensor<1018x122x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x128x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x128x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<1018x122x250xf32>) -> tensor<1018x122x250xf32>\n  return %ret : tensor<1018x122x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c128, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x128x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c1018, %c122, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1018x122x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x128x256xf32>, tensor<7x7x7xf32>, tensor<1018x122x250xf32>) -> (tensor<1018x122x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1018, 1], ["%arg4", 0, 122, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32>_464": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32>\n  return %ret : tensor<510x254x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x254x510xf32>) -> tensor<510x254x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254x510xf32>\n    memref.copy %2, %alloc : memref<510x254x510xf32> to memref<510x254x510xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x254x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x254x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254x510xf32>\n    return %3 : tensor<510x254x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x510xf32>) -> tensor<510x254x510xf32>\n  return %ret : tensor<510x254x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x254x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x512xf32>, tensor<3x3x3xf32>, tensor<510x254x510xf32>) -> (tensor<510x254x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>_465": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>\n  return %ret : tensor<32x1024x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x512xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<32x1024x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024x512xf32>\n    memref.copy %2, %alloc : memref<32x1024x512xf32> to memref<32x1024x512xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<32x1024x512xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<32x1024x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<32x1024x512xf32>\n    return %3 : tensor<32x1024x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x512xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x512xf32>, tensor<1x1x1xf32>) outs(%output: tensor<32x1024x512xf32>) -> tensor<32x1024x512xf32>\n  return %ret : tensor<32x1024x512xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c32, %c1024, %c512) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<32x1024x512xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x512xf32>, tensor<1x1x1xf32>, tensor<32x1024x512xf32>) -> (tensor<32x1024x512xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32>_466": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32>\n  return %ret : tensor<30x1022x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x1022x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x1022x62xf32>\n    memref.copy %2, %alloc : memref<30x1022x62xf32> to memref<30x1022x62xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 1022 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x1022x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x1022x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x1022x62xf32>\n    return %3 : tensor<30x1022x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x1024x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x1024x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x1022x62xf32>) -> tensor<30x1022x62xf32>\n  return %ret : tensor<30x1022x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c1022, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x1022x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x1024x64xf32>, tensor<3x3x3xf32>, tensor<30x1022x62xf32>) -> (tensor<30x1022x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 1022, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>_467": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>\n  return %ret : tensor<510x62x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x62x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x62x510xf32>\n    memref.copy %2, %alloc : memref<510x62x510xf32> to memref<510x62x510xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x62x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x62x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x62x510xf32>\n    return %3 : tensor<510x62x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x62x510xf32>) -> tensor<510x62x510xf32>\n  return %ret : tensor<510x62x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c62, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x62x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x512xf32>, tensor<3x3x3xf32>, tensor<510x62x510xf32>) -> (tensor<510x62x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32>_468": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32>\n  return %ret : tensor<30x254x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x254x126xf32>) -> tensor<30x254x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254x126xf32>\n    memref.copy %2, %alloc : memref<30x254x126xf32> to memref<30x254x126xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x254x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x254x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254x126xf32>\n    return %3 : tensor<30x254x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x126xf32>) -> tensor<30x254x126xf32>\n  return %ret : tensor<30x254x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x254x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x128xf32>, tensor<3x3x3xf32>, tensor<30x254x126xf32>) -> (tensor<30x254x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32>_469": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32>\n  return %ret : tensor<26x58x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x64xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<26x58x58xf32>) -> tensor<26x58x58xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<26x58x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<26x58x58xf32>\n    memref.copy %2, %alloc : memref<26x58x58xf32> to memref<26x58x58xf32>\n    affine.for %arg3 = 0 to 26 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<26x58x58xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<26x58x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<26x58x58xf32>\n    return %3 : tensor<26x58x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x64xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x64xf32>, tensor<7x7x7xf32>) outs(%output: tensor<26x58x58xf32>) -> tensor<26x58x58xf32>\n  return %ret : tensor<26x58x58xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c26, %c58, %c58) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<26x58x58xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x64xf32>, tensor<7x7x7xf32>, tensor<26x58x58xf32>) -> (tensor<26x58x58xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 26, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32>_470": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32>\n  return %ret : tensor<62x510x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x510x126xf32>) -> tensor<62x510x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x510x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x510x126xf32>\n    memref.copy %2, %alloc : memref<62x510x126xf32> to memref<62x510x126xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 510 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x510x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x510x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x510x126xf32>\n    return %3 : tensor<62x510x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x512x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x512x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x510x126xf32>) -> tensor<62x510x126xf32>\n  return %ret : tensor<62x510x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c510, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x510x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x512x128xf32>, tensor<3x3x3xf32>, tensor<62x510x126xf32>) -> (tensor<62x510x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 510, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32>_471": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32>\n  return %ret : tensor<1020x1020x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x1024x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x1020x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x1020x60xf32>\n    memref.copy %2, %alloc : memref<1020x1020x60xf32> to memref<1020x1020x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 1020 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x1020x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x1020x60xf32>\n    return %3 : tensor<1020x1020x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x1024x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x1024x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x1020x60xf32>) -> tensor<1020x1020x60xf32>\n  return %ret : tensor<1020x1020x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c1020, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x1020x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x1024x64xf32>, tensor<5x5x5xf32>, tensor<1020x1020x60xf32>) -> (tensor<1020x1020x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 1020, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32>_472": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32>\n  return %ret : tensor<58x1018x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x1024x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x1018x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x1018x26xf32>\n    memref.copy %2, %alloc : memref<58x1018x26xf32> to memref<58x1018x26xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x1024x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x1018x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x1018x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x1018x26xf32>\n    return %3 : tensor<58x1018x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x1024x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x1024x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x1018x26xf32>) -> tensor<58x1018x26xf32>\n  return %ret : tensor<58x1018x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c1024, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x1024x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c1018, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x1018x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x1024x32xf32>, tensor<7x7x7xf32>, tensor<58x1018x26xf32>) -> (tensor<58x1018x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32>_473": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32>\n  return %ret : tensor<256x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x32xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x32xf32>) -> tensor<256x256x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x32xf32>\n    memref.copy %2, %alloc : memref<256x256x32xf32> to memref<256x256x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x32xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x32xf32>\n    return %3 : tensor<256x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x32xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x32xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x32xf32>) -> tensor<256x256x32xf32>\n  return %ret : tensor<256x256x32xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c32) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x32xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x32xf32>, tensor<1x1x1xf32>, tensor<256x256x32xf32>) -> (tensor<256x256x32xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32>_474": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32>\n  return %ret : tensor<58x58x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x58x26xf32>) -> tensor<58x58x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x58x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x58x26xf32>\n    memref.copy %2, %alloc : memref<58x58x26xf32> to memref<58x58x26xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x58x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x58x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x58x26xf32>\n    return %3 : tensor<58x58x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x58x26xf32>) -> tensor<58x58x26xf32>\n  return %ret : tensor<58x58x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c58, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x58x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x64x32xf32>, tensor<7x7x7xf32>, tensor<58x58x26xf32>) -> (tensor<58x58x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32>_475": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32>\n  return %ret : tensor<510x254x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x256x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<510x254x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<510x254x1022xf32>\n    memref.copy %2, %alloc : memref<510x254x1022xf32> to memref<510x254x1022xf32>\n    affine.for %arg3 = 0 to 510 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<510x254x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<510x254x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<510x254x1022xf32>\n    return %3 : tensor<510x254x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x256x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x256x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<510x254x1022xf32>) -> tensor<510x254x1022xf32>\n  return %ret : tensor<510x254x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c510, %c254, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<510x254x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x256x1024xf32>, tensor<3x3x3xf32>, tensor<510x254x1022xf32>) -> (tensor<510x254x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 510, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>_476": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x64xf32>\n    memref.copy %2, %alloc : memref<512x1024x64xf32> to memref<512x1024x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x64xf32>\n    return %3 : tensor<512x1024x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x64xf32>, tensor<1x1x1xf32>, tensor<512x1024x64xf32>) -> (tensor<512x1024x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32>_477": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32>\n  return %ret : tensor<252x508x1020xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x1024xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x512x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<252x508x1020xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<252x508x1020xf32>\n    memref.copy %2, %alloc : memref<252x508x1020xf32> to memref<252x508x1020xf32>\n    affine.for %arg3 = 0 to 252 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 1020 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x512x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<252x508x1020xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<252x508x1020xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<252x508x1020xf32>\n    return %3 : tensor<252x508x1020xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x512x1024xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x512x1024xf32>, tensor<5x5x5xf32>) outs(%output: tensor<252x508x1020xf32>) -> tensor<252x508x1020xf32>\n  return %ret : tensor<252x508x1020xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c512, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x512x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c252, %c508, %c1020) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<252x508x1020xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x512x1024xf32>, tensor<5x5x5xf32>, tensor<252x508x1020xf32>) -> (tensor<252x508x1020xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 252, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 1020, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32>_478": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32>\n  return %ret : tensor<62x30x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x256xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x30x254xf32>) -> tensor<62x30x254xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30x254xf32>\n    memref.copy %2, %alloc : memref<62x30x254xf32> to memref<62x30x254xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x30x254xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x30x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30x254xf32>\n    return %3 : tensor<62x30x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x256xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x256xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x254xf32>) -> tensor<62x30x254xf32>\n  return %ret : tensor<62x30x254xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30, %c254) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x30x254xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x256xf32>, tensor<3x3x3xf32>, tensor<62x30x254xf32>) -> (tensor<62x30x254xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32>_479": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32>\n  return %ret : tensor<124x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<124x508x124xf32>) -> tensor<124x508x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<124x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<124x508x124xf32>\n    memref.copy %2, %alloc : memref<124x508x124xf32> to memref<124x508x124xf32>\n    affine.for %arg3 = 0 to 124 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<124x508x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<124x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<124x508x124xf32>\n    return %3 : tensor<124x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<124x508x124xf32>) -> tensor<124x508x124xf32>\n  return %ret : tensor<124x508x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c124, %c508, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<124x508x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x128xf32>, tensor<5x5x5xf32>, tensor<124x508x124xf32>) -> (tensor<124x508x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 124, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32>_480": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32>\n  return %ret : tensor<1022x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x128xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1022x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1022x30x126xf32>\n    memref.copy %2, %alloc : memref<1022x30x126xf32> to memref<1022x30x126xf32>\n    affine.for %arg3 = 0 to 1022 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1022x30x126xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1022x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1022x30x126xf32>\n    return %3 : tensor<1022x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x128xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x128xf32>, tensor<3x3x3xf32>) outs(%output: tensor<1022x30x126xf32>) -> tensor<1022x30x126xf32>\n  return %ret : tensor<1022x30x126xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c126 = arith.constant 126 : index\n  %c128 = arith.constant 128 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c1022, %c30, %c126) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1022x30x126xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x128xf32>, tensor<3x3x3xf32>, tensor<1022x30x126xf32>) -> (tensor<1022x30x126xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1022, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>_481": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>\n  return %ret : tensor<122x250x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x256x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x256x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<122x250x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<122x250x1018xf32>\n    memref.copy %2, %alloc : memref<122x250x1018xf32> to memref<122x250x1018xf32>\n    affine.for %arg3 = 0 to 122 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x256x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<122x250x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<122x250x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<122x250x1018xf32>\n    return %3 : tensor<122x250x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x256x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x256x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<122x250x1018xf32>) -> tensor<122x250x1018xf32>\n  return %ret : tensor<122x250x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c256, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x256x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c122, %c250, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<122x250x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x256x1024xf32>, tensor<7x7x7xf32>, tensor<122x250x1018xf32>) -> (tensor<122x250x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 122, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32>_482": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32>\n  return %ret : tensor<250x1018x1018xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x1024xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x1024x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x1018x1018xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x1018x1018xf32>\n    memref.copy %2, %alloc : memref<250x1018x1018xf32> to memref<250x1018x1018xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 1018 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x1024x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x1018x1018xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x1018x1018xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x1018x1018xf32>\n    return %3 : tensor<250x1018x1018xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x1024x1024xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x1024x1024xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x1018x1018xf32>) -> tensor<250x1018x1018xf32>\n  return %ret : tensor<250x1018x1018xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c1024, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x1024x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c1018, %c1018) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x1018x1018xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x1024x1024xf32>, tensor<7x7x7xf32>, tensor<250x1018x1018xf32>) -> (tensor<250x1018x1018xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 1018, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32>_483": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32>\n  return %ret : tensor<250x58x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x256xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x58x250xf32>) -> tensor<250x58x250xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x64x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x58x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x58x250xf32>\n    memref.copy %2, %alloc : memref<250x58x250xf32> to memref<250x58x250xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x64x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x58x250xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x58x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x58x250xf32>\n    return %3 : tensor<250x58x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x64x256xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x64x256xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x58x250xf32>) -> tensor<250x58x250xf32>\n  return %ret : tensor<250x58x250xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c64, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x64x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c58, %c250) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x58x250xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x64x256xf32>, tensor<7x7x7xf32>, tensor<250x58x250xf32>) -> (tensor<250x58x250xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>_484": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>\n  return %ret : tensor<256x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x128xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x128xf32>\n    memref.copy %2, %alloc : memref<256x32x128xf32> to memref<256x32x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x32x128xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x32x128xf32>\n    return %3 : tensor<256x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x32x128xf32>) -> tensor<256x32x128xf32>\n  return %ret : tensor<256x32x128xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c32 = arith.constant 32 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x128xf32>, tensor<1x1x1xf32>, tensor<256x32x128xf32>) -> (tensor<256x32x128xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32>_485": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32>\n  return %ret : tensor<30x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x64xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x254x62xf32>) -> tensor<30x254x62xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x256x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x254x62xf32>\n    memref.copy %2, %alloc : memref<30x254x62xf32> to memref<30x254x62xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 254 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x256x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x254x62xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x254x62xf32>\n    return %3 : tensor<30x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x256x64xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x256x64xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x254x62xf32>) -> tensor<30x254x62xf32>\n  return %ret : tensor<30x254x62xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c254 = arith.constant 254 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c256, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x256x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c254, %c62) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x254x62xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x256x64xf32>, tensor<3x3x3xf32>, tensor<30x254x62xf32>) -> (tensor<30x254x62xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 254, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32>_486": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32>\n  return %ret : tensor<62x30x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x512xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<62x30x510xf32>) -> tensor<62x30x510xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x32x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<62x30x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<62x30x510xf32>\n    memref.copy %2, %alloc : memref<62x30x510xf32> to memref<62x30x510xf32>\n    affine.for %arg3 = 0 to 62 {\n      affine.for %arg4 = 0 to 30 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x32x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<62x30x510xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<62x30x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<62x30x510xf32>\n    return %3 : tensor<62x30x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x32x512xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x32x512xf32>, tensor<3x3x3xf32>) outs(%output: tensor<62x30x510xf32>) -> tensor<62x30x510xf32>\n  return %ret : tensor<62x30x510xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c510 = arith.constant 510 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c32, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x32x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c62, %c30, %c510) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<62x30x510xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x32x512xf32>, tensor<3x3x3xf32>, tensor<62x30x510xf32>) -> (tensor<62x30x510xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 62, 1], ["%arg4", 0, 30, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32>_487": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32>\n  return %ret : tensor<250x26x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<250x26x122xf32>) -> tensor<250x26x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x32x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<250x26x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<250x26x122xf32>\n    memref.copy %2, %alloc : memref<250x26x122xf32> to memref<250x26x122xf32>\n    affine.for %arg3 = 0 to 250 {\n      affine.for %arg4 = 0 to 26 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x32x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<250x26x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<250x26x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<250x26x122xf32>\n    return %3 : tensor<250x26x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x32x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x32x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<250x26x122xf32>) -> tensor<250x26x122xf32>\n  return %ret : tensor<250x26x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c32, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x32x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c250, %c26, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<250x26x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x32x128xf32>, tensor<7x7x7xf32>, tensor<250x26x122xf32>) -> (tensor<250x26x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 250, 1], ["%arg4", 0, 26, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32>_488": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32>\n  return %ret : tensor<508x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x512x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x508x124xf32>) -> tensor<508x508x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x508x124xf32>\n    memref.copy %2, %alloc : memref<508x508x124xf32> to memref<508x508x124xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x508x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x508x124xf32>\n    return %3 : tensor<508x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x508x124xf32>) -> tensor<508x508x124xf32>\n  return %ret : tensor<508x508x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c508, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x508x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x512x128xf32>, tensor<5x5x5xf32>, tensor<508x508x124xf32>) -> (tensor<508x508x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>_489": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x64xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1024x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1024x64xf32>\n    memref.copy %2, %alloc : memref<512x1024x64xf32> to memref<512x1024x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<512x1024x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1024x64xf32>\n    return %3 : tensor<512x1024x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x64xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x64xf32>, tensor<1x1x1xf32>) outs(%output: tensor<512x1024x64xf32>) -> tensor<512x1024x64xf32>\n  return %ret : tensor<512x1024x64xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c64 = arith.constant 64 : index\n  %c512 = arith.constant 512 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c512, %c1024, %c64) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<512x1024x64xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x64xf32>, tensor<1x1x1xf32>, tensor<512x1024x64xf32>) -> (tensor<512x1024x64xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32>_490": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32>\n  return %ret : tensor<1020x252x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x256x256xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x252x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x252x252xf32>\n    memref.copy %2, %alloc : memref<1020x252x252xf32> to memref<1020x252x252xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 252 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x252x252xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x252x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x252x252xf32>\n    return %3 : tensor<1020x252x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x256x256xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x256x256xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x252x252xf32>) -> tensor<1020x252x252xf32>\n  return %ret : tensor<1020x252x252xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c252 = arith.constant 252 : index\n  %c256 = arith.constant 256 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c252, %c252) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x252x252xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x256x256xf32>, tensor<5x5x5xf32>, tensor<1020x252x252xf32>) -> (tensor<1020x252x252xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 252, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32>_491": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32>\n  return %ret : tensor<30x62x1022xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x1024xf32>, %arg1: tensor<3x3x3xf32>, %arg2: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<32x64x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<30x62x1022xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<30x62x1022xf32>\n    memref.copy %2, %alloc : memref<30x62x1022xf32> to memref<30x62x1022xf32>\n    affine.for %arg3 = 0 to 30 {\n      affine.for %arg4 = 0 to 62 {\n        affine.for %arg5 = 0 to 1022 {\n          affine.for %arg6 = 0 to 3 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<32x64x1024xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<3x3x3xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<30x62x1022xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<30x62x1022xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<30x62x1022xf32>\n    return %3 : tensor<30x62x1022xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<32x64x1024xf32>, %filter: tensor<3x3x3xf32>, %output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<32x64x1024xf32>, tensor<3x3x3xf32>) outs(%output: tensor<30x62x1022xf32>) -> tensor<30x62x1022xf32>\n  return %ret : tensor<30x62x1022xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c3 = arith.constant 3 : index\n  %c30 = arith.constant 30 : index\n  %c32 = arith.constant 32 : index\n  %c62 = arith.constant 62 : index\n  %c64 = arith.constant 64 : index\n  %c1022 = arith.constant 1022 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c32, %c64, %c1024) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<32x64x1024xf32>\n  %filter_temp = bufferization.alloc_tensor(%c3, %c3, %c3) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<3x3x3xf32>\n  %output_temp = bufferization.alloc_tensor(%c30, %c62, %c1022) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<30x62x1022xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<32x64x1024xf32>, tensor<3x3x3xf32>, tensor<30x62x1022xf32>) -> (tensor<30x62x1022xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 30, 1], ["%arg4", 0, 62, 1], ["%arg5", 0, 1022, 1], ["%arg6", 0, 3, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32>_492": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32>\n  return %ret : tensor<506x58x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x64x32xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x58x26xf32>) -> tensor<506x58x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x64x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x58x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x58x26xf32>\n    memref.copy %2, %alloc : memref<506x58x26xf32> to memref<506x58x26xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 58 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x64x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x58x26xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x58x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x58x26xf32>\n    return %3 : tensor<506x58x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x64x32xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x64x32xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x58x26xf32>) -> tensor<506x58x26xf32>\n  return %ret : tensor<506x58x26xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c26 = arith.constant 26 : index\n  %c32 = arith.constant 32 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c64, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x64x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c58, %c26) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x58x26xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x64x32xf32>, tensor<7x7x7xf32>, tensor<506x58x26xf32>) -> (tensor<506x58x26xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 58, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>_493": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>\n  return %ret : tensor<508x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x128x32xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x128x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<508x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<508x124x28xf32>\n    memref.copy %2, %alloc : memref<508x124x28xf32> to memref<508x124x28xf32>\n    affine.for %arg3 = 0 to 508 {\n      affine.for %arg4 = 0 to 124 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x128x32xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<508x124x28xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<508x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<508x124x28xf32>\n    return %3 : tensor<508x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x128x32xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x128x32xf32>, tensor<5x5x5xf32>) outs(%output: tensor<508x124x28xf32>) -> tensor<508x124x28xf32>\n  return %ret : tensor<508x124x28xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c128, %c32) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x128x32xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c508, %c124, %c28) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<508x124x28xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x128x32xf32>, tensor<5x5x5xf32>, tensor<508x124x28xf32>) -> (tensor<508x124x28xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 508, 1], ["%arg4", 0, 124, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<512x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32>_494": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<512x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<512x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32>\n  return %ret : tensor<506x1018x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1024x512xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1024x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<506x1018x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<506x1018x506xf32>\n    memref.copy %2, %alloc : memref<506x1018x506xf32> to memref<506x1018x506xf32>\n    affine.for %arg3 = 0 to 506 {\n      affine.for %arg4 = 0 to 1018 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<512x1024x512xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<506x1018x506xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<506x1018x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<506x1018x506xf32>\n    return %3 : tensor<506x1018x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<512x1024x512xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<512x1024x512xf32>, tensor<7x7x7xf32>) outs(%output: tensor<506x1018x506xf32>) -> tensor<506x1018x506xf32>\n  return %ret : tensor<506x1018x506xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c506 = arith.constant 506 : index\n  %c512 = arith.constant 512 : index\n  %c1018 = arith.constant 1018 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c512, %c1024, %c512) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<512x1024x512xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c506, %c1018, %c506) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<506x1018x506xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<512x1024x512xf32>, tensor<7x7x7xf32>, tensor<506x1018x506xf32>) -> (tensor<506x1018x506xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 506, 1], ["%arg4", 0, 1018, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>_495": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x32x64xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x32x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x28x60xf32>\n    memref.copy %2, %alloc : memref<1020x28x60xf32> to memref<1020x28x60xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x32x64xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x28x60xf32>\n    return %3 : tensor<1020x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x32x64xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x32x64xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x28x60xf32>) -> tensor<1020x28x60xf32>\n  return %ret : tensor<1020x28x60xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c28 = arith.constant 28 : index\n  %c32 = arith.constant 32 : index\n  %c60 = arith.constant 60 : index\n  %c64 = arith.constant 64 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c32, %c64) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x32x64xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c28, %c60) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x28x60xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x32x64xf32>, tensor<5x5x5xf32>, tensor<1020x28x60xf32>) -> (tensor<1020x28x60xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>_496": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>\n  return %ret : tensor<1020x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<1024x512x128xf32>, %arg1: tensor<5x5x5xf32>, %arg2: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x5xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1024x512x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1020x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1020x508x124xf32>\n    memref.copy %2, %alloc : memref<1020x508x124xf32> to memref<1020x508x124xf32>\n    affine.for %arg3 = 0 to 1020 {\n      affine.for %arg4 = 0 to 508 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 5 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<1024x512x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<5x5x5xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1020x508x124xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<1020x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1020x508x124xf32>\n    return %3 : tensor<1020x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<1024x512x128xf32>, %filter: tensor<5x5x5xf32>, %output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<1024x512x128xf32>, tensor<5x5x5xf32>) outs(%output: tensor<1020x508x124xf32>) -> tensor<1020x508x124xf32>\n  return %ret : tensor<1020x508x124xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c5 = arith.constant 5 : index\n  %c124 = arith.constant 124 : index\n  %c128 = arith.constant 128 : index\n  %c508 = arith.constant 508 : index\n  %c512 = arith.constant 512 : index\n  %c1020 = arith.constant 1020 : index\n  %c1024 = arith.constant 1024 : index\n  %input_temp = bufferization.alloc_tensor(%c1024, %c512, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<1024x512x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c5, %c5, %c5) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<5x5x5xf32>\n  %output_temp = bufferization.alloc_tensor(%c1020, %c508, %c124) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<1020x508x124xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<1024x512x128xf32>, tensor<5x5x5xf32>, tensor<1020x508x124xf32>) -> (tensor<1020x508x124xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1020, 1], ["%arg4", 0, 508, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 5, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32>_497": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32>\n  return %ret : tensor<256x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x256x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<256x256x256xf32>) -> tensor<256x256x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<256x256x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<256x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x256x256xf32>\n    memref.copy %2, %alloc : memref<256x256x256xf32> to memref<256x256x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<256x256x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<256x256x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<256x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<256x256x256xf32>\n    return %3 : tensor<256x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<256x256x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<256x256x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<256x256x256xf32>) -> tensor<256x256x256xf32>\n  return %ret : tensor<256x256x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c256, %c256, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<256x256x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c256, %c256, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<256x256x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<256x256x256xf32>, tensor<1x1x1xf32>, tensor<256x256x256xf32>) -> (tensor<256x256x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32>_498": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<128x512x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32>\n  return %ret : tensor<128x512x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<128x512x256xf32>, %arg1: tensor<1x1x1xf32>, %arg2: tensor<128x512x256xf32>) -> tensor<128x512x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<128x512x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<128x512x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x512x256xf32>\n    memref.copy %2, %alloc : memref<128x512x256xf32> to memref<128x512x256xf32>\n    affine.for %arg3 = 0 to 128 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 1 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<128x512x256xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<1x1x1xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<128x512x256xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<128x512x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<128x512x256xf32>\n    return %3 : tensor<128x512x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<128x512x256xf32>, %filter: tensor<1x1x1xf32>, %output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<128x512x256xf32>, tensor<1x1x1xf32>) outs(%output: tensor<128x512x256xf32>) -> tensor<128x512x256xf32>\n  return %ret : tensor<128x512x256xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1 = arith.constant 1 : index\n  %c128 = arith.constant 128 : index\n  %c256 = arith.constant 256 : index\n  %c512 = arith.constant 512 : index\n  %input_temp = bufferization.alloc_tensor(%c128, %c512, %c256) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<128x512x256xf32>\n  %filter_temp = bufferization.alloc_tensor(%c1, %c1, %c1) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<1x1x1xf32>\n  %output_temp = bufferization.alloc_tensor(%c128, %c512, %c256) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<128x512x256xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<128x512x256xf32>, tensor<1x1x1xf32>, tensor<128x512x256xf32>) -> (tensor<128x512x256xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 128, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 1, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}, "linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>_499": {"operation": "linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>\n  return %ret : tensor<58x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x128xf32>, %arg1: tensor<7x7x7xf32>, %arg2: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x7xf32>\n    %1 = bufferization.to_memref %arg0 : memref<64x256x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<58x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<58x250x122xf32>\n    memref.copy %2, %alloc : memref<58x250x122xf32> to memref<58x250x122xf32>\n    affine.for %arg3 = 0 to 58 {\n      affine.for %arg4 = 0 to 250 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 7 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %4 = affine.apply #map(%arg3, %arg6)\n                %5 = affine.apply #map(%arg4, %arg7)\n                %6 = affine.apply #map(%arg5, %arg8)\n                %7 = affine.load %1[%4, %5, %6] : memref<64x256x128xf32>\n                %8 = affine.load %0[%arg6, %arg7, %arg8] : memref<7x7x7xf32>\n                %9 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<58x250x122xf32>\n                %10 = arith.mulf %7, %8 : f32\n                %11 = arith.addf %9, %10 : f32\n                affine.store %11, %alloc[%arg3, %arg4, %arg5] : memref<58x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<58x250x122xf32>\n    return %3 : tensor<58x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%input: tensor<64x256x128xf32>, %filter: tensor<7x7x7xf32>, %output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32> {\n  %ret = linalg.conv_3d ins(%input, %filter: tensor<64x256x128xf32>, tensor<7x7x7xf32>) outs(%output: tensor<58x250x122xf32>) -> tensor<58x250x122xf32>\n  return %ret : tensor<58x250x122xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c7 = arith.constant 7 : index\n  %c58 = arith.constant 58 : index\n  %c64 = arith.constant 64 : index\n  %c122 = arith.constant 122 : index\n  %c128 = arith.constant 128 : index\n  %c250 = arith.constant 250 : index\n  %c256 = arith.constant 256 : index\n  %input_temp = bufferization.alloc_tensor(%c64, %c256, %c128) : tensor<?x?x?xf32>\n  %input = tensor.cast %input_temp : tensor<?x?x?xf32> to tensor<64x256x128xf32>\n  %filter_temp = bufferization.alloc_tensor(%c7, %c7, %c7) : tensor<?x?x?xf32>\n  %filter = tensor.cast %filter_temp : tensor<?x?x?xf32> to tensor<7x7x7xf32>\n  %output_temp = bufferization.alloc_tensor(%c58, %c250, %c122) : tensor<?x?x?xf32>\n  %output = tensor.cast %output_temp : tensor<?x?x?xf32> to tensor<58x250x122xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%input, %filter, %output) : (tensor<64x256x128xf32>, tensor<7x7x7xf32>, tensor<58x250x122xf32>) -> (tensor<58x250x122xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 58, 1], ["%arg4", 0, 250, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 7, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3 + %arg6", "%arg4 + %arg7", "%arg5 + %arg8"], ["%arg6", "%arg7", "%arg8"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}}}