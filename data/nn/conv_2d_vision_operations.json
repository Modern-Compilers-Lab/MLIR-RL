[["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<2240x1x1x224xf32>, %7: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n  return %ret : tensor<512x1x1x2240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<2240x1x1x224xf32>, %arg2: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<2240x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x2240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x2240xf32>\n    memref.copy %2, %alloc : memref<512x1x1x2240xf32> to memref<512x1x1x2240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 2240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<2240x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x2240xf32>\n    return %3 : tensor<512x1x1x2240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x2240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<2240x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<2240x1x1x224xf32>) -> tensor<2240x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x2240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<2240x1x1x224xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x2240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x2240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 2240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 912597488}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<12x1x1x48xf32>, %7: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n  return %ret : tensor<512x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<12x1x1x48xf32>, %arg2: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<12x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x12xf32>\n    memref.copy %2, %alloc : memref<512x1x1x12xf32> to memref<512x1x1x12xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<12x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x12xf32>\n    return %3 : tensor<512x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<12x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<12x1x1x48xf32>) -> tensor<12x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<12x1x1x48xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 796958}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x83x83x42xf32>, %3: tensor<42x1x1x42xf32>, %7: tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n  return %ret : tensor<512x83x83x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x83x83x42xf32>, %arg1: tensor<42x1x1x42xf32>, %arg2: tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x83x83x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x83x83x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x83x83x42xf32>\n    memref.copy %2, %alloc : memref<512x83x83x42xf32> to memref<512x83x83x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x83x83x42xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x83x83x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x83x83x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x83x83x42xf32>\n    return %3 : tensor<512x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x83x83x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x42xf32>) -> tensor<42x1x1x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x83x83x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<42x1x1x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19285756128}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x11x11x176xf32>, %3: tensor<5x5x176x1xf32>, %7: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n  return %ret : tensor<512x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x11x11x176xf32>, %arg1: tensor<5x5x176x1xf32>, %arg2: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x11x11x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176x1xf32> to memref<512x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x11x11x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176x1xf32>\n    return %3 : tensor<512x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x11x11x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x11x11x176xf32>) -> tensor<512x11x11x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x176x1xf32>) -> tensor<5x5x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x11x11x176xf32>, tensor<5x5x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 320116897}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x113x113x96xf32>, %3: tensor<3x3x96x1xf32>, %7: tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32>\n  return %ret : tensor<512x56x56x96x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x113x113x96xf32>, %arg1: tensor<3x3x96x1xf32>, %arg2: tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x96x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x113x113x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x96x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x96x1xf32>\n    memref.copy %2, %alloc : memref<512x56x56x96x1xf32> to memref<512x56x56x96x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x113x113x96xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x96x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x96x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x56x56x96x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x96x1xf32>\n    return %3 : tensor<512x56x56x96x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x96x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x113x113x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x113x113x96xf32>) -> tensor<512x113x113x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x96x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x96x1xf32>) -> tensor<3x3x96x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x96x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x113x113x96xf32>, tensor<3x3x96x1xf32>) outs(%7 : tensor<512x56x56x96x1xf32>) -> tensor<512x56x56x96x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x96x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x96x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3591362806}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x480xf32>, %3: tensor<128x1x1x480xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x480xf32>, %arg1: tensor<128x1x1x480xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x480xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x480xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 480 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x480xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x480xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x480xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x480xf32>) -> tensor<512x14x14x480xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x480xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x480xf32>) -> tensor<128x1x1x480xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<128x1x1x480xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 480, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22916801529}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x324xf32>, %3: tensor<1296x1x1x324xf32>, %7: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n  return %ret : tensor<512x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x324xf32>, %arg1: tensor<1296x1x1x324xf32>, %arg2: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1296x1x1x324xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x324xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1296xf32> to memref<512x1x1x1296xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 324 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x324xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1296x1x1x324xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1296xf32>\n    return %3 : tensor<512x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x324xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1296x1x1x324xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1296x1x1x324xf32>) -> tensor<1296x1x1x324xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1296x1x1x324xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 324, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 778655772}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<104x1x1x48xf32>, %7: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n  return %ret : tensor<512x28x28x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<104x1x1x48xf32>, %arg2: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x104xf32>\n    memref.copy %2, %alloc : memref<512x28x28x104xf32> to memref<512x28x28x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x104xf32>\n    return %3 : tensor<512x28x28x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x48xf32>) -> tensor<104x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6266716527}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x34x34x22xf32>, %3: tensor<7x7x22x1xf32>, %7: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n  return %ret : tensor<512x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x34x34x22xf32>, %arg1: tensor<7x7x22x1xf32>, %arg2: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x34x34x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x22x1xf32> to memref<512x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x34x34x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x22x1xf32>\n    return %3 : tensor<512x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x34x34x22xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x34x34x22xf32>) -> tensor<512x34x34x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x22x1xf32>) -> tensor<7x7x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x34x34x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1436258880}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1216xf32>, %3: tensor<128x1x1x1216xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1216xf32>, %arg1: tensor<128x1x1x1216xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1216xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1216xf32>) -> tensor<512x7x7x1216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1216xf32>) -> tensor<128x1x1x1216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<128x1x1x1216xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14660980834}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x448xf32>, %3: tensor<42x1x1x448xf32>, %7: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n  return %ret : tensor<512x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x448xf32>, %arg1: tensor<42x1x1x448xf32>, %arg2: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x42xf32>\n    memref.copy %2, %alloc : memref<512x1x1x42xf32> to memref<512x1x1x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x42xf32>\n    return %3 : tensor<512x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x448xf32>) -> tensor<512x1x1x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x448xf32>) -> tensor<42x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<42x1x1x448xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 35246261}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1824xf32>, %3: tensor<128x1x1x1824xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1824xf32>, %arg1: tensor<128x1x1x1824xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1824xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1824xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1824 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1824xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1824xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1824xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1824xf32>) -> tensor<512x7x7x1824xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1824xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1824xf32>) -> tensor<128x1x1x1824xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<128x1x1x1824xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1824, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22038864106}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x32xf32>, %3: tensor<128x1x1x32xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x32xf32>, %arg1: tensor<128x1x1x32xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4926888}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x864xf32>, %3: tensor<128x1x1x864xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x864xf32>, %arg1: tensor<128x1x1x864xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x864xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x864xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 864 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x864xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x864xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x864xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x864xf32>) -> tensor<512x7x7x864xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x864xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x864xf32>) -> tensor<128x1x1x864xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<128x1x1x864xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 864, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10390123801}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<56x1x1x224xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<56x1x1x224xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x224xf32>) -> tensor<56x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<56x1x1x224xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22795377}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x72xf32>, %3: tensor<216x1x1x72xf32>, %7: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n  return %ret : tensor<512x28x28x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x72xf32>, %arg1: tensor<216x1x1x72xf32>, %arg2: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<216x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x216xf32>\n    memref.copy %2, %alloc : memref<512x28x28x216xf32> to memref<512x28x28x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<216x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x216xf32>\n    return %3 : tensor<512x28x28x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<216x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<216x1x1x72xf32>) -> tensor<216x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<216x1x1x72xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20968013007}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x576xf32>, %3: tensor<160x1x1x576xf32>, %7: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n  return %ret : tensor<512x7x7x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x576xf32>, %arg1: tensor<160x1x1x576xf32>, %arg2: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x160xf32>\n    memref.copy %2, %alloc : memref<512x7x7x160xf32> to memref<512x7x7x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x160xf32>\n    return %3 : tensor<512x7x7x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x576xf32>) -> tensor<512x7x7x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x576xf32>) -> tensor<160x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<160x1x1x576xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8625102690}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<96x1x1x48xf32>, %7: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n  return %ret : tensor<512x28x28x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<96x1x1x48xf32>, %arg2: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x96xf32>\n    memref.copy %2, %alloc : memref<512x28x28x96xf32> to memref<512x28x28x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x96xf32>\n    return %3 : tensor<512x28x28x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x1x1x48xf32>) -> tensor<96x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<96x1x1x48xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5950361559}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x26xf32>, %3: tensor<104x1x1x26xf32>, %7: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n  return %ret : tensor<512x1x1x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x26xf32>, %arg1: tensor<104x1x1x26xf32>, %arg2: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x104xf32>\n    memref.copy %2, %alloc : memref<512x1x1x104xf32> to memref<512x1x1x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x26xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x104xf32>\n    return %3 : tensor<512x1x1x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x26xf32>) -> tensor<104x1x1x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<104x1x1x26xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3168510}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<80x1x1x32xf32>, %7: tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n  return %ret : tensor<512x56x56x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<80x1x1x32xf32>, %arg2: tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x80xf32>\n    memref.copy %2, %alloc : memref<512x56x56x80xf32> to memref<512x56x56x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x80xf32>\n    return %3 : tensor<512x56x56x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x32xf32>) -> tensor<80x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<80x1x1x32xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11588765455}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1088xf32>, %3: tensor<128x1x1x1088xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1088xf32>, %arg1: tensor<128x1x1x1088xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1088xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1088xf32>) -> tensor<128x1x1x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<128x1x1x1088xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 266205950}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x800xf32>, %3: tensor<128x1x1x800xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x800xf32>, %arg1: tensor<128x1x1x800xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x800xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x800xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 800 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x800xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x800xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x800xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x800xf32>) -> tensor<512x7x7x800xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x800xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x800xf32>) -> tensor<128x1x1x800xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<128x1x1x800xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 800, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9611931095}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x152xf32>, %3: tensor<152x1x1x152xf32>, %7: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n  return %ret : tensor<512x14x14x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x152xf32>, %arg1: tensor<152x1x1x152xf32>, %arg2: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x152xf32>\n    memref.copy %2, %alloc : memref<512x14x14x152xf32> to memref<512x14x14x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x152xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x152xf32>\n    return %3 : tensor<512x14x14x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x152xf32>) -> tensor<152x1x1x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<152x1x1x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8294093327}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x87x87x84xf32>, %3: tensor<5x5x84x1xf32>, %7: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n  return %ret : tensor<512x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x87x87x84xf32>, %arg1: tensor<5x5x84x1xf32>, %arg2: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x87x87x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<512x42x42x84x1xf32> to memref<512x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x87x87x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x84x1xf32>\n    return %3 : tensor<512x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x87x87x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x87x87x84xf32>) -> tensor<512x87x87x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x84x1xf32>) -> tensor<5x5x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x87x87x84xf32>, tensor<5x5x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5414070136}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x114x114x32xf32>, %3: tensor<3x3x32x1xf32>, %7: tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32>\n  return %ret : tensor<512x112x112x32x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x114x114x32xf32>, %arg1: tensor<3x3x32x1xf32>, %arg2: tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x32x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x114x114x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x112x112x32x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x112x112x32x1xf32>\n    memref.copy %2, %alloc : memref<512x112x112x32x1xf32> to memref<512x112x112x32x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x114x114x32xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x32x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x112x112x32x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x112x112x32x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x112x112x32x1xf32>\n    return %3 : tensor<512x112x112x32x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x112x112x32x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x114x114x32xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x114x114x32xf32>) -> tensor<512x114x114x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x32x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x32x1xf32>) -> tensor<3x3x32x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x112x112x32x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x114x114x32xf32>, tensor<3x3x32x1xf32>) outs(%7 : tensor<512x112x112x32x1xf32>) -> tensor<512x112x112x32x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x112x112x32x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x112x112x32x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 4748259060}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<64x1x1x32xf32>, %7: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n  return %ret : tensor<512x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<64x1x1x32xf32>, %arg2: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x64xf32>\n    memref.copy %2, %alloc : memref<512x56x56x64xf32> to memref<512x56x56x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x64xf32>\n    return %3 : tensor<512x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x32xf32>) -> tensor<64x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<64x1x1x32xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9174335939}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x216xf32>, %3: tensor<18x1x1x216xf32>, %7: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n  return %ret : tensor<512x1x1x18xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x216xf32>, %arg1: tensor<18x1x1x216xf32>, %arg2: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<18x1x1x216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x18xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x18xf32>\n    memref.copy %2, %alloc : memref<512x1x1x18xf32> to memref<512x1x1x18xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 18 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x216xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<18x1x1x216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x18xf32>\n    return %3 : tensor<512x1x1x18xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x18xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<18x1x1x216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<18x1x1x216xf32>) -> tensor<18x1x1x216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<18x1x1x216xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x18xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x18xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 18, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7032611}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1280xf32>, %3: tensor<128x1x1x1280xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1280xf32>, %arg1: tensor<128x1x1x1280xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1280xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1280 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1280xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1280xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1280xf32>) -> tensor<512x7x7x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1280xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1280xf32>) -> tensor<128x1x1x1280xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<128x1x1x1280xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15437984340}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1376xf32>, %3: tensor<128x1x1x1376xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1376xf32>, %arg1: tensor<128x1x1x1376xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1376xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1376xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1376 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1376xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1376xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1376xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1376xf32>) -> tensor<512x7x7x1376xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1376xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1376xf32>) -> tensor<128x1x1x1376xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<128x1x1x1376xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1376, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16601249155}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x64xf32>, %3: tensor<8x1x1x64xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x64xf32>, %arg1: tensor<8x1x1x64xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x64xf32>) -> tensor<8x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<8x1x1x64xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 743383}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x64xf32>, %3: tensor<160x1x1x64xf32>, %7: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n  return %ret : tensor<512x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x64xf32>, %arg1: tensor<160x1x1x64xf32>, %arg2: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x160xf32>\n    memref.copy %2, %alloc : memref<512x14x14x160xf32> to memref<512x14x14x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x160xf32>\n    return %3 : tensor<512x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x64xf32>) -> tensor<160x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<160x1x1x64xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3436354275}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x528xf32>, %3: tensor<88x1x1x528xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x528xf32>, %arg1: tensor<88x1x1x528xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x528xf32>) -> tensor<512x14x14x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x528xf32>) -> tensor<88x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<88x1x1x528xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17345135595}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1856xf32>, %3: tensor<128x1x1x1856xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1856xf32>, %arg1: tensor<128x1x1x1856xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1856xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1856xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1856 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1856xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1856xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1856xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1856xf32>) -> tensor<512x7x7x1856xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1856xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1856xf32>) -> tensor<128x1x1x1856xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<128x1x1x1856xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1856, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22428779124}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x30xf32>, %3: tensor<336x1x1x30xf32>, %7: tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n  return %ret : tensor<512x1x1x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x30xf32>, %arg1: tensor<336x1x1x30xf32>, %arg2: tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x30xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x30xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x336xf32>\n    memref.copy %2, %alloc : memref<512x1x1x336xf32> to memref<512x1x1x336xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 30 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x30xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x30xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x336xf32>\n    return %3 : tensor<512x1x1x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x30xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x30xf32>) -> tensor<512x1x1x30xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x30xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x30xf32>) -> tensor<336x1x1x30xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<336x1x1x30xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 30, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12119670}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x87x87x42xf32>, %3: tensor<5x5x42x1xf32>, %7: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n  return %ret : tensor<512x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x87x87x42xf32>, %arg1: tensor<5x5x42x1xf32>, %arg2: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x87x87x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<512x83x83x42x1xf32> to memref<512x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x87x87x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x83x83x42x1xf32>\n    return %3 : tensor<512x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x87x87x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x87x87x42xf32>) -> tensor<512x87x87x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x42x1xf32>) -> tensor<5x5x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x87x87x42xf32>, tensor<5x5x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 10398749710}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x56xf32>, %3: tensor<56x1x1x56xf32>, %7: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n  return %ret : tensor<512x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x56xf32>, %arg1: tensor<56x1x1x56xf32>, %arg2: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x56xf32>\n    memref.copy %2, %alloc : memref<512x28x28x56xf32> to memref<512x28x28x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x56xf32>\n    return %3 : tensor<512x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x56xf32>) -> tensor<56x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<56x1x1x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4098350094}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x208xf32>, %3: tensor<26x1x1x208xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x208xf32>, %arg1: tensor<26x1x1x208xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<26x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<26x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<26x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<26x1x1x208xf32>) -> tensor<26x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<26x1x1x208xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9916890}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x42x42x84xf32>, %3: tensor<84x1x1x84xf32>, %7: tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n  return %ret : tensor<512x42x42x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x42x42x84xf32>, %arg1: tensor<84x1x1x84xf32>, %arg2: tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x42x42x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x84xf32>\n    memref.copy %2, %alloc : memref<512x42x42x84xf32> to memref<512x42x42x84xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 84 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x42x42x84xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x42x42x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x42x42x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x84xf32>\n    return %3 : tensor<512x42x42x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x42x42x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x84xf32>) -> tensor<84x1x1x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<84x1x1x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21791693510}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<896x1x1x224xf32>, %7: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n  return %ret : tensor<512x1x1x896xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<896x1x1x224xf32>, %arg2: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<896x1x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x896xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x896xf32>\n    memref.copy %2, %alloc : memref<512x1x1x896xf32> to memref<512x1x1x896xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 896 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<896x1x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x896xf32>\n    return %3 : tensor<512x1x1x896xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x896xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<896x1x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<896x1x1x224xf32>) -> tensor<896x1x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x896xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<896x1x1x224xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x896xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x896xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 896, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 365004127}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x440xf32>, %3: tensor<440x1x1x440xf32>, %7: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n  return %ret : tensor<512x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x440xf32>, %arg1: tensor<440x1x1x440xf32>, %arg2: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x440xf32>\n    memref.copy %2, %alloc : memref<512x7x7x440xf32> to memref<512x7x7x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x440xf32>\n    return %3 : tensor<512x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x440xf32>) -> tensor<440x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<440x1x1x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18012668666}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x57x57x144xf32>, %3: tensor<3x3x144x1xf32>, %7: tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32>\n  return %ret : tensor<512x28x28x144x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x57x57x144xf32>, %arg1: tensor<3x3x144x1xf32>, %arg2: tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x144x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x57x57x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x144x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x144x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x144x1xf32> to memref<512x28x28x144x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x57x57x144xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x144x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x144x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x144x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x144x1xf32>\n    return %3 : tensor<512x28x28x144x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x144x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x57x57x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x57x57x144xf32>) -> tensor<512x57x57x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x144x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x144x1xf32>) -> tensor<3x3x144x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x144x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x57x57x144xf32>, tensor<3x3x144x1xf32>) outs(%7 : tensor<512x28x28x144x1xf32>) -> tensor<512x28x28x144x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x144x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x144x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1342233389}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x112xf32>, %3: tensor<256x1x1x112xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x112xf32>, %arg1: tensor<256x1x1x112xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x112xf32>) -> tensor<512x28x28x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x112xf32>) -> tensor<256x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<256x1x1x112xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10108211188}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x85x85x42xf32>, %3: tensor<3x3x42x1xf32>, %7: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n  return %ret : tensor<512x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x85x85x42xf32>, %arg1: tensor<3x3x42x1xf32>, %arg2: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x85x85x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<512x83x83x42x1xf32> to memref<512x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x85x85x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x83x83x42x1xf32>\n    return %3 : tensor<512x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x85x85x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x85x85x42xf32>) -> tensor<512x85x85x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x42x1xf32>) -> tensor<3x3x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x85x85x42xf32>, tensor<3x3x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3318919131}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<64x1x1x8xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<64x1x1x8xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x8xf32>) -> tensor<64x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<64x1x1x8xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 433110}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x58xf32>, %3: tensor<232x1x1x58xf32>, %7: tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n  return %ret : tensor<512x1x1x232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x58xf32>, %arg1: tensor<232x1x1x58xf32>, %arg2: tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<232x1x1x58xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x232xf32> to memref<512x1x1x232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x58xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<232x1x1x58xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x232xf32>\n    return %3 : tensor<512x1x1x232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x58xf32>) -> tensor<512x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<232x1x1x58xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<232x1x1x58xf32>) -> tensor<232x1x1x58xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<232x1x1x58xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20164262}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1512xf32>, %3: tensor<144x1x1x1512xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1512xf32>, %arg1: tensor<144x1x1x1512xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x1512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x1512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x1512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x1512xf32>) -> tensor<144x1x1x1512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<144x1x1x1512xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 417745176}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x26xf32>, %3: tensor<208x1x1x26xf32>, %7: tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n  return %ret : tensor<512x1x1x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x26xf32>, %arg1: tensor<208x1x1x26xf32>, %arg2: tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<208x1x1x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x208xf32>\n    memref.copy %2, %alloc : memref<512x1x1x208xf32> to memref<512x1x1x208xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x26xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<208x1x1x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x208xf32>\n    return %3 : tensor<512x1x1x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<208x1x1x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<208x1x1x26xf32>) -> tensor<208x1x1x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<208x1x1x26xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6352862}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<120x1x1x48xf32>, %7: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n  return %ret : tensor<512x28x28x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<120x1x1x48xf32>, %arg2: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<120x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x120xf32>\n    memref.copy %2, %alloc : memref<512x28x28x120xf32> to memref<512x28x28x120xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<120x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x120xf32>\n    return %3 : tensor<512x28x28x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<120x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<120x1x1x48xf32>) -> tensor<120x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<120x1x1x48xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7230318965}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x89x89x42xf32>, %3: tensor<7x7x42x1xf32>, %7: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n  return %ret : tensor<512x83x83x42x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x89x89x42xf32>, %arg1: tensor<7x7x42x1xf32>, %arg2: tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x42x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x89x89x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x83x83x42x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x83x83x42x1xf32>\n    memref.copy %2, %alloc : memref<512x83x83x42x1xf32> to memref<512x83x83x42x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x89x89x42xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x42x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x83x83x42x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x83x83x42x1xf32>\n    return %3 : tensor<512x83x83x42x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x83x83x42x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x89x89x42xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x89x89x42xf32>) -> tensor<512x89x89x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x42x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x42x1xf32>) -> tensor<7x7x42x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x83x83x42x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x89x89x42xf32>, tensor<7x7x42x1xf32>) outs(%7 : tensor<512x83x83x42x1xf32>) -> tensor<512x83x83x42x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x83x83x42x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x83x83x42x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 23396758796}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x64xf32>, %3: tensor<256x1x1x64xf32>, %7: tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n  return %ret : tensor<512x1x1x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x64xf32>, %arg1: tensor<256x1x1x64xf32>, %arg2: tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x256xf32>\n    memref.copy %2, %alloc : memref<512x1x1x256xf32> to memref<512x1x1x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x256xf32>\n    return %3 : tensor<512x1x1x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x64xf32>) -> tensor<256x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<256x1x1x64xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25266361}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x104xf32>, %3: tensor<26x1x1x104xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x104xf32>, %arg1: tensor<26x1x1x104xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<26x1x1x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x104xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<26x1x1x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<26x1x1x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<26x1x1x104xf32>) -> tensor<26x1x1x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<26x1x1x104xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4892304}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x18xf32>, %3: tensor<216x1x1x18xf32>, %7: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n  return %ret : tensor<512x1x1x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x18xf32>, %arg1: tensor<216x1x1x18xf32>, %arg2: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<216x1x1x18xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x216xf32>\n    memref.copy %2, %alloc : memref<512x1x1x216xf32> to memref<512x1x1x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x18xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<216x1x1x18xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x216xf32>\n    return %3 : tensor<512x1x1x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<216x1x1x18xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<216x1x1x18xf32>) -> tensor<216x1x1x18xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<216x1x1x18xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4297457}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1184xf32>, %3: tensor<128x1x1x1184xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1184xf32>, %arg1: tensor<128x1x1x1184xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1184xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1184xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1184 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1184xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1184xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1184xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1184xf32>) -> tensor<512x7x7x1184xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1184xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1184xf32>) -> tensor<128x1x1x1184xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<128x1x1x1184xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1184, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14270660581}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x128xf32>, %3: tensor<128x1x1x128xf32>, %7: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n  return %ret : tensor<512x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x128xf32>, %arg1: tensor<128x1x1x128xf32>, %arg2: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x128xf32>\n    memref.copy %2, %alloc : memref<512x28x28x128xf32> to memref<512x28x28x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x128xf32>\n    return %3 : tensor<512x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x128xf32>) -> tensor<128x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<128x1x1x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23327690049}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x896xf32>, %3: tensor<128x1x1x896xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x896xf32>, %arg1: tensor<128x1x1x896xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x896xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x896xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 896 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x896xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x896xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x896xf32>) -> tensor<512x7x7x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x896xf32>) -> tensor<128x1x1x896xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<128x1x1x896xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10776475394}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<24x1x1x32xf32>, %7: tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n  return %ret : tensor<512x112x112x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<24x1x1x32xf32>, %arg2: tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<24x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x112x112x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x112x112x24xf32>\n    memref.copy %2, %alloc : memref<512x112x112x24xf32> to memref<512x112x112x24xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<24x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x112x112x24xf32>\n    return %3 : tensor<512x112x112x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x112x112x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<24x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<24x1x1x32xf32>) -> tensor<24x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x112x112x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<24x1x1x32xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x112x112x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x112x112x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13646831742}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x61x61x22xf32>, %3: tensor<7x7x22x1xf32>, %7: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n  return %ret : tensor<512x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x61x61x22xf32>, %arg1: tensor<7x7x22x1xf32>, %arg2: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x61x61x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x22x1xf32> to memref<512x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x61x61x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x22x1xf32>\n    return %3 : tensor<512x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x61x61x22xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x61x61x22xf32>) -> tensor<512x61x61x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x22x1xf32>) -> tensor<7x7x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x61x61x22xf32>, tensor<7x7x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1439289782}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x88xf32>, %3: tensor<88x1x1x88xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x88xf32>, %arg1: tensor<88x1x1x88xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<88x1x1x88xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 88 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x88xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<88x1x1x88xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<88x1x1x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<88x1x1x88xf32>) -> tensor<88x1x1x88xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<88x1x1x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 88, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2670730540}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x256xf32>, %3: tensor<256x1x1x256xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x256xf32>, %arg1: tensor<256x1x1x256xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x1x1x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x256xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x1x1x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x1x1x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x1x1x256xf32>) -> tensor<256x1x1x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<256x1x1x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24085597960}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x72xf32>, %3: tensor<72x1x1x72xf32>, %7: tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n  return %ret : tensor<512x56x56x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x72xf32>, %arg1: tensor<72x1x1x72xf32>, %arg2: tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x72xf32>\n    memref.copy %2, %alloc : memref<512x56x56x72xf32> to memref<512x56x56x72xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x72xf32>\n    return %3 : tensor<512x56x56x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x72xf32>) -> tensor<72x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<72x1x1x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28198555629}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x30x30x22xf32>, %3: tensor<3x3x22x1xf32>, %7: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n  return %ret : tensor<512x28x28x22x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x30x30x22xf32>, %arg1: tensor<3x3x22x1xf32>, %arg2: tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x22x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x30x30x22xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x22x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x22x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x22x1xf32> to memref<512x28x28x22x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 22 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x30x30x22xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x22x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x22x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x22x1xf32>\n    return %3 : tensor<512x28x28x22x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x22x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x30x30x22xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x30x30x22xf32>) -> tensor<512x30x30x22xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x22x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x22x1xf32>) -> tensor<3x3x22x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x22x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x22xf32>, tensor<3x3x22x1xf32>) outs(%7 : tensor<512x28x28x22x1xf32>) -> tensor<512x28x28x22x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x22x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x22x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 22, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 211492136}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1056xf32>, %3: tensor<176x1x1x1056xf32>, %7: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n  return %ret : tensor<512x7x7x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1056xf32>, %arg1: tensor<176x1x1x1056xf32>, %arg2: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<176x1x1x1056xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176xf32> to memref<512x7x7x176xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1056 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1056xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<176x1x1x1056xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176xf32>\n    return %3 : tensor<512x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1056xf32>) -> tensor<512x7x7x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<176x1x1x1056xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<176x1x1x1056xf32>) -> tensor<176x1x1x1056xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<176x1x1x1056xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17490963543}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1392xf32>, %3: tensor<174x1x1x1392xf32>, %7: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n  return %ret : tensor<512x1x1x174xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1392xf32>, %arg1: tensor<174x1x1x1392xf32>, %arg2: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<174x1x1x1392xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1392xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x174xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x174xf32>\n    memref.copy %2, %alloc : memref<512x1x1x174xf32> to memref<512x1x1x174xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 174 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1392 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1392xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<174x1x1x1392xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x174xf32>\n    return %3 : tensor<512x1x1x174xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x174xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1392xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<174x1x1x1392xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<174x1x1x1392xf32>) -> tensor<174x1x1x1392xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x174xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<174x1x1x1392xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x174xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x174xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 174, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1392, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 464042370}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1472xf32>, %3: tensor<128x1x1x1472xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1472xf32>, %arg1: tensor<128x1x1x1472xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1472xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1472xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1472 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1472xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1472xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1472xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1472xf32>) -> tensor<512x7x7x1472xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1472xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1472xf32>) -> tensor<128x1x1x1472xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<128x1x1x1472xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1472, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17766579553}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x264xf32>, %3: tensor<44x1x1x264xf32>, %7: tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n  return %ret : tensor<512x14x14x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x264xf32>, %arg1: tensor<44x1x1x264xf32>, %arg2: tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x264xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x264xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x44xf32>\n    memref.copy %2, %alloc : memref<512x14x14x44xf32> to memref<512x14x14x44xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 264 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x264xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x264xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x44xf32>\n    return %3 : tensor<512x14x14x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x264xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x264xf32>) -> tensor<512x14x14x264xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x264xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x264xf32>) -> tensor<44x1x1x264xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<44x1x1x264xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 264, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4266863919}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1408xf32>, %3: tensor<128x1x1x1408xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1408xf32>, %arg1: tensor<128x1x1x1408xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1408xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1408xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1408 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1408xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1408xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1408xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1408xf32>) -> tensor<512x7x7x1408xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1408xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1408xf32>) -> tensor<128x1x1x1408xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<128x1x1x1408xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1408, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16988834185}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x48x48x84xf32>, %3: tensor<7x7x84x1xf32>, %7: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n  return %ret : tensor<512x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x48x48x84xf32>, %arg1: tensor<7x7x84x1xf32>, %arg2: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x48x48x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<512x42x42x84x1xf32> to memref<512x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x48x48x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x84x1xf32>\n    return %3 : tensor<512x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x48x48x84xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x48x48x84xf32>) -> tensor<512x48x48x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x84x1xf32>) -> tensor<7x7x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x48x48x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 12034875504}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1536xf32>, %3: tensor<128x1x1x1536xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1536xf32>, %arg1: tensor<128x1x1x1536xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1536xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1536xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1536 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1536xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1536xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1536xf32>) -> tensor<512x7x7x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1536xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1536xf32>) -> tensor<128x1x1x1536xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<128x1x1x1536xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18541472548}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x16x16x88xf32>, %3: tensor<3x3x88x1xf32>, %7: tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n  return %ret : tensor<512x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x16x16x88xf32>, %arg1: tensor<3x3x88x1xf32>, %arg2: tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x16x16x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88x1xf32> to memref<512x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x16x16x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88x1xf32>\n    return %3 : tensor<512x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x16x16x88xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x16x16x88xf32>) -> tensor<512x16x16x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x88x1xf32>) -> tensor<3x3x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x88xf32>, tensor<3x3x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 198811019}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x832xf32>, %3: tensor<128x1x1x832xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x832xf32>, %arg1: tensor<128x1x1x832xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x832xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x832xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 832 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x832xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x832xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x832xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x832xf32>) -> tensor<512x7x7x832xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x832xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x832xf32>) -> tensor<128x1x1x832xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<128x1x1x832xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 832, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10000189043}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x18xf32>, %3: tensor<72x1x1x18xf32>, %7: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n  return %ret : tensor<512x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x18xf32>, %arg1: tensor<72x1x1x18xf32>, %arg2: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<72x1x1x18xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x72xf32>\n    memref.copy %2, %alloc : memref<512x1x1x72xf32> to memref<512x1x1x72xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x18xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<72x1x1x18xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x72xf32>\n    return %3 : tensor<512x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<72x1x1x18xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<72x1x1x18xf32>) -> tensor<72x1x1x18xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<72x1x1x18xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1417035}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x56xf32>, %3: tensor<6x1x1x56xf32>, %7: tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n  return %ret : tensor<512x1x1x6xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x56xf32>, %arg1: tensor<6x1x1x56xf32>, %arg2: tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<6x1x1x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x6xf32>\n    memref.copy %2, %alloc : memref<512x1x1x6xf32> to memref<512x1x1x6xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 6 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x56xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<6x1x1x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x6xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x6xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x6xf32>\n    return %3 : tensor<512x1x1x6xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x6xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<6x1x1x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<6x1x1x56xf32>) -> tensor<6x1x1x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x6xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<6x1x1x56xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x6xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x6xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 6, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 491078}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1664xf32>, %3: tensor<128x1x1x1664xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1664xf32>, %arg1: tensor<128x1x1x1664xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1664xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1664xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1664 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1664xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1664xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1664xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1664xf32>) -> tensor<512x7x7x1664xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1664xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1664xf32>) -> tensor<128x1x1x1664xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<128x1x1x1664xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1664, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20102003721}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x8x10x192xf32>, %3: tensor<224x1x3x192xf32>, %7: tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n  return %ret : tensor<512x8x8x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x8x10x192xf32>, %arg1: tensor<224x1x3x192xf32>, %arg2: tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<224x1x3x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x8x10x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x8x8x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x8x8x224xf32>\n    memref.copy %2, %alloc : memref<512x8x8x224xf32> to memref<512x8x8x224xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x8x10x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<224x1x3x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x8x8x224xf32>\n    return %3 : tensor<512x8x8x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x8x8x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x8x10x192xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x8x10x192xf32>) -> tensor<512x8x10x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<224x1x3x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<224x1x3x192xf32>) -> tensor<224x1x3x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x8x8x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<224x1x3x192xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x8x8x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x8x8x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15784155418}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x15x15x672xf32>, %3: tensor<5x5x672x1xf32>, %7: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n  return %ret : tensor<512x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x15x15x672xf32>, %arg1: tensor<5x5x672x1xf32>, %arg2: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x15x15x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<512x11x11x672x1xf32> to memref<512x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x15x15x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x11x11x672x1xf32>\n    return %3 : tensor<512x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x15x15x672xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x15x15x672xf32>) -> tensor<512x15x15x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x672x1xf32>) -> tensor<5x5x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x15x15x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3085561916}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x192xf32>, %3: tensor<48x1x1x192xf32>, %7: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n  return %ret : tensor<512x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x192xf32>, %arg1: tensor<48x1x1x192xf32>, %arg2: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x48xf32>\n    memref.copy %2, %alloc : memref<512x1x1x48xf32> to memref<512x1x1x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x48xf32>\n    return %3 : tensor<512x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x192xf32>) -> tensor<48x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<48x1x1x192xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16662140}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<92x1x1x368xf32>, %7: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n  return %ret : tensor<512x1x1x92xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<92x1x1x368xf32>, %arg2: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<92x1x1x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x92xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x92xf32>\n    memref.copy %2, %alloc : memref<512x1x1x92xf32> to memref<512x1x1x92xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 92 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<92x1x1x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x92xf32>\n    return %3 : tensor<512x1x1x92xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x92xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<92x1x1x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<92x1x1x368xf32>) -> tensor<92x1x1x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x92xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<92x1x1x368xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x92xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x92xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 92, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 63222248}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x36xf32>, %3: tensor<144x1x1x36xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x36xf32>, %arg1: tensor<144x1x1x36xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x36xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x36xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 36 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x36xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x36xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x36xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x36xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x36xf32>) -> tensor<144x1x1x36xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<144x1x1x36xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 36, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6597395}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x96xf32>, %3: tensor<240x1x1x96xf32>, %7: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n  return %ret : tensor<512x14x14x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x96xf32>, %arg1: tensor<240x1x1x96xf32>, %arg2: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<240x1x1x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x240xf32>\n    memref.copy %2, %alloc : memref<512x14x14x240xf32> to memref<512x14x14x240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x96xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<240x1x1x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x240xf32>\n    return %3 : tensor<512x14x14x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<240x1x1x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<240x1x1x96xf32>) -> tensor<240x1x1x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<240x1x1x96xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8012673380}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x352xf32>, %3: tensor<128x1x1x352xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x352xf32>, %arg1: tensor<128x1x1x352xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x352xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x352xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 352 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x352xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x352xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x352xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x352xf32>) -> tensor<512x14x14x352xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x352xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x352xf32>) -> tensor<128x1x1x352xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<128x1x1x352xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 352, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16711630450}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<32x1x1x128xf32>, %7: tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n  return %ret : tensor<512x1x1x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<32x1x1x128xf32>, %arg2: tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x32xf32>\n    memref.copy %2, %alloc : memref<512x1x1x32xf32> to memref<512x1x1x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x32xf32>\n    return %3 : tensor<512x1x1x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x1x1x128xf32>) -> tensor<32x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<32x1x1x128xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7119620}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x120xf32>, %3: tensor<336x1x1x120xf32>, %7: tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n  return %ret : tensor<512x14x14x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x120xf32>, %arg1: tensor<336x1x1x120xf32>, %arg2: tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<336x1x1x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x336xf32>\n    memref.copy %2, %alloc : memref<512x14x14x336xf32> to memref<512x14x14x336xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<336x1x1x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x336xf32>\n    return %3 : tensor<512x14x14x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<336x1x1x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<336x1x1x120xf32>) -> tensor<336x1x1x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<336x1x1x120xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14292278417}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x9x9x960xf32>, %3: tensor<3x3x960x1xf32>, %7: tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n  return %ret : tensor<512x7x7x960x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x9x9x960xf32>, %arg1: tensor<3x3x960x1xf32>, %arg2: tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x960x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x9x9x960xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x960x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x960x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x960x1xf32> to memref<512x7x7x960x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 960 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x9x9x960xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x960x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x960x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x960x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x960x1xf32>\n    return %3 : tensor<512x7x7x960x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x960x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x9x9x960xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x9x9x960xf32>) -> tensor<512x9x9x960xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x960x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x960x1xf32>) -> tensor<3x3x960x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x960x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x960xf32>, tensor<3x3x960x1xf32>) outs(%7 : tensor<512x7x7x960x1xf32>) -> tensor<512x7x7x960x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x960x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x960x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 960, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 541601666}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x168xf32>, %3: tensor<408x1x1x168xf32>, %7: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n  return %ret : tensor<512x14x14x408xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x168xf32>, %arg1: tensor<408x1x1x168xf32>, %arg2: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<408x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x408xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x408xf32>\n    memref.copy %2, %alloc : memref<512x14x14x408xf32> to memref<512x14x14x408xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 408 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<408x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x408xf32>\n    return %3 : tensor<512x14x14x408xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x408xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x168xf32>) -> tensor<512x28x28x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<408x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<408x1x1x168xf32>) -> tensor<408x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x408xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<408x1x1x168xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x408xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x408xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 408, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24784601924}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x149x149x64xf32>, %3: tensor<3x3x64x1xf32>, %7: tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32>\n  return %ret : tensor<512x147x147x64x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x149x149x64xf32>, %arg1: tensor<3x3x64x1xf32>, %arg2: tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x64x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x149x149x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x147x147x64x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x147x147x64x1xf32>\n    memref.copy %2, %alloc : memref<512x147x147x64x1xf32> to memref<512x147x147x64x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 147 {\n        affine.for %arg5 = 0 to 147 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x149x149x64xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x64x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x147x147x64x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x147x147x64x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x147x147x64x1xf32>\n    return %3 : tensor<512x147x147x64x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x147x147x64x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x149x149x64xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x149x149x64xf32>) -> tensor<512x149x149x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x64x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x64x1xf32>) -> tensor<3x3x64x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x147x147x64x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x149x149x64xf32>, tensor<3x3x64x1xf32>) outs(%7 : tensor<512x147x147x64x1xf32>) -> tensor<512x147x147x64x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x147x147x64x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x147x147x64x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 147, 1], ["%arg5", 0, 147, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 16168975017}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<1296x1x1x144xf32>, %7: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n  return %ret : tensor<512x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<1296x1x1x144xf32>, %arg2: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1296x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1296xf32> to memref<512x1x1x1296xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1296x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1296xf32>\n    return %3 : tensor<512x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1296x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1296x1x1x144xf32>) -> tensor<1296x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1296x1x1x144xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 327663569}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<1088x1x1x128xf32>, %7: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n  return %ret : tensor<512x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<1088x1x1x128xf32>, %arg2: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1088x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1088xf32> to memref<512x1x1x1088xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1088x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1088xf32>\n    return %3 : tensor<512x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1088x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1088x1x1x128xf32>) -> tensor<1088x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1088x1x1x128xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 241482119}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1120xf32>, %3: tensor<128x1x1x1120xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1120xf32>, %arg1: tensor<128x1x1x1120xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1120xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1120xf32>) -> tensor<512x7x7x1120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1120xf32>) -> tensor<128x1x1x1120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<128x1x1x1120xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13494347974}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x320xf32>, %3: tensor<80x1x1x320xf32>, %7: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n  return %ret : tensor<512x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x320xf32>, %arg1: tensor<80x1x1x320xf32>, %arg2: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x320xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x80xf32>\n    memref.copy %2, %alloc : memref<512x1x1x80xf32> to memref<512x1x1x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x320xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x320xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x80xf32>\n    return %3 : tensor<512x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x320xf32>) -> tensor<512x1x1x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x320xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x320xf32>) -> tensor<80x1x1x320xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<80x1x1x320xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 47716213}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x58xf32>, %3: tensor<696x1x1x58xf32>, %7: tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n  return %ret : tensor<512x1x1x696xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x58xf32>, %arg1: tensor<696x1x1x58xf32>, %arg2: tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<696x1x1x58xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x696xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x696xf32>\n    memref.copy %2, %alloc : memref<512x1x1x696xf32> to memref<512x1x1x696xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 696 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x58xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<696x1x1x58xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x696xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x696xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x696xf32>\n    return %3 : tensor<512x1x1x696xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x696xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x58xf32>) -> tensor<512x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<696x1x1x58xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<696x1x1x58xf32>) -> tensor<696x1x1x58xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x696xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<696x1x1x58xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x696xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x696xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 696, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 60465504}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<144x1x1x8xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<144x1x1x8xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x8xf32>) -> tensor<144x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<144x1x1x8xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 984088}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<192x1x1x48xf32>, %7: tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n  return %ret : tensor<512x1x1x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<192x1x1x48xf32>, %arg2: tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<192x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x192xf32>\n    memref.copy %2, %alloc : memref<512x1x1x192xf32> to memref<512x1x1x192xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<192x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x192xf32>\n    return %3 : tensor<512x1x1x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<192x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<192x1x1x48xf32>) -> tensor<192x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<192x1x1x48xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13250446}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x112xf32>, %3: tensor<28x1x1x112xf32>, %7: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n  return %ret : tensor<512x1x1x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x112xf32>, %arg1: tensor<28x1x1x112xf32>, %arg2: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<28x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x28xf32>\n    memref.copy %2, %alloc : memref<512x1x1x28xf32> to memref<512x1x1x28xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<28x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x28xf32>\n    return %3 : tensor<512x1x1x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<28x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<28x1x1x112xf32>) -> tensor<28x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x28xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<28x1x1x112xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5495953}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x13x13x672xf32>, %3: tensor<3x3x672x1xf32>, %7: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n  return %ret : tensor<512x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x13x13x672xf32>, %arg1: tensor<3x3x672x1xf32>, %arg2: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x13x13x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<512x11x11x672x1xf32> to memref<512x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x13x13x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x11x11x672x1xf32>\n    return %3 : tensor<512x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x13x13x672xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x13x13x672xf32>) -> tensor<512x13x13x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x672x1xf32>) -> tensor<3x3x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x13x13x672xf32>, tensor<3x3x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 945164591}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x272xf32>, %3: tensor<1088x1x1x272xf32>, %7: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n  return %ret : tensor<512x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x272xf32>, %arg1: tensor<1088x1x1x272xf32>, %arg2: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1088x1x1x272xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x272xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1088xf32> to memref<512x1x1x1088xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 272 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x272xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1088x1x1x272xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1088xf32>\n    return %3 : tensor<512x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x272xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x272xf32>) -> tensor<512x1x1x272xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1088x1x1x272xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1088x1x1x272xf32>) -> tensor<1088x1x1x272xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1088x1x1x272xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 272, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 544523526}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x512xf32>, %3: tensor<128x1x1x512xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x512xf32>, %arg1: tensor<128x1x1x512xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24491304710}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x240xf32>, %3: tensor<528x1x1x240xf32>, %7: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n  return %ret : tensor<512x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x240xf32>, %arg1: tensor<528x1x1x240xf32>, %arg2: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<528x1x1x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x528xf32>\n    memref.copy %2, %alloc : memref<512x7x7x528xf32> to memref<512x7x7x528xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x240xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<528x1x1x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x528xf32>\n    return %3 : tensor<512x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<528x1x1x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<528x1x1x240xf32>) -> tensor<528x1x1x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<528x1x1x240xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11621307145}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x39x39x256xf32>, %3: tensor<3x3x256x1xf32>, %7: tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n  return %ret : tensor<512x37x37x256x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x39x39x256xf32>, %arg1: tensor<3x3x256x1xf32>, %arg2: tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x256x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x39x39x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x37x37x256x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x37x37x256x1xf32>\n    memref.copy %2, %alloc : memref<512x37x37x256x1xf32> to memref<512x37x37x256x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 37 {\n        affine.for %arg5 = 0 to 37 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x39x39x256xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x256x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x256x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x256x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x37x37x256x1xf32>\n    return %3 : tensor<512x37x37x256x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x37x37x256x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x39x39x256xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x39x39x256xf32>) -> tensor<512x39x39x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x256x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x256x1xf32>) -> tensor<3x3x256x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x37x37x256x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x256xf32>, tensor<3x3x256x1xf32>) outs(%7 : tensor<512x37x37x256x1xf32>) -> tensor<512x37x37x256x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x37x37x256x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x37x37x256x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 37, 1], ["%arg5", 0, 37, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 4344316826}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x38xf32>, %3: tensor<368x1x1x38xf32>, %7: tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n  return %ret : tensor<512x1x1x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x38xf32>, %arg1: tensor<368x1x1x38xf32>, %arg2: tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<368x1x1x38xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x38xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x368xf32>\n    memref.copy %2, %alloc : memref<512x1x1x368xf32> to memref<512x1x1x368xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 38 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x38xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<368x1x1x38xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x368xf32>\n    return %3 : tensor<512x1x1x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<368x1x1x38xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<368x1x1x38xf32>) -> tensor<368x1x1x38xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<368x1x1x38xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 38, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17822533}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1760xf32>, %3: tensor<128x1x1x1760xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1760xf32>, %arg1: tensor<128x1x1x1760xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1760xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1760xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1760 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1760xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1760xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1760xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1760xf32>) -> tensor<512x7x7x1760xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1760xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1760xf32>) -> tensor<128x1x1x1760xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<128x1x1x1760xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1760, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21265774443}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<128x1x1x32xf32>, %7: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n  return %ret : tensor<512x56x56x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<128x1x1x32xf32>, %arg2: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x128xf32>\n    memref.copy %2, %alloc : memref<512x56x56x128xf32> to memref<512x56x56x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x128xf32>\n    return %3 : tensor<512x56x56x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x32xf32>) -> tensor<128x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<128x1x1x32xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18152242853}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<1512x1x1x144xf32>, %7: tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n  return %ret : tensor<512x1x1x1512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<1512x1x1x144xf32>, %arg2: tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1512x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1512xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1512xf32> to memref<512x1x1x1512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1512x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1512xf32>\n    return %3 : tensor<512x1x1x1512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1512x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1512x1x1x144xf32>) -> tensor<1512x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1512x1x1x144xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 382425560}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x160xf32>, %3: tensor<160x1x1x160xf32>, %7: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n  return %ret : tensor<512x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x160xf32>, %arg1: tensor<160x1x1x160xf32>, %arg2: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<160x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x160xf32>\n    memref.copy %2, %alloc : memref<512x14x14x160xf32> to memref<512x14x14x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<160x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x160xf32>\n    return %3 : tensor<512x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<160x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<160x1x1x160xf32>) -> tensor<160x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<160x1x1x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9350246073}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x576xf32>, %3: tensor<144x1x1x576xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x576xf32>, %arg1: tensor<144x1x1x576xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<144x1x1x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x576xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<144x1x1x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<144x1x1x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<144x1x1x576xf32>) -> tensor<144x1x1x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<144x1x1x576xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 157070752}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x11xf32>, %3: tensor<11x1x1x11xf32>, %7: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n  return %ret : tensor<512x56x56x11xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x11xf32>, %arg1: tensor<11x1x1x11xf32>, %arg2: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<11x1x1x11xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x11xf32>\n    memref.copy %2, %alloc : memref<512x56x56x11xf32> to memref<512x56x56x11xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 11 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x11xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<11x1x1x11xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x11xf32>\n    return %3 : tensor<512x56x56x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x11xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<11x1x1x11xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<11x1x1x11xf32>) -> tensor<11x1x1x11xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x11xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<11x1x1x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 11, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 489984715}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x25x25x672xf32>, %3: tensor<5x5x672x1xf32>, %7: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n  return %ret : tensor<512x11x11x672x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x25x25x672xf32>, %arg1: tensor<5x5x672x1xf32>, %arg2: tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x672x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x25x25x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x11x11x672x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x11x11x672x1xf32>\n    memref.copy %2, %alloc : memref<512x11x11x672x1xf32> to memref<512x11x11x672x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 11 {\n        affine.for %arg5 = 0 to 11 {\n          affine.for %arg6 = 0 to 672 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x25x25x672xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x672x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x11x11x672x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x11x11x672x1xf32>\n    return %3 : tensor<512x11x11x672x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x11x11x672x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x25x25x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x25x25x672xf32>) -> tensor<512x25x25x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x672x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x672x1xf32>) -> tensor<5x5x672x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x11x11x672x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x25x25x672xf32>, tensor<5x5x672x1xf32>) outs(%7 : tensor<512x11x11x672x1xf32>) -> tensor<512x11x11x672x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x11x11x672x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x11x11x672x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 11, 1], ["%arg5", 0, 11, 1], ["%arg6", 0, 672, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3147322031}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x16x16x384xf32>, %3: tensor<3x3x384x1xf32>, %7: tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32>\n  return %ret : tensor<512x14x14x384x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x16x16x384xf32>, %arg1: tensor<3x3x384x1xf32>, %arg2: tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x384x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x16x16x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x384x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x384x1xf32>\n    memref.copy %2, %alloc : memref<512x14x14x384x1xf32> to memref<512x14x14x384x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x16x16x384xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x384x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x384x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x384x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x384x1xf32>\n    return %3 : tensor<512x14x14x384x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x384x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x16x16x384xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x16x16x384xf32>) -> tensor<512x16x16x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x384x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x384x1xf32>) -> tensor<3x3x384x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x384x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x16x16x384xf32>, tensor<3x3x384x1xf32>) outs(%7 : tensor<512x14x14x384x1xf32>) -> tensor<512x14x14x384x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x384x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x384x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 916669969}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x29x29x192xf32>, %3: tensor<3x3x192x1xf32>, %7: tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n  return %ret : tensor<512x14x14x192x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x29x29x192xf32>, %arg1: tensor<3x3x192x1xf32>, %arg2: tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x192x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x29x29x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x192x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x192x1xf32>\n    memref.copy %2, %alloc : memref<512x14x14x192x1xf32> to memref<512x14x14x192x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x29x29x192xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x192x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x192x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x192x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x192x1xf32>\n    return %3 : tensor<512x14x14x192x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x192x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x29x29x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x29x29x192xf32>) -> tensor<512x29x29x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x192x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x192x1xf32>) -> tensor<3x3x192x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x192x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x29x29x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x14x14x192x1xf32>) -> tensor<512x14x14x192x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x192x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x192x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 469903830}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x25x25x336xf32>, %3: tensor<5x5x336x1xf32>, %7: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n  return %ret : tensor<512x21x21x336x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x25x25x336xf32>, %arg1: tensor<5x5x336x1xf32>, %arg2: tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x336x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x25x25x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x21x21x336x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x21x21x336x1xf32>\n    memref.copy %2, %alloc : memref<512x21x21x336x1xf32> to memref<512x21x21x336x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 21 {\n        affine.for %arg5 = 0 to 21 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x25x25x336xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x336x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x21x21x336x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x21x21x336x1xf32>\n    return %3 : tensor<512x21x21x336x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x21x21x336x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x25x25x336xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x25x25x336xf32>) -> tensor<512x25x25x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x336x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x336x1xf32>) -> tensor<5x5x336x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x21x21x336x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x25x25x336xf32>, tensor<5x5x336x1xf32>) outs(%7 : tensor<512x21x21x336x1xf32>) -> tensor<512x21x21x336x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x21x21x336x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x21x21x336x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 21, 1], ["%arg5", 0, 21, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 5486046415}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x14xf32>, %3: tensor<152x1x1x14xf32>, %7: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n  return %ret : tensor<512x1x1x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x14xf32>, %arg1: tensor<152x1x1x14xf32>, %arg2: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<152x1x1x14xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x152xf32>\n    memref.copy %2, %alloc : memref<512x1x1x152xf32> to memref<512x1x1x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 14 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x14xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<152x1x1x14xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x152xf32>\n    return %3 : tensor<512x1x1x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x14xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x14xf32>) -> tensor<512x1x1x14xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<152x1x1x14xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<152x1x1x14xf32>) -> tensor<152x1x1x14xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<152x1x1x14xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 14, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2062977}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x72xf32>, %3: tensor<8x1x1x72xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x72xf32>, %arg1: tensor<8x1x1x72xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x72xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x72xf32>) -> tensor<8x1x1x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<8x1x1x72xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 871523}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x299x299x3xf32>, %3: tensor<32x3x3x3xf32>, %7: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n  return %ret : tensor<512x149x149x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x299x299x3xf32>, %arg1: tensor<32x3x3x3xf32>, %arg2: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x299x299x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x149x149x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x149x149x32xf32>\n    memref.copy %2, %alloc : memref<512x149x149x32xf32> to memref<512x149x149x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 149 {\n        affine.for %arg5 = 0 to 149 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x299x299x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x149x149x32xf32>\n    return %3 : tensor<512x149x149x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x149x149x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x299x299x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x299x299x3xf32>) -> tensor<512x299x299x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x3xf32>) -> tensor<32x3x3x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x149x149x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x149x149x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x149x149x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 149, 1], ["%arg5", 0, 149, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28978723292}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x224x224x3xf32>, %3: tensor<96x4x4x3xf32>, %7: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n  return %ret : tensor<512x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 4 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x224x224x3xf32>, %arg1: tensor<96x4x4x3xf32>, %arg2: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<96x4x4x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x96xf32>\n    memref.copy %2, %alloc : memref<512x56x56x96xf32> to memref<512x56x56x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 4 {\n              affine.for %arg8 = 0 to 4 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<96x4x4x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x96xf32>\n    return %3 : tensor<512x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x224x224x3xf32>) -> tensor<512x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<96x4x4x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<96x4x4x3xf32>) -> tensor<96x4x4x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<4> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<96x4x4x3xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 4, 1], ["%arg8", 0, 4, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 4 + %arg7", "%arg5 * 4 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24529612924}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x15x15x576xf32>, %3: tensor<3x3x576x1xf32>, %7: tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32>\n  return %ret : tensor<512x7x7x576x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x15x15x576xf32>, %arg1: tensor<3x3x576x1xf32>, %arg2: tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x576x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x15x15x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x576x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x576x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x576x1xf32> to memref<512x7x7x576x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x15x15x576xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x576x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x576x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x576x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x576x1xf32>\n    return %3 : tensor<512x7x7x576x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x576x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x15x15x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x15x15x576xf32>) -> tensor<512x15x15x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x576x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x576x1xf32>) -> tensor<3x3x576x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x576x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x15x15x576xf32>, tensor<3x3x576x1xf32>) outs(%7 : tensor<512x7x7x576x1xf32>) -> tensor<512x7x7x576x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x576x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x576x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 362349734}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x704xf32>, %3: tensor<128x1x1x704xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x704xf32>, %arg1: tensor<128x1x1x704xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x704xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x704xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 704 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x704xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x704xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x704xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x704xf32>) -> tensor<512x7x7x704xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x704xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x704xf32>) -> tensor<128x1x1x704xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<128x1x1x704xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 704, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8451325123}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x16xf32>, %3: tensor<128x1x1x16xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x16xf32>, %arg1: tensor<128x1x1x16xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x16xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x16xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 16 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x16xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x16xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x16xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x16xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x16xf32>) -> tensor<128x1x1x16xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<128x1x1x16xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 16, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2218297}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x308xf32>, %3: tensor<1232x1x1x308xf32>, %7: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n  return %ret : tensor<512x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x308xf32>, %arg1: tensor<1232x1x1x308xf32>, %arg2: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1232x1x1x308xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x308xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1232xf32> to memref<512x1x1x1232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 308 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x308xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1232x1x1x308xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1232xf32>\n    return %3 : tensor<512x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x308xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1232x1x1x308xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1232x1x1x308xf32>) -> tensor<1232x1x1x308xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1232x1x1x308xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 308, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 702456460}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1440xf32>, %3: tensor<128x1x1x1440xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1440xf32>, %arg1: tensor<128x1x1x1440xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1440xf32>) -> tensor<512x7x7x1440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1440xf32>) -> tensor<128x1x1x1440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<128x1x1x1440xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17390854815}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x24xf32>, %3: tensor<56x1x1x24xf32>, %7: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n  return %ret : tensor<512x56x56x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x24xf32>, %arg1: tensor<56x1x1x24xf32>, %arg2: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x56xf32>\n    memref.copy %2, %alloc : memref<512x56x56x56xf32> to memref<512x56x56x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x56xf32>\n    return %3 : tensor<512x56x56x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x24xf32>) -> tensor<56x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5684956034}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<168x1x1x32xf32>, %7: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n  return %ret : tensor<512x56x56x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<168x1x1x32xf32>, %arg2: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x168xf32>\n    memref.copy %2, %alloc : memref<512x56x56x168xf32> to memref<512x56x56x168xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x168xf32>\n    return %3 : tensor<512x56x56x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x32xf32>) -> tensor<168x1x1x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<168x1x1x32xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23985081288}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x39x39x728xf32>, %3: tensor<3x3x728x1xf32>, %7: tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32>\n  return %ret : tensor<512x37x37x728x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x39x39x728xf32>, %arg1: tensor<3x3x728x1xf32>, %arg2: tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x728x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x39x39x728xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x37x37x728x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x37x37x728x1xf32>\n    memref.copy %2, %alloc : memref<512x37x37x728x1xf32> to memref<512x37x37x728x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 37 {\n        affine.for %arg5 = 0 to 37 {\n          affine.for %arg6 = 0 to 728 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x39x39x728xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x728x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x728x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x37x37x728x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x37x37x728x1xf32>\n    return %3 : tensor<512x37x37x728x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x37x37x728x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x39x39x728xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x39x39x728xf32>) -> tensor<512x39x39x728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x728x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x728x1xf32>) -> tensor<3x3x728x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x37x37x728x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x39x39x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x37x37x728x1xf32>) -> tensor<512x37x37x728x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x37x37x728x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x37x37x728x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 37, 1], ["%arg5", 0, 37, 1], ["%arg6", 0, 728, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 11366905232}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1232xf32>, %3: tensor<112x1x1x1232xf32>, %7: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n  return %ret : tensor<512x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1232xf32>, %arg1: tensor<112x1x1x1232xf32>, %arg2: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x1232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x112xf32>\n    memref.copy %2, %alloc : memref<512x1x1x112xf32> to memref<512x1x1x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1232xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x1232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x112xf32>\n    return %3 : tensor<512x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x1232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x1232xf32>) -> tensor<112x1x1x1232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<112x1x1x1232xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 264072347}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x768xf32>, %3: tensor<128x1x1x768xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x768xf32>, %arg1: tensor<128x1x1x768xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x768xf32>) -> tensor<512x7x7x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x768xf32>) -> tensor<128x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<128x1x1x768xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9227458287}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1504xf32>, %3: tensor<128x1x1x1504xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1504xf32>, %arg1: tensor<128x1x1x1504xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1504xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1504xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1504 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1504xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1504xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1504xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1504xf32>) -> tensor<512x7x7x1504xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1504xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1504xf32>) -> tensor<128x1x1x1504xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<128x1x1x1504xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1504, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18164155290}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x89x89x84xf32>, %3: tensor<7x7x84x1xf32>, %7: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n  return %ret : tensor<512x42x42x84x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x89x89x84xf32>, %arg1: tensor<7x7x84x1xf32>, %arg2: tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<7x7x84x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x89x89x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x84x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x84x1xf32>\n    memref.copy %2, %alloc : memref<512x42x42x84x1xf32> to memref<512x42x42x84x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 7 {\n                affine.for %arg9 = 0 to 7 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x89x89x84xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<7x7x84x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x84x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x84x1xf32>\n    return %3 : tensor<512x42x42x84x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x84x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x89x89x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x89x89x84xf32>) -> tensor<512x89x89x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<7x7x84x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<7x7x84x1xf32>) -> tensor<7x7x84x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x84x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x89x89x84xf32>, tensor<7x7x84x1xf32>) outs(%7 : tensor<512x42x42x84x1xf32>) -> tensor<512x42x42x84x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x84x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x84x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 7, 1], ["%arg9", 0, 7, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 12133648993}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<576x1x1x144xf32>, %7: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n  return %ret : tensor<512x1x1x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<576x1x1x144xf32>, %arg2: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<576x1x1x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x576xf32>\n    memref.copy %2, %alloc : memref<512x1x1x576xf32> to memref<512x1x1x576xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<576x1x1x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x576xf32>\n    return %3 : tensor<512x1x1x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<576x1x1x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<576x1x1x144xf32>) -> tensor<576x1x1x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<576x1x1x144xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 146955063}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x24xf32>, %3: tensor<56x1x1x24xf32>, %7: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n  return %ret : tensor<512x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x24xf32>, %arg1: tensor<56x1x1x24xf32>, %arg2: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x56xf32>\n    memref.copy %2, %alloc : memref<512x28x28x56xf32> to memref<512x28x28x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x24xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x56xf32>\n    return %3 : tensor<512x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x24xf32>) -> tensor<56x1x1x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<56x1x1x24xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1425254746}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x512xf32>, %3: tensor<128x1x1x512xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x512xf32>, %arg1: tensor<128x1x1x512xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x512xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x512xf32>) -> tensor<128x1x1x512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<128x1x1x512xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 123590439}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1792xf32>, %3: tensor<128x1x1x1792xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1792xf32>, %arg1: tensor<128x1x1x1792xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1792xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1792xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1792 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1792xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1792xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1792xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1792xf32>) -> tensor<512x7x7x1792xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1792xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1792xf32>) -> tensor<128x1x1x1792xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<128x1x1x1792xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1792, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21658905243}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x168xf32>, %3: tensor<8x1x1x168xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x168xf32>, %arg1: tensor<8x1x1x168xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x168xf32>) -> tensor<8x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<8x1x1x168xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2350810}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x768xf32>, %3: tensor<80x1x1x768xf32>, %7: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n  return %ret : tensor<512x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x768xf32>, %arg1: tensor<80x1x1x768xf32>, %arg2: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<80x1x1x768xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x80xf32>\n    memref.copy %2, %alloc : memref<512x1x1x80xf32> to memref<512x1x1x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x768xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<80x1x1x768xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x80xf32>\n    return %3 : tensor<512x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x768xf32>) -> tensor<512x1x1x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<80x1x1x768xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<80x1x1x768xf32>) -> tensor<80x1x1x768xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<80x1x1x768xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 116986327}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<104x1x1x48xf32>, %7: tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n  return %ret : tensor<512x56x56x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<104x1x1x48xf32>, %arg2: tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<104x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x104xf32>\n    memref.copy %2, %alloc : memref<512x56x56x104xf32> to memref<512x56x56x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<104x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x104xf32>\n    return %3 : tensor<512x56x56x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<104x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<104x1x1x48xf32>) -> tensor<104x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<104x1x1x48xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25065352248}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x672xf32>, %3: tensor<128x1x1x672xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x672xf32>, %arg1: tensor<128x1x1x672xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x672xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 672 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x672xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x672xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x672xf32>) -> tensor<512x7x7x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x672xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x672xf32>) -> tensor<128x1x1x672xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<128x1x1x672xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8060611507}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<52x1x1x440xf32>, %7: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n  return %ret : tensor<512x1x1x52xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<52x1x1x440xf32>, %arg2: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<52x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x52xf32>\n    memref.copy %2, %alloc : memref<512x1x1x52xf32> to memref<512x1x1x52xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 52 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<52x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x52xf32>\n    return %3 : tensor<512x1x1x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<52x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<52x1x1x440xf32>) -> tensor<52x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x52xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<52x1x1x440xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 52, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 42837909}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<38x1x1x368xf32>, %7: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n  return %ret : tensor<512x1x1x38xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<38x1x1x368xf32>, %arg2: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<38x1x1x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x38xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x38xf32>\n    memref.copy %2, %alloc : memref<512x1x1x38xf32> to memref<512x1x1x38xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 38 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<38x1x1x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x38xf32>\n    return %3 : tensor<512x1x1x38xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x38xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<38x1x1x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<38x1x1x368xf32>) -> tensor<38x1x1x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<38x1x1x368xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x38xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x38xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 38, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26022214}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x608xf32>, %3: tensor<64x1x1x608xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x608xf32>, %arg1: tensor<64x1x1x608xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x608xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x608xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x608xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x608xf32>) -> tensor<512x1x1x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x608xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x608xf32>) -> tensor<64x1x1x608xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<64x1x1x608xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 73762580}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x21x21x728xf32>, %3: tensor<3x3x728x1xf32>, %7: tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n  return %ret : tensor<512x19x19x728x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x21x21x728xf32>, %arg1: tensor<3x3x728x1xf32>, %arg2: tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x728x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x21x21x728xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x19x19x728x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x19x19x728x1xf32>\n    memref.copy %2, %alloc : memref<512x19x19x728x1xf32> to memref<512x19x19x728x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 19 {\n        affine.for %arg5 = 0 to 19 {\n          affine.for %arg6 = 0 to 728 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x21x21x728xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x728x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x19x19x728x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x19x19x728x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x19x19x728x1xf32>\n    return %3 : tensor<512x19x19x728x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x19x19x728x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x21x21x728xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x21x21x728xf32>) -> tensor<512x21x21x728xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x728x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x728x1xf32>) -> tensor<3x3x728x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x19x19x728x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x21x21x728xf32>, tensor<3x3x728x1xf32>) outs(%7 : tensor<512x19x19x728x1xf32>) -> tensor<512x19x19x728x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x19x19x728x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x19x19x728x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 19, 1], ["%arg5", 0, 19, 1], ["%arg6", 0, 728, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 3002796100}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<110x1x1x440xf32>, %7: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n  return %ret : tensor<512x1x1x110xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<110x1x1x440xf32>, %arg2: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<110x1x1x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x110xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x110xf32>\n    memref.copy %2, %alloc : memref<512x1x1x110xf32> to memref<512x1x1x110xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 110 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<110x1x1x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x110xf32>\n    return %3 : tensor<512x1x1x110xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x110xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<110x1x1x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<110x1x1x440xf32>) -> tensor<110x1x1x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x110xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<110x1x1x440xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x110xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x110xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 110, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 90661553}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x35x35x192xf32>, %3: tensor<64x1x1x192xf32>, %7: tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n  return %ret : tensor<512x35x35x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x35x35x192xf32>, %arg1: tensor<64x1x1x192xf32>, %arg2: tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x35x35x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x35x35x64xf32>\n    memref.copy %2, %alloc : memref<512x35x35x64xf32> to memref<512x35x35x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x35x35x192xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x35x35x64xf32>\n    return %3 : tensor<512x35x35x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x35x35x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x35x35x192xf32>) -> tensor<512x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x192xf32>) -> tensor<64x1x1x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x35x35x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<64x1x1x192xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x35x35x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x35x35x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27932361893}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x6xf32>, %3: tensor<56x1x1x6xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x6xf32>, %arg1: tensor<56x1x1x6xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<56x1x1x6xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x6xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 6 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x6xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<56x1x1x6xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x6xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<56x1x1x6xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<56x1x1x6xf32>) -> tensor<56x1x1x6xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<56x1x1x6xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 6, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 266486}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x44xf32>, %3: tensor<44x1x1x44xf32>, %7: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n  return %ret : tensor<512x28x28x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x44xf32>, %arg1: tensor<44x1x1x44xf32>, %arg2: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<44x1x1x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x44xf32>\n    memref.copy %2, %alloc : memref<512x28x28x44xf32> to memref<512x28x28x44xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 44 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x44xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<44x1x1x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x44xf32>\n    return %3 : tensor<512x28x28x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<44x1x1x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<44x1x1x44xf32>) -> tensor<44x1x1x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<44x1x1x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 44, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2432297450}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x76x76x128xf32>, %3: tensor<3x3x128x1xf32>, %7: tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32>\n  return %ret : tensor<512x74x74x128x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x76x76x128xf32>, %arg1: tensor<3x3x128x1xf32>, %arg2: tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x128x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x76x76x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x74x74x128x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x74x74x128x1xf32>\n    memref.copy %2, %alloc : memref<512x74x74x128x1xf32> to memref<512x74x74x128x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 74 {\n        affine.for %arg5 = 0 to 74 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x76x76x128xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x128x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x74x74x128x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x74x74x128x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x74x74x128x1xf32>\n    return %3 : tensor<512x74x74x128x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x74x74x128x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x76x76x128xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x76x76x128xf32>) -> tensor<512x76x76x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x128x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x128x1xf32>) -> tensor<3x3x128x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x74x74x128x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x76x76x128xf32>, tensor<3x3x128x1xf32>) outs(%7 : tensor<512x74x74x128x1xf32>) -> tensor<512x74x74x128x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x74x74x128x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x74x74x128x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 74, 1], ["%arg5", 0, 74, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 8469958390}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x336xf32>, %3: tensor<84x1x1x336xf32>, %7: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n  return %ret : tensor<512x1x1x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x336xf32>, %arg1: tensor<84x1x1x336xf32>, %arg2: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<84x1x1x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x84xf32>\n    memref.copy %2, %alloc : memref<512x1x1x84xf32> to memref<512x1x1x84xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x336xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<84x1x1x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x84xf32>\n    return %3 : tensor<512x1x1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<84x1x1x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<84x1x1x336xf32>) -> tensor<84x1x1x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<84x1x1x336xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 52326969}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x9x9x176xf32>, %3: tensor<3x3x176x1xf32>, %7: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n  return %ret : tensor<512x7x7x176x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x9x9x176xf32>, %arg1: tensor<3x3x176x1xf32>, %arg2: tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x176x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x9x9x176xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176x1xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176x1xf32> to memref<512x7x7x176x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x9x9x176xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x176x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x7x7x176x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176x1xf32>\n    return %3 : tensor<512x7x7x176x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x9x9x176xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x9x9x176xf32>) -> tensor<512x9x9x176xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x176x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x176x1xf32>) -> tensor<3x3x176x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x9x9x176xf32>, tensor<3x3x176x1xf32>) outs(%7 : tensor<512x7x7x176x1xf32>) -> tensor<512x7x7x176x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 101257658}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1696xf32>, %3: tensor<128x1x1x1696xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1696xf32>, %arg1: tensor<128x1x1x1696xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1696xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1696xf32>) -> tensor<512x7x7x1696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1696xf32>) -> tensor<128x1x1x1696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<128x1x1x1696xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20488054302}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<48x1x1x8xf32>, %7: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n  return %ret : tensor<512x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<48x1x1x8xf32>, %arg2: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<48x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x48xf32>\n    memref.copy %2, %alloc : memref<512x1x1x48xf32> to memref<512x1x1x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<48x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x48xf32>\n    return %3 : tensor<512x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<48x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<48x1x1x8xf32>) -> tensor<48x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<48x1x1x8xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 329426}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x222xf32>, %3: tensor<888x1x1x222xf32>, %7: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n  return %ret : tensor<512x1x1x888xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x222xf32>, %arg1: tensor<888x1x1x222xf32>, %arg2: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<888x1x1x222xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x222xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x888xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x888xf32>\n    memref.copy %2, %alloc : memref<512x1x1x888xf32> to memref<512x1x1x888xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 888 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 222 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x222xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<888x1x1x222xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x888xf32>\n    return %3 : tensor<512x1x1x888xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x888xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x222xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x222xf32>) -> tensor<512x1x1x222xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<888x1x1x222xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<888x1x1x222xf32>) -> tensor<888x1x1x222xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x888xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<888x1x1x222xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x888xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x888xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 888, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 222, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 358505979}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x46x46x168xf32>, %3: tensor<5x5x168x1xf32>, %7: tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n  return %ret : tensor<512x42x42x168x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x46x46x168xf32>, %arg1: tensor<5x5x168x1xf32>, %arg2: tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x168x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x46x46x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x168x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x168x1xf32>\n    memref.copy %2, %alloc : memref<512x42x42x168x1xf32> to memref<512x42x42x168x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x46x46x168xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x168x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x168x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x42x42x168x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x168x1xf32>\n    return %3 : tensor<512x42x42x168x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x168x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x46x46x168xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x46x46x168xf32>) -> tensor<512x46x46x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x168x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x168x1xf32>) -> tensor<5x5x168x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x168x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x46x46x168xf32>, tensor<5x5x168x1xf32>) outs(%7 : tensor<512x42x42x168x1xf32>) -> tensor<512x42x42x168x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x168x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x168x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 10848168321}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x112xf32>, %3: tensor<1232x1x1x112xf32>, %7: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n  return %ret : tensor<512x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x112xf32>, %arg1: tensor<1232x1x1x112xf32>, %arg2: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1232x1x1x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1232xf32> to memref<512x1x1x1232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x112xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<1232x1x1x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1232xf32>\n    return %3 : tensor<512x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1232x1x1x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1232x1x1x112xf32>) -> tensor<1232x1x1x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1232x1x1x112xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 235390004}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x544xf32>, %3: tensor<128x1x1x544xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x544xf32>, %arg1: tensor<128x1x1x544xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x544xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x544xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 544 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x544xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x544xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x544xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x544xf32>) -> tensor<512x14x14x544xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x544xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x544xf32>) -> tensor<128x1x1x544xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<128x1x1x544xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 544, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26020879384}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x10x8x224xf32>, %3: tensor<256x3x1x224xf32>, %7: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n  return %ret : tensor<512x8x8x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x10x8x224xf32>, %arg1: tensor<256x3x1x224xf32>, %arg2: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<256x3x1x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x10x8x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x8x8x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x8x8x256xf32>\n    memref.copy %2, %alloc : memref<512x8x8x256xf32> to memref<512x8x8x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x10x8x224xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<256x3x1x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x8x8x256xf32>\n    return %3 : tensor<512x8x8x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x8x8x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x10x8x224xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x10x8x224xf32>) -> tensor<512x10x8x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<256x3x1x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<256x3x1x224xf32>) -> tensor<256x3x1x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x8x8x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<256x3x1x224xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x8x8x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x8x8x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21130217112}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1632xf32>, %3: tensor<128x1x1x1632xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1632xf32>, %arg1: tensor<128x1x1x1632xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1632xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1632xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1632 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1632xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1632xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1632xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1632xf32>) -> tensor<512x7x7x1632xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1632xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1632xf32>) -> tensor<128x1x1x1632xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<128x1x1x1632xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1632, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19707134318}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x208xf32>, %3: tensor<440x1x1x208xf32>, %7: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n  return %ret : tensor<512x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x208xf32>, %arg1: tensor<440x1x1x208xf32>, %arg2: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<440x1x1x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x440xf32>\n    memref.copy %2, %alloc : memref<512x7x7x440xf32> to memref<512x7x7x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x208xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<440x1x1x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x440xf32>\n    return %3 : tensor<512x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<440x1x1x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<440x1x1x208xf32>) -> tensor<440x1x1x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<440x1x1x208xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8338338562}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<168x1x1x8xf32>, %7: tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n  return %ret : tensor<512x1x1x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<168x1x1x8xf32>, %arg2: tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<168x1x1x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x168xf32>\n    memref.copy %2, %alloc : memref<512x1x1x168xf32> to memref<512x1x1x168xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<168x1x1x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x168xf32>\n    return %3 : tensor<512x1x1x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<168x1x1x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<168x1x1x8xf32>) -> tensor<168x1x1x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<168x1x1x8xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1122148}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1568xf32>, %3: tensor<128x1x1x1568xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1568xf32>, %arg1: tensor<128x1x1x1568xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x1568xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1568xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1568 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1568xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x1568xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1568xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1568xf32>) -> tensor<512x7x7x1568xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x1568xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x1568xf32>) -> tensor<128x1x1x1568xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<128x1x1x1568xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1568, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18932875019}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<112x1x1x48xf32>, %7: tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n  return %ret : tensor<512x56x56x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<112x1x1x48xf32>, %arg2: tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<112x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x112xf32>\n    memref.copy %2, %alloc : memref<512x56x56x112xf32> to memref<512x56x56x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<112x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x112xf32>\n    return %3 : tensor<512x56x56x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<112x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<112x1x1x48xf32>) -> tensor<112x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<112x1x1x48xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26980838525}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x168xf32>, %3: tensor<42x1x1x168xf32>, %7: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n  return %ret : tensor<512x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x168xf32>, %arg1: tensor<42x1x1x168xf32>, %arg2: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<42x1x1x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x42xf32>\n    memref.copy %2, %alloc : memref<512x1x1x42xf32> to memref<512x1x1x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x168xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<42x1x1x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x42xf32>\n    return %3 : tensor<512x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<42x1x1x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<42x1x1x168xf32>) -> tensor<42x1x1x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<42x1x1x168xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12565645}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x64xf32>, %3: tensor<64x1x1x64xf32>, %7: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n  return %ret : tensor<512x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x64xf32>, %arg1: tensor<64x1x1x64xf32>, %arg2: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<64x1x1x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x64xf32>\n    memref.copy %2, %alloc : memref<512x56x56x64xf32> to memref<512x56x56x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x64xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<64x1x1x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x64xf32>\n    return %3 : tensor<512x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<64x1x1x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<64x1x1x64xf32>) -> tensor<64x1x1x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<64x1x1x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21903592228}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<8x1x1x48xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<8x1x1x48xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<8x1x1x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<8x1x1x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<8x1x1x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<8x1x1x48xf32>) -> tensor<8x1x1x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<8x1x1x48xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 488268}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x224x224x3xf32>, %3: tensor<32x3x3x3xf32>, %7: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n  return %ret : tensor<512x111x111x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x224x224x3xf32>, %arg1: tensor<32x3x3x3xf32>, %arg2: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<32x3x3x3xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x111x111x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x111x111x32xf32>\n    memref.copy %2, %alloc : memref<512x111x111x32xf32> to memref<512x111x111x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 111 {\n        affine.for %arg5 = 0 to 111 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x224x224x3xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<32x3x3x3xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x111x111x32xf32>\n    return %3 : tensor<512x111x111x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x111x111x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x224x224x3xf32>) -> tensor<512x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<32x3x3x3xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<32x3x3x3xf32>) -> tensor<32x3x3x3xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x111x111x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<32x3x3x3xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x111x111x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x111x111x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 111, 1], ["%arg5", 0, 111, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg7", "%arg5 * 2 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16069005745}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x30x30x192xf32>, %3: tensor<3x3x192x1xf32>, %7: tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32>\n  return %ret : tensor<512x28x28x192x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x30x30x192xf32>, %arg1: tensor<3x3x192x1xf32>, %arg2: tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x192x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x30x30x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x192x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x192x1xf32>\n    memref.copy %2, %alloc : memref<512x28x28x192x1xf32> to memref<512x28x28x192x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x30x30x192xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<3x3x192x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x192x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x28x28x192x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x192x1xf32>\n    return %3 : tensor<512x28x28x192x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x192x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x30x30x192xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x30x30x192xf32>) -> tensor<512x30x30x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x192x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x192x1xf32>) -> tensor<3x3x192x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x192x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x30x30x192xf32>, tensor<3x3x192x1xf32>) outs(%7 : tensor<512x28x28x192x1xf32>) -> tensor<512x28x28x192x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x192x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x192x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg8", "%arg5 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 1808581475}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x448xf32>, %3: tensor<128x1x1x448xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x448xf32>, %arg1: tensor<128x1x1x448xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<128x1x1x448xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x448xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<128x1x1x448xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x448xf32>) -> tensor<512x14x14x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<128x1x1x448xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<128x1x1x448xf32>) -> tensor<128x1x1x448xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<128x1x1x448xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21361081003}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x160xf32>, %3: tensor<384x1x1x160xf32>, %7: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n  return %ret : tensor<512x14x14x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x160xf32>, %arg1: tensor<384x1x1x160xf32>, %arg2: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<384x1x1x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x384xf32>\n    memref.copy %2, %alloc : memref<512x14x14x384xf32> to memref<512x14x14x384xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x160xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<384x1x1x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x384xf32>\n    return %3 : tensor<512x14x14x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<384x1x1x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<384x1x1x160xf32>) -> tensor<384x1x1x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<384x1x1x160xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22228367078}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<16x1x1x128xf32>, %7: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n  return %ret : tensor<512x1x1x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<16x1x1x128xf32>, %arg2: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<16x1x1x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x16xf32>\n    memref.copy %2, %alloc : memref<512x1x1x16xf32> to memref<512x1x1x16xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<16x1x1x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x16xf32>\n    return %3 : tensor<512x1x1x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<16x1x1x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<16x1x1x128xf32>) -> tensor<16x1x1x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x16xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<16x1x1x128xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x16xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3556124}], ["linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>", {"operation": "linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x31x31x88xf32>, %3: tensor<5x5x88x1xf32>, %7: tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32> {\n  %ret = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n  return %ret : tensor<512x14x14x88x1xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x31x31x88xf32>, %arg1: tensor<5x5x88x1xf32>, %arg2: tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<5x5x88x1xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x31x31x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88x1xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88x1xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88x1xf32> to memref<512x14x14x88x1xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 5 {\n                affine.for %arg9 = 0 to 5 {\n                  %4 = affine.apply #map(%arg4, %arg8)\n                  %5 = affine.apply #map(%arg5, %arg9)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg6] : memref<512x31x31x88xf32>\n                  %7 = affine.load %0[%arg8, %arg9, %arg6, %arg7] : memref<5x5x88x1xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x88x1xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6, %arg7] : memref<512x14x14x88x1xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88x1xf32>\n    return %3 : tensor<512x14x14x88x1xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88x1xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x31x31x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x31x31x88xf32>) -> tensor<512x31x31x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<5x5x88x1xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<5x5x88x1xf32>) -> tensor<5x5x88x1xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88x1xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.depthwise_conv_2d_nhwc_hwcm {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%1, %3 : tensor<512x31x31x88xf32>, tensor<5x5x88x1xf32>) outs(%7 : tensor<512x14x14x88x1xf32>) -> tensor<512x14x14x88x1xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88x1xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88x1xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 5, 1], ["%arg9", 0, 5, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 * 2 + %arg8", "%arg5 * 2 + %arg9", "%arg6"], ["%arg8", "%arg9", "%arg6", "%arg7"], ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6", "%arg7"]}, "execution_time": 637606565}], ["linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x528xf32>, %3: tensor<528x1x1x528xf32>, %7: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n  return %ret : tensor<512x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x528xf32>, %arg1: tensor<528x1x1x528xf32>, %arg2: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<528x1x1x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x528xf32>\n    memref.copy %2, %alloc : memref<512x7x7x528xf32> to memref<512x7x7x528xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x528xf32>\n                  %7 = affine.load %0[%arg6, %arg7, %arg8, %arg9] : memref<528x1x1x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x528xf32>\n    return %3 : tensor<512x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<528x1x1x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<528x1x1x528xf32>) -> tensor<528x1x1x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_fhwc {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<528x1x1x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg6", "%arg7", "%arg8", "%arg9"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26028296621}]]