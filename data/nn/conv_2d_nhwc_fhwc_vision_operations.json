[["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x2240xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x2240xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<1x1x224x2240xf32>, %7: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x2240xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n  return %ret : tensor<512x1x1x2240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<1x1x224x2240xf32>, %arg2: tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x224x2240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x2240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x2240xf32>\n    memref.copy %2, %alloc : memref<512x1x1x2240xf32> to memref<512x1x1x2240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 2240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x224x2240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x2240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x2240xf32>\n    return %3 : tensor<512x1x1x2240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x2240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x224x2240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x224x2240xf32>) -> tensor<1x1x224x2240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x2240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x2240xf32>) outs(%7 : tensor<512x1x1x2240xf32>) -> tensor<512x1x1x2240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x2240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x2240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 2240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 912597488}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x12xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x12xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<1x1x48x12xf32>, %7: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x12xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n  return %ret : tensor<512x1x1x12xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<1x1x48x12xf32>, %arg2: tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x12xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x12xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x12xf32>\n    memref.copy %2, %alloc : memref<512x1x1x12xf32> to memref<512x1x1x12xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 12 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x12xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x12xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x12xf32>\n    return %3 : tensor<512x1x1x12xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x12xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x12xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x12xf32>) -> tensor<1x1x48x12xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x12xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x12xf32>) outs(%7 : tensor<512x1x1x12xf32>) -> tensor<512x1x1x12xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x12xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x12xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 12, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 796958}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<1x1x42x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<1x1x42x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x83x83x42xf32>, %3: tensor<1x1x42x42xf32>, %7: tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<1x1x42x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n  return %ret : tensor<512x83x83x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x83x83x42xf32>, %arg1: tensor<1x1x42x42xf32>, %arg2: tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x42x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x83x83x42xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x83x83x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x83x83x42xf32>\n    memref.copy %2, %alloc : memref<512x83x83x42xf32> to memref<512x83x83x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 83 {\n        affine.for %arg5 = 0 to 83 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 42 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x83x83x42xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x42x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x83x83x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x83x83x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x83x83x42xf32>\n    return %3 : tensor<512x83x83x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x83x83x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x83x83x42xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x42x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x42x42xf32>) -> tensor<1x1x42x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x83x83x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x83x83x42xf32>, tensor<1x1x42x42xf32>) outs(%7 : tensor<512x83x83x42xf32>) -> tensor<512x83x83x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x83x83x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x83x83x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 83, 1], ["%arg5", 0, 83, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 42, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19285756128}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<1x1x480x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<1x1x480x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x480xf32>, %3: tensor<1x1x480x128xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<1x1x480x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x480xf32>, %arg1: tensor<1x1x480x128xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x480x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x480xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 480 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x480xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x480x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x480xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x480xf32>) -> tensor<512x14x14x480xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x480x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x480x128xf32>) -> tensor<1x1x480x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x480xf32>, tensor<1x1x480x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 480, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22916801529}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1x1x324x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1x1x324x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x324xf32>, %3: tensor<1x1x324x1296xf32>, %7: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1x1x324x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n  return %ret : tensor<512x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x324xf32>, %arg1: tensor<1x1x324x1296xf32>, %arg2: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x324x1296xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x324xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1296xf32> to memref<512x1x1x1296xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 324 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x324xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x324x1296xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1296xf32>\n    return %3 : tensor<512x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x324xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x324xf32>) -> tensor<512x1x1x324xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x324x1296xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x324x1296xf32>) -> tensor<1x1x324x1296xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x324xf32>, tensor<1x1x324x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 324, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 778655772}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<1x1x48x104xf32>, %7: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n  return %ret : tensor<512x28x28x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<1x1x48x104xf32>, %arg2: tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x104xf32>\n    memref.copy %2, %alloc : memref<512x28x28x104xf32> to memref<512x28x28x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x104xf32>\n    return %3 : tensor<512x28x28x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x104xf32>) -> tensor<1x1x48x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x28x28x104xf32>) -> tensor<512x28x28x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6266716527}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<1x1x1216x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<1x1x1216x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1216xf32>, %3: tensor<1x1x1216x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<1x1x1216x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1216xf32>, %arg1: tensor<1x1x1216x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1216x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1216xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1216x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1216xf32>) -> tensor<512x7x7x1216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1216x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1216x128xf32>) -> tensor<1x1x1216x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1216xf32>, tensor<1x1x1216x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14660980834}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<1x1x448x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<1x1x448x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x448xf32>, %3: tensor<1x1x448x42xf32>, %7: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<1x1x448x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n  return %ret : tensor<512x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x448xf32>, %arg1: tensor<1x1x448x42xf32>, %arg2: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x448x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x42xf32>\n    memref.copy %2, %alloc : memref<512x1x1x42xf32> to memref<512x1x1x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x448xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x448x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x42xf32>\n    return %3 : tensor<512x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x448xf32>) -> tensor<512x1x1x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x448x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x448x42xf32>) -> tensor<1x1x448x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x448xf32>, tensor<1x1x448x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 35246261}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<1x1x1824x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<1x1x1824x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1824xf32>, %3: tensor<1x1x1824x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<1x1x1824x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1824xf32>, %arg1: tensor<1x1x1824x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1824x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1824xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1824 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1824xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1824x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1824xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1824xf32>) -> tensor<512x7x7x1824xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1824x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1824x128xf32>) -> tensor<1x1x1824x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1824xf32>, tensor<1x1x1824x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1824, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22038864106}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x32xf32>, %3: tensor<1x1x32x128xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x32xf32>, %arg1: tensor<1x1x32x128xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x128xf32>) -> tensor<1x1x32x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4926888}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<1x1x864x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<1x1x864x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x864xf32>, %3: tensor<1x1x864x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<1x1x864x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x864xf32>, %arg1: tensor<1x1x864x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x864x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x864xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 864 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x864xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x864x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x864xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x864xf32>) -> tensor<512x7x7x864xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x864x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x864x128xf32>) -> tensor<1x1x864x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x864xf32>, tensor<1x1x864x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 864, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10390123801}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<1x1x224x56xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<1x1x224x56xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x224x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x224x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x224x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x224x56xf32>) -> tensor<1x1x224x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22795377}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x216xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x216xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x72xf32>, %3: tensor<1x1x72x216xf32>, %7: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x216xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n  return %ret : tensor<512x28x28x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x72xf32>, %arg1: tensor<1x1x72x216xf32>, %arg2: tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x72x216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x216xf32>\n    memref.copy %2, %alloc : memref<512x28x28x216xf32> to memref<512x28x28x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x72xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x72x216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x216xf32>\n    return %3 : tensor<512x28x28x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x72x216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x72x216xf32>) -> tensor<1x1x72x216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x216xf32>) outs(%7 : tensor<512x28x28x216xf32>) -> tensor<512x28x28x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20968013007}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<1x1x576x160xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<1x1x576x160xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x576xf32>, %3: tensor<1x1x576x160xf32>, %7: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<1x1x576x160xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n  return %ret : tensor<512x7x7x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x576xf32>, %arg1: tensor<1x1x576x160xf32>, %arg2: tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x576x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x160xf32>\n    memref.copy %2, %alloc : memref<512x7x7x160xf32> to memref<512x7x7x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x576xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x576x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x160xf32>\n    return %3 : tensor<512x7x7x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x576xf32>) -> tensor<512x7x7x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x576x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x576x160xf32>) -> tensor<1x1x576x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x576xf32>, tensor<1x1x576x160xf32>) outs(%7 : tensor<512x7x7x160xf32>) -> tensor<512x7x7x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8625102690}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<1x1x48x96xf32>, %7: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n  return %ret : tensor<512x28x28x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<1x1x48x96xf32>, %arg2: tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x96xf32>\n    memref.copy %2, %alloc : memref<512x28x28x96xf32> to memref<512x28x28x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x96xf32>\n    return %3 : tensor<512x28x28x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x96xf32>) -> tensor<1x1x48x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x96xf32>) outs(%7 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5950361559}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x104xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x104xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x26xf32>, %3: tensor<1x1x26x104xf32>, %7: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x104xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n  return %ret : tensor<512x1x1x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x26xf32>, %arg1: tensor<1x1x26x104xf32>, %arg2: tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x26x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x104xf32>\n    memref.copy %2, %alloc : memref<512x1x1x104xf32> to memref<512x1x1x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x26xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x26x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x104xf32>\n    return %3 : tensor<512x1x1x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x26x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x26x104xf32>) -> tensor<1x1x26x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x104xf32>) outs(%7 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3168510}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x80xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x80xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<1x1x32x80xf32>, %7: tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x80xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n  return %ret : tensor<512x56x56x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<1x1x32x80xf32>, %arg2: tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x80xf32>\n    memref.copy %2, %alloc : memref<512x56x56x80xf32> to memref<512x56x56x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x80xf32>\n    return %3 : tensor<512x56x56x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x80xf32>) -> tensor<1x1x32x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x80xf32>) outs(%7 : tensor<512x56x56x80xf32>) -> tensor<512x56x56x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11588765455}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<1x1x1088x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<1x1x1088x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1088xf32>, %3: tensor<1x1x1088x128xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<1x1x1088x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1088xf32>, %arg1: tensor<1x1x1088x128xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1088x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1088 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1088xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1088x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1088x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1088x128xf32>) -> tensor<1x1x1088x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1088xf32>, tensor<1x1x1088x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 266205950}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<1x1x800x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<1x1x800x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x800xf32>, %3: tensor<1x1x800x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<1x1x800x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x800xf32>, %arg1: tensor<1x1x800x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x800x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x800xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 800 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x800xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x800x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x800xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x800xf32>) -> tensor<512x7x7x800xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x800x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x800x128xf32>) -> tensor<1x1x800x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x800xf32>, tensor<1x1x800x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 800, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9611931095}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<1x1x152x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<1x1x152x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x152xf32>, %3: tensor<1x1x152x152xf32>, %7: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<1x1x152x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n  return %ret : tensor<512x14x14x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x152xf32>, %arg1: tensor<1x1x152x152xf32>, %arg2: tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x152x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x152xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x152xf32>\n    memref.copy %2, %alloc : memref<512x14x14x152xf32> to memref<512x14x14x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 152 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x152xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x152x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x152xf32>\n    return %3 : tensor<512x14x14x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x152x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x152x152xf32>) -> tensor<1x1x152x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x152xf32>, tensor<1x1x152x152xf32>) outs(%7 : tensor<512x14x14x152xf32>) -> tensor<512x14x14x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 152, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8294093327}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<1x1x32x64xf32>, %7: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n  return %ret : tensor<512x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<1x1x32x64xf32>, %arg2: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x64xf32>\n    memref.copy %2, %alloc : memref<512x56x56x64xf32> to memref<512x56x56x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x64xf32>\n    return %3 : tensor<512x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x64xf32>) -> tensor<1x1x32x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9174335939}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<1x1x216x18xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<1x1x216x18xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x216xf32>, %3: tensor<1x1x216x18xf32>, %7: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<1x1x216x18xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n  return %ret : tensor<512x1x1x18xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x216xf32>, %arg1: tensor<1x1x216x18xf32>, %arg2: tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x216x18xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x216xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x18xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x18xf32>\n    memref.copy %2, %alloc : memref<512x1x1x18xf32> to memref<512x1x1x18xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 18 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 216 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x216xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x216x18xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x18xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x18xf32>\n    return %3 : tensor<512x1x1x18xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x18xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x216x18xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x216x18xf32>) -> tensor<1x1x216x18xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x216xf32>, tensor<1x1x216x18xf32>) outs(%7 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x18xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x18xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 18, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 216, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7032611}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<1x1x1280x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<1x1x1280x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1280xf32>, %3: tensor<1x1x1280x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<1x1x1280x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1280xf32>, %arg1: tensor<1x1x1280x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1280x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1280 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1280xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1280x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1280xf32>) -> tensor<512x7x7x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1280x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1280x128xf32>) -> tensor<1x1x1280x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1280xf32>, tensor<1x1x1280x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15437984340}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<1x1x1376x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<1x1x1376x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1376xf32>, %3: tensor<1x1x1376x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<1x1x1376x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1376xf32>, %arg1: tensor<1x1x1376x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1376x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1376xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1376 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1376xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1376x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1376xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1376xf32>) -> tensor<512x7x7x1376xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1376x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1376x128xf32>) -> tensor<1x1x1376x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1376xf32>, tensor<1x1x1376x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1376, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16601249155}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x64xf32>, %3: tensor<1x1x64x8xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x64xf32>, %arg1: tensor<1x1x64x8xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x64x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x64xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x64x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x64x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x64x8xf32>) -> tensor<1x1x64x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 743383}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<1x1x64x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<1x1x64x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x64xf32>, %3: tensor<1x1x64x160xf32>, %7: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<1x1x64x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n  return %ret : tensor<512x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x64xf32>, %arg1: tensor<1x1x64x160xf32>, %arg2: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x64x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x160xf32>\n    memref.copy %2, %alloc : memref<512x14x14x160xf32> to memref<512x14x14x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x64xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x64x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x160xf32>\n    return %3 : tensor<512x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x64xf32>) -> tensor<512x28x28x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x64x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x64x160xf32>) -> tensor<1x1x64x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x64xf32>, tensor<1x1x64x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3436354275}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<1x1x528x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<1x1x528x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x528xf32>, %3: tensor<1x1x528x88xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<1x1x528x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x528xf32>, %arg1: tensor<1x1x528x88xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x528x88xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x528xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x528x88xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x528xf32>) -> tensor<512x14x14x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x528x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x528x88xf32>) -> tensor<1x1x528x88xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x528xf32>, tensor<1x1x528x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17345135595}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<1x1x1856x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<1x1x1856x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1856xf32>, %3: tensor<1x1x1856x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<1x1x1856x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1856xf32>, %arg1: tensor<1x1x1856x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1856x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1856xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1856 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1856xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1856x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1856xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1856xf32>) -> tensor<512x7x7x1856xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1856x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1856x128xf32>) -> tensor<1x1x1856x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1856xf32>, tensor<1x1x1856x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1856, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22428779124}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<1x1x30x336xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<1x1x30x336xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x30xf32>, %3: tensor<1x1x30x336xf32>, %7: tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<1x1x30x336xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n  return %ret : tensor<512x1x1x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x30xf32>, %arg1: tensor<1x1x30x336xf32>, %arg2: tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x30x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x30xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x336xf32>\n    memref.copy %2, %alloc : memref<512x1x1x336xf32> to memref<512x1x1x336xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 30 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x30xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x30x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x336xf32>\n    return %3 : tensor<512x1x1x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x30xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x30xf32>) -> tensor<512x1x1x30xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x30x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x30x336xf32>) -> tensor<1x1x30x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x30xf32>, tensor<1x1x30x336xf32>) outs(%7 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 30, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12119670}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<1x1x56x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<1x1x56x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x56xf32>, %3: tensor<1x1x56x56xf32>, %7: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<1x1x56x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n  return %ret : tensor<512x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x56xf32>, %arg1: tensor<1x1x56x56xf32>, %arg2: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x56x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x56xf32>\n    memref.copy %2, %alloc : memref<512x28x28x56xf32> to memref<512x28x28x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x56xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x56x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x56xf32>\n    return %3 : tensor<512x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x56x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x56x56xf32>) -> tensor<1x1x56x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x56xf32>, tensor<1x1x56x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4098350094}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<1x1x208x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<1x1x208x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x208xf32>, %3: tensor<1x1x208x26xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<1x1x208x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x208xf32>, %arg1: tensor<1x1x208x26xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x208x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x208xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x208x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x208x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x208x26xf32>) -> tensor<1x1x208x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x208xf32>, tensor<1x1x208x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9916890}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<1x1x84x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<1x1x84x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x42x42x84xf32>, %3: tensor<1x1x84x84xf32>, %7: tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<1x1x84x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n  return %ret : tensor<512x42x42x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x42x42x84xf32>, %arg1: tensor<1x1x84x84xf32>, %arg2: tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x84x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x42x42x84xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x42x42x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x42x42x84xf32>\n    memref.copy %2, %alloc : memref<512x42x42x84xf32> to memref<512x42x42x84xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 42 {\n        affine.for %arg5 = 0 to 42 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 84 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x42x42x84xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x84x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x42x42x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x42x42x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x42x42x84xf32>\n    return %3 : tensor<512x42x42x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x42x42x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x42x42x84xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x84x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x84x84xf32>) -> tensor<1x1x84x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x42x42x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x42x42x84xf32>, tensor<1x1x84x84xf32>) outs(%7 : tensor<512x42x42x84xf32>) -> tensor<512x42x42x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x42x42x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x42x42x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 42, 1], ["%arg5", 0, 42, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 84, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21791693510}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x896xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x896xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x224xf32>, %3: tensor<1x1x224x896xf32>, %7: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x896xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n  return %ret : tensor<512x1x1x896xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x224xf32>, %arg1: tensor<1x1x224x896xf32>, %arg2: tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x224x896xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x896xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x896xf32>\n    memref.copy %2, %alloc : memref<512x1x1x896xf32> to memref<512x1x1x896xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 896 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x224xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x224x896xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x896xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x896xf32>\n    return %3 : tensor<512x1x1x896xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x896xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x224xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x224xf32>) -> tensor<512x1x1x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x224x896xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x224x896xf32>) -> tensor<1x1x224x896xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x896xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x224xf32>, tensor<1x1x224x896xf32>) outs(%7 : tensor<512x1x1x896xf32>) -> tensor<512x1x1x896xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x896xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x896xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 896, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 365004127}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<1x1x440x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<1x1x440x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x440xf32>, %3: tensor<1x1x440x440xf32>, %7: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<1x1x440x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n  return %ret : tensor<512x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x440xf32>, %arg1: tensor<1x1x440x440xf32>, %arg2: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x440x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x440xf32>\n    memref.copy %2, %alloc : memref<512x7x7x440xf32> to memref<512x7x7x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x440xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x440x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x440xf32>\n    return %3 : tensor<512x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x440x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x440x440xf32>) -> tensor<1x1x440x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x440xf32>, tensor<1x1x440x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18012668666}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<1x1x112x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<1x1x112x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x112xf32>, %3: tensor<1x1x112x256xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<1x1x112x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x112xf32>, %arg1: tensor<1x1x112x256xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x112x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x112xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x112x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x112xf32>) -> tensor<512x28x28x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x112x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x112x256xf32>) -> tensor<1x1x112x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x112xf32>, tensor<1x1x112x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10108211188}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<1x1x8x64xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<1x1x8x64xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x8x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x8x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x8x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x8x64xf32>) -> tensor<1x1x8x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 433110}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x232xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x232xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x58xf32>, %3: tensor<1x1x58x232xf32>, %7: tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x232xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n  return %ret : tensor<512x1x1x232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x58xf32>, %arg1: tensor<1x1x58x232xf32>, %arg2: tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x58x232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x232xf32> to memref<512x1x1x232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x58xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x58x232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x232xf32>\n    return %3 : tensor<512x1x1x232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x58xf32>) -> tensor<512x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x58x232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x58x232xf32>) -> tensor<1x1x58x232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x232xf32>) outs(%7 : tensor<512x1x1x232xf32>) -> tensor<512x1x1x232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20164262}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<1x1x1512x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<1x1x1512x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1512xf32>, %3: tensor<1x1x1512x144xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<1x1x1512x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1512xf32>, %arg1: tensor<1x1x1512x144xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1512x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1512xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1512x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1512x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1512x144xf32>) -> tensor<1x1x1512x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1512xf32>, tensor<1x1x1512x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 417745176}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x208xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x208xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x26xf32>, %3: tensor<1x1x26x208xf32>, %7: tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x208xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n  return %ret : tensor<512x1x1x208xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x26xf32>, %arg1: tensor<1x1x26x208xf32>, %arg2: tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x26x208xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x26xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x208xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x208xf32>\n    memref.copy %2, %alloc : memref<512x1x1x208xf32> to memref<512x1x1x208xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 208 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 26 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x26xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x26x208xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x208xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x208xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x208xf32>\n    return %3 : tensor<512x1x1x208xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x208xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x26x208xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x26x208xf32>) -> tensor<1x1x26x208xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x208xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x26xf32>, tensor<1x1x26x208xf32>) outs(%7 : tensor<512x1x1x208xf32>) -> tensor<512x1x1x208xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x208xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x208xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 208, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 26, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6352862}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x120xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x120xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<1x1x48x120xf32>, %7: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x120xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n  return %ret : tensor<512x28x28x120xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<1x1x48x120xf32>, %arg2: tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x120xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x120xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x120xf32>\n    memref.copy %2, %alloc : memref<512x28x28x120xf32> to memref<512x28x28x120xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 120 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x120xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x120xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x120xf32>\n    return %3 : tensor<512x28x28x120xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x120xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x120xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x120xf32>) -> tensor<1x1x48x120xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x120xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x120xf32>) outs(%7 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x120xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x120xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 120, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7230318965}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x256xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x256xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x64xf32>, %3: tensor<1x1x64x256xf32>, %7: tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x256xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n  return %ret : tensor<512x1x1x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x64xf32>, %arg1: tensor<1x1x64x256xf32>, %arg2: tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x64x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x256xf32>\n    memref.copy %2, %alloc : memref<512x1x1x256xf32> to memref<512x1x1x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x64xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x64x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x256xf32>\n    return %3 : tensor<512x1x1x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x64x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x64x256xf32>) -> tensor<1x1x64x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x64xf32>, tensor<1x1x64x256xf32>) outs(%7 : tensor<512x1x1x256xf32>) -> tensor<512x1x1x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25266361}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<1x1x104x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<1x1x104x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x104xf32>, %3: tensor<1x1x104x26xf32>, %7: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<1x1x104x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n  return %ret : tensor<512x1x1x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x104xf32>, %arg1: tensor<1x1x104x26xf32>, %arg2: tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x104x26xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x104xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x26xf32>\n    memref.copy %2, %alloc : memref<512x1x1x26xf32> to memref<512x1x1x26xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 104 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x104xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x104x26xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x26xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x26xf32>\n    return %3 : tensor<512x1x1x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x104xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x104xf32>) -> tensor<512x1x1x104xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x104x26xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x104x26xf32>) -> tensor<1x1x104x26xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x26xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x104xf32>, tensor<1x1x104x26xf32>) outs(%7 : tensor<512x1x1x26xf32>) -> tensor<512x1x1x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x26xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 104, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4892304}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x216xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x216xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x18xf32>, %3: tensor<1x1x18x216xf32>, %7: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x216xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n  return %ret : tensor<512x1x1x216xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x18xf32>, %arg1: tensor<1x1x18x216xf32>, %arg2: tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x18x216xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x216xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x216xf32>\n    memref.copy %2, %alloc : memref<512x1x1x216xf32> to memref<512x1x1x216xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 216 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x18xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x18x216xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x216xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x216xf32>\n    return %3 : tensor<512x1x1x216xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x216xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x18x216xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x18x216xf32>) -> tensor<1x1x18x216xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x216xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x216xf32>) outs(%7 : tensor<512x1x1x216xf32>) -> tensor<512x1x1x216xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x216xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x216xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 216, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4297457}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<1x1x1184x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<1x1x1184x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1184xf32>, %3: tensor<1x1x1184x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<1x1x1184x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1184xf32>, %arg1: tensor<1x1x1184x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1184x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1184xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1184 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1184xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1184x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1184xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1184xf32>) -> tensor<512x7x7x1184xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1184x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1184x128xf32>) -> tensor<1x1x1184x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1184xf32>, tensor<1x1x1184x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1184, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14270660581}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<1x1x128x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<1x1x128x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x128xf32>, %3: tensor<1x1x128x128xf32>, %7: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<1x1x128x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n  return %ret : tensor<512x28x28x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x128xf32>, %arg1: tensor<1x1x128x128xf32>, %arg2: tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x128x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x128xf32>\n    memref.copy %2, %alloc : memref<512x28x28x128xf32> to memref<512x28x28x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x128xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x128x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x128xf32>\n    return %3 : tensor<512x28x28x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x128x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x128x128xf32>) -> tensor<1x1x128x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x128xf32>, tensor<1x1x128x128xf32>) outs(%7 : tensor<512x28x28x128xf32>) -> tensor<512x28x28x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23327690049}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<1x1x896x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<1x1x896x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x896xf32>, %3: tensor<1x1x896x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<1x1x896x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x896xf32>, %arg1: tensor<1x1x896x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x896x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x896xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 896 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x896xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x896x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x896xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x896xf32>) -> tensor<512x7x7x896xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x896x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x896x128xf32>) -> tensor<1x1x896x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x896xf32>, tensor<1x1x896x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 896, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10776475394}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x24xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x24xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<1x1x32x24xf32>, %7: tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x24xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n  return %ret : tensor<512x112x112x24xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<1x1x32x24xf32>, %arg2: tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x24xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x112x112x24xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x112x112x24xf32>\n    memref.copy %2, %alloc : memref<512x112x112x24xf32> to memref<512x112x112x24xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 112 {\n        affine.for %arg5 = 0 to 112 {\n          affine.for %arg6 = 0 to 24 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x24xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x24xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x112x112x24xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x112x112x24xf32>\n    return %3 : tensor<512x112x112x24xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x112x112x24xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x24xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x24xf32>) -> tensor<1x1x32x24xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x112x112x24xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x24xf32>) outs(%7 : tensor<512x112x112x24xf32>) -> tensor<512x112x112x24xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x112x112x24xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x112x112x24xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 112, 1], ["%arg5", 0, 112, 1], ["%arg6", 0, 24, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13646831742}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<1x1x88x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<1x1x88x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x88xf32>, %3: tensor<1x1x88x88xf32>, %7: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<1x1x88x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n  return %ret : tensor<512x14x14x88xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x88xf32>, %arg1: tensor<1x1x88x88xf32>, %arg2: tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x88x88xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x88xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x88xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x88xf32>\n    memref.copy %2, %alloc : memref<512x14x14x88xf32> to memref<512x14x14x88xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 88 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 88 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x88xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x88x88xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x88xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x88xf32>\n    return %3 : tensor<512x14x14x88xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x88xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x88x88xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x88x88xf32>) -> tensor<1x1x88x88xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x88xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x88xf32>, tensor<1x1x88x88xf32>) outs(%7 : tensor<512x14x14x88xf32>) -> tensor<512x14x14x88xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x88xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x88xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 88, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 88, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2670730540}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<1x1x256x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<1x1x256x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x256xf32>, %3: tensor<1x1x256x256xf32>, %7: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<1x1x256x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n  return %ret : tensor<512x14x14x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x256xf32>, %arg1: tensor<1x1x256x256xf32>, %arg2: tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x256x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x256xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x256xf32>\n    memref.copy %2, %alloc : memref<512x14x14x256xf32> to memref<512x14x14x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 256 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x256xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x256x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x256xf32>\n    return %3 : tensor<512x14x14x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x256x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x256x256xf32>) -> tensor<1x1x256x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x256xf32>, tensor<1x1x256x256xf32>) outs(%7 : tensor<512x14x14x256xf32>) -> tensor<512x14x14x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 256, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24085597960}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x72xf32>, %3: tensor<1x1x72x72xf32>, %7: tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n  return %ret : tensor<512x56x56x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x72xf32>, %arg1: tensor<1x1x72x72xf32>, %arg2: tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x72x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x72xf32>\n    memref.copy %2, %alloc : memref<512x56x56x72xf32> to memref<512x56x56x72xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x72xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x72x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x72xf32>\n    return %3 : tensor<512x56x56x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x72x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x72x72xf32>) -> tensor<1x1x72x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x72xf32>, tensor<1x1x72x72xf32>) outs(%7 : tensor<512x56x56x72xf32>) -> tensor<512x56x56x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28198555629}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<1x1x1056x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<1x1x1056x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1056xf32>, %3: tensor<1x1x1056x176xf32>, %7: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<1x1x1056x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n  return %ret : tensor<512x7x7x176xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1056xf32>, %arg1: tensor<1x1x1056x176xf32>, %arg2: tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1056x176xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x176xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x176xf32>\n    memref.copy %2, %alloc : memref<512x7x7x176xf32> to memref<512x7x7x176xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 176 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1056 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1056xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1056x176xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x176xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x176xf32>\n    return %3 : tensor<512x7x7x176xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x176xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1056xf32>) -> tensor<512x7x7x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1056x176xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1056x176xf32>) -> tensor<1x1x1056x176xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x176xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1056xf32>, tensor<1x1x1056x176xf32>) outs(%7 : tensor<512x7x7x176xf32>) -> tensor<512x7x7x176xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x176xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x176xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 176, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17490963543}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<1x1x1392x174xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<1x1x1392x174xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1392xf32>, %3: tensor<1x1x1392x174xf32>, %7: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<1x1x1392x174xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n  return %ret : tensor<512x1x1x174xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1392xf32>, %arg1: tensor<1x1x1392x174xf32>, %arg2: tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1392x174xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1392xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x174xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x174xf32>\n    memref.copy %2, %alloc : memref<512x1x1x174xf32> to memref<512x1x1x174xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 174 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1392 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1392xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1392x174xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x174xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x174xf32>\n    return %3 : tensor<512x1x1x174xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x174xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1392xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1392xf32>) -> tensor<512x1x1x1392xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1392x174xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1392x174xf32>) -> tensor<1x1x1392x174xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x174xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1392xf32>, tensor<1x1x1392x174xf32>) outs(%7 : tensor<512x1x1x174xf32>) -> tensor<512x1x1x174xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x174xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x174xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 174, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1392, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 464042370}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<1x1x1472x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<1x1x1472x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1472xf32>, %3: tensor<1x1x1472x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<1x1x1472x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1472xf32>, %arg1: tensor<1x1x1472x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1472x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1472xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1472 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1472xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1472x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1472xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1472xf32>) -> tensor<512x7x7x1472xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1472x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1472x128xf32>) -> tensor<1x1x1472x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1472xf32>, tensor<1x1x1472x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1472, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17766579553}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<1x1x264x44xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<1x1x264x44xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x264xf32>, %3: tensor<1x1x264x44xf32>, %7: tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<1x1x264x44xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n  return %ret : tensor<512x14x14x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x264xf32>, %arg1: tensor<1x1x264x44xf32>, %arg2: tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x264x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x264xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x44xf32>\n    memref.copy %2, %alloc : memref<512x14x14x44xf32> to memref<512x14x14x44xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 264 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x264xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x264x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x44xf32>\n    return %3 : tensor<512x14x14x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x264xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x264xf32>) -> tensor<512x14x14x264xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x264x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x264x44xf32>) -> tensor<1x1x264x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x264xf32>, tensor<1x1x264x44xf32>) outs(%7 : tensor<512x14x14x44xf32>) -> tensor<512x14x14x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 264, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 4266863919}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<1x1x1408x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<1x1x1408x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1408xf32>, %3: tensor<1x1x1408x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<1x1x1408x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1408xf32>, %arg1: tensor<1x1x1408x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1408x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1408xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1408 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1408xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1408x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1408xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1408xf32>) -> tensor<512x7x7x1408xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1408x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1408x128xf32>) -> tensor<1x1x1408x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1408xf32>, tensor<1x1x1408x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1408, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16988834185}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<1x1x1536x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<1x1x1536x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1536xf32>, %3: tensor<1x1x1536x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<1x1x1536x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1536xf32>, %arg1: tensor<1x1x1536x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1536x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1536xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1536 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1536xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1536x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1536xf32>) -> tensor<512x7x7x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1536x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1536x128xf32>) -> tensor<1x1x1536x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1536xf32>, tensor<1x1x1536x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18541472548}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<1x1x832x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<1x1x832x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x832xf32>, %3: tensor<1x1x832x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<1x1x832x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x832xf32>, %arg1: tensor<1x1x832x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x832x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x832xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 832 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x832xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x832x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x832xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x832xf32>) -> tensor<512x7x7x832xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x832x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x832x128xf32>) -> tensor<1x1x832x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x832xf32>, tensor<1x1x832x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 832, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 10000189043}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x72xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x72xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x18xf32>, %3: tensor<1x1x18x72xf32>, %7: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x72xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n  return %ret : tensor<512x1x1x72xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x18xf32>, %arg1: tensor<1x1x18x72xf32>, %arg2: tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x18x72xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x18xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x72xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x72xf32>\n    memref.copy %2, %alloc : memref<512x1x1x72xf32> to memref<512x1x1x72xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 72 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 18 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x18xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x18x72xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x72xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x72xf32>\n    return %3 : tensor<512x1x1x72xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x72xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x18xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x18xf32>) -> tensor<512x1x1x18xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x18x72xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x18x72xf32>) -> tensor<1x1x18x72xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x18xf32>, tensor<1x1x18x72xf32>) outs(%7 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x72xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x72xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 72, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 18, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1417035}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<1x1x56x6xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<1x1x56x6xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x56xf32>, %3: tensor<1x1x56x6xf32>, %7: tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<1x1x56x6xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n  return %ret : tensor<512x1x1x6xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x56xf32>, %arg1: tensor<1x1x56x6xf32>, %arg2: tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x56x6xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x56xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x6xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x6xf32>\n    memref.copy %2, %alloc : memref<512x1x1x6xf32> to memref<512x1x1x6xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 6 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 56 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x56xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x56x6xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x6xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x6xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x6xf32>\n    return %3 : tensor<512x1x1x6xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x6xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x56x6xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x56x6xf32>) -> tensor<1x1x56x6xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x6xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x56xf32>, tensor<1x1x56x6xf32>) outs(%7 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x6xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x6xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 6, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 56, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 491078}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<1x1x1664x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<1x1x1664x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1664xf32>, %3: tensor<1x1x1664x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<1x1x1664x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1664xf32>, %arg1: tensor<1x1x1664x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1664x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1664xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1664 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1664xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1664x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1664xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1664xf32>) -> tensor<512x7x7x1664xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1664x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1664x128xf32>) -> tensor<1x1x1664x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1664xf32>, tensor<1x1x1664x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1664, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20102003721}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<1x3x192x224xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<1x3x192x224xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x8x10x192xf32>, %3: tensor<1x3x192x224xf32>, %7: tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<1x3x192x224xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n  return %ret : tensor<512x8x8x224xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x8x10x192xf32>, %arg1: tensor<1x3x192x224xf32>, %arg2: tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x3x192x224xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x8x10x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x8x8x224xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x8x8x224xf32>\n    memref.copy %2, %alloc : memref<512x8x8x224xf32> to memref<512x8x8x224xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 224 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x8x10x192xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x3x192x224xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x224xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x224xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x8x8x224xf32>\n    return %3 : tensor<512x8x8x224xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x8x8x224xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x8x10x192xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x8x10x192xf32>) -> tensor<512x8x10x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x3x192x224xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x3x192x224xf32>) -> tensor<1x3x192x224xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x8x8x224xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x8x10x192xf32>, tensor<1x3x192x224xf32>) outs(%7 : tensor<512x8x8x224xf32>) -> tensor<512x8x8x224xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x8x8x224xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x8x8x224xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 224, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 15784155418}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<1x1x192x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<1x1x192x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x192xf32>, %3: tensor<1x1x192x48xf32>, %7: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<1x1x192x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n  return %ret : tensor<512x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x192xf32>, %arg1: tensor<1x1x192x48xf32>, %arg2: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x192x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x48xf32>\n    memref.copy %2, %alloc : memref<512x1x1x48xf32> to memref<512x1x1x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x192xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x192x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x48xf32>\n    return %3 : tensor<512x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x192x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x192x48xf32>) -> tensor<1x1x192x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x192xf32>, tensor<1x1x192x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16662140}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x92xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x92xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<1x1x368x92xf32>, %7: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x92xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n  return %ret : tensor<512x1x1x92xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<1x1x368x92xf32>, %arg2: tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x368x92xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x92xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x92xf32>\n    memref.copy %2, %alloc : memref<512x1x1x92xf32> to memref<512x1x1x92xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 92 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x368x92xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x92xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x92xf32>\n    return %3 : tensor<512x1x1x92xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x92xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x368x92xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x368x92xf32>) -> tensor<1x1x368x92xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x92xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x92xf32>) outs(%7 : tensor<512x1x1x92xf32>) -> tensor<512x1x1x92xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x92xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x92xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 92, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 63222248}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<1x1x36x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<1x1x36x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x36xf32>, %3: tensor<1x1x36x144xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<1x1x36x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x36xf32>, %arg1: tensor<1x1x36x144xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x36x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x36xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 36 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x36xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x36x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x36xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x36xf32>) -> tensor<512x1x1x36xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x36x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x36x144xf32>) -> tensor<1x1x36x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x36xf32>, tensor<1x1x36x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 36, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 6597395}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<1x1x96x240xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<1x1x96x240xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x96xf32>, %3: tensor<1x1x96x240xf32>, %7: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<1x1x96x240xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n  return %ret : tensor<512x14x14x240xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x96xf32>, %arg1: tensor<1x1x96x240xf32>, %arg2: tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x96x240xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x96xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x240xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x240xf32>\n    memref.copy %2, %alloc : memref<512x14x14x240xf32> to memref<512x14x14x240xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 240 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 96 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x96xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x96x240xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x240xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x240xf32>\n    return %3 : tensor<512x14x14x240xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x240xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x96xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x96xf32>) -> tensor<512x28x28x96xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x96x240xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x96x240xf32>) -> tensor<1x1x96x240xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x240xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x96xf32>, tensor<1x1x96x240xf32>) outs(%7 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x240xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x240xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 240, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 96, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8012673380}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<1x1x352x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<1x1x352x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x352xf32>, %3: tensor<1x1x352x128xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<1x1x352x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x352xf32>, %arg1: tensor<1x1x352x128xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x352x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x352xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 352 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x352xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x352x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x352xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x352xf32>) -> tensor<512x14x14x352xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x352x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x352x128xf32>) -> tensor<1x1x352x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x352xf32>, tensor<1x1x352x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 352, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16711630450}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x32xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x32xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<1x1x128x32xf32>, %7: tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x32xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n  return %ret : tensor<512x1x1x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<1x1x128x32xf32>, %arg2: tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x128x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x32xf32>\n    memref.copy %2, %alloc : memref<512x1x1x32xf32> to memref<512x1x1x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x128x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x32xf32>\n    return %3 : tensor<512x1x1x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x128x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x128x32xf32>) -> tensor<1x1x128x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x32xf32>) outs(%7 : tensor<512x1x1x32xf32>) -> tensor<512x1x1x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 7119620}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<1x1x120x336xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<1x1x120x336xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x120xf32>, %3: tensor<1x1x120x336xf32>, %7: tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<1x1x120x336xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n  return %ret : tensor<512x14x14x336xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x120xf32>, %arg1: tensor<1x1x120x336xf32>, %arg2: tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x120x336xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x336xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x336xf32>\n    memref.copy %2, %alloc : memref<512x14x14x336xf32> to memref<512x14x14x336xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 336 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x120xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x120x336xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x336xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x336xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x336xf32>\n    return %3 : tensor<512x14x14x336xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x336xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x120xf32>) -> tensor<512x28x28x120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x120x336xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x120x336xf32>) -> tensor<1x1x120x336xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x336xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x120xf32>, tensor<1x1x120x336xf32>) outs(%7 : tensor<512x14x14x336xf32>) -> tensor<512x14x14x336xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x336xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x336xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 336, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 14292278417}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<1x1x168x408xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<1x1x168x408xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x168xf32>, %3: tensor<1x1x168x408xf32>, %7: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<1x1x168x408xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n  return %ret : tensor<512x14x14x408xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x168xf32>, %arg1: tensor<1x1x168x408xf32>, %arg2: tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x168x408xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x408xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x408xf32>\n    memref.copy %2, %alloc : memref<512x14x14x408xf32> to memref<512x14x14x408xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 408 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x168xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x168x408xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x408xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x408xf32>\n    return %3 : tensor<512x14x14x408xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x408xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x168xf32>) -> tensor<512x28x28x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x168x408xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x168x408xf32>) -> tensor<1x1x168x408xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x408xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x168xf32>, tensor<1x1x168x408xf32>) outs(%7 : tensor<512x14x14x408xf32>) -> tensor<512x14x14x408xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x408xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x408xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 408, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24784601924}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<1x1x144x1296xf32>, %7: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n  return %ret : tensor<512x1x1x1296xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<1x1x144x1296xf32>, %arg2: tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x144x1296xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1296xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1296xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1296xf32> to memref<512x1x1x1296xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1296 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x144x1296xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1296xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1296xf32>\n    return %3 : tensor<512x1x1x1296xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1296xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x144x1296xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x144x1296xf32>) -> tensor<1x1x144x1296xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1296xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1296xf32>) outs(%7 : tensor<512x1x1x1296xf32>) -> tensor<512x1x1x1296xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1296xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1296xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1296, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 327663569}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<1x1x128x1088xf32>, %7: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n  return %ret : tensor<512x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<1x1x128x1088xf32>, %arg2: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x128x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1088xf32> to memref<512x1x1x1088xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x128x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1088xf32>\n    return %3 : tensor<512x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x128x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x128x1088xf32>) -> tensor<1x1x128x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 241482119}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<1x1x1120x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<1x1x1120x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1120xf32>, %3: tensor<1x1x1120x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<1x1x1120x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1120xf32>, %arg1: tensor<1x1x1120x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1120x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1120xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1120 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1120xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1120x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1120xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1120xf32>) -> tensor<512x7x7x1120xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1120x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1120x128xf32>) -> tensor<1x1x1120x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1120xf32>, tensor<1x1x1120x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1120, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13494347974}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<1x1x320x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<1x1x320x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x320xf32>, %3: tensor<1x1x320x80xf32>, %7: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<1x1x320x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n  return %ret : tensor<512x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x320xf32>, %arg1: tensor<1x1x320x80xf32>, %arg2: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x320x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x320xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x80xf32>\n    memref.copy %2, %alloc : memref<512x1x1x80xf32> to memref<512x1x1x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 320 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x320xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x320x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x80xf32>\n    return %3 : tensor<512x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x320xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x320xf32>) -> tensor<512x1x1x320xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x320x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x320x80xf32>) -> tensor<1x1x320x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x320xf32>, tensor<1x1x320x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 320, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 47716213}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x696xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x696xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x58xf32>, %3: tensor<1x1x58x696xf32>, %7: tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x696xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n  return %ret : tensor<512x1x1x696xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x58xf32>, %arg1: tensor<1x1x58x696xf32>, %arg2: tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x58x696xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x58xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x696xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x696xf32>\n    memref.copy %2, %alloc : memref<512x1x1x696xf32> to memref<512x1x1x696xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 696 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 58 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x58xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x58x696xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x696xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x696xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x696xf32>\n    return %3 : tensor<512x1x1x696xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x696xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x58xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x58xf32>) -> tensor<512x1x1x58xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x58x696xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x58x696xf32>) -> tensor<1x1x58x696xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x696xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x58xf32>, tensor<1x1x58x696xf32>) outs(%7 : tensor<512x1x1x696xf32>) -> tensor<512x1x1x696xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x696xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x696xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 696, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 58, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 60465504}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<1x1x8x144xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<1x1x8x144xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x8x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x8x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x8x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x8x144xf32>) -> tensor<1x1x8x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 984088}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x192xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x192xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<1x1x48x192xf32>, %7: tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x192xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n  return %ret : tensor<512x1x1x192xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<1x1x48x192xf32>, %arg2: tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x192xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x192xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x192xf32>\n    memref.copy %2, %alloc : memref<512x1x1x192xf32> to memref<512x1x1x192xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 192 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x192xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x192xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x192xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x192xf32>\n    return %3 : tensor<512x1x1x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x192xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x192xf32>) -> tensor<1x1x48x192xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x192xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x192xf32>) outs(%7 : tensor<512x1x1x192xf32>) -> tensor<512x1x1x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 192, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 13250446}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x28xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x28xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x112xf32>, %3: tensor<1x1x112x28xf32>, %7: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x28xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n  return %ret : tensor<512x1x1x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x112xf32>, %arg1: tensor<1x1x112x28xf32>, %arg2: tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x112x28xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x28xf32>\n    memref.copy %2, %alloc : memref<512x1x1x28xf32> to memref<512x1x1x28xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x112xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x112x28xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x28xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x28xf32>\n    return %3 : tensor<512x1x1x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x112x28xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x112x28xf32>) -> tensor<1x1x112x28xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x28xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x28xf32>) outs(%7 : tensor<512x1x1x28xf32>) -> tensor<512x1x1x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x28xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5495953}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1x1x272x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1x1x272x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x272xf32>, %3: tensor<1x1x272x1088xf32>, %7: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1x1x272x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n  return %ret : tensor<512x1x1x1088xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x272xf32>, %arg1: tensor<1x1x272x1088xf32>, %arg2: tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x272x1088xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x272xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1088xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1088xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1088xf32> to memref<512x1x1x1088xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1088 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 272 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x272xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x272x1088xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1088xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1088xf32>\n    return %3 : tensor<512x1x1x1088xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1088xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x272xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x272xf32>) -> tensor<512x1x1x272xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x272x1088xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x272x1088xf32>) -> tensor<1x1x272x1088xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1088xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x272xf32>, tensor<1x1x272x1088xf32>) outs(%7 : tensor<512x1x1x1088xf32>) -> tensor<512x1x1x1088xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1088xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1088xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1088, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 272, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 544523526}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x512xf32>, %3: tensor<1x1x512x128xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x512xf32>, %arg1: tensor<1x1x512x128xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x512x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x512xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x512x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x512xf32>) -> tensor<512x14x14x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x512x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24491304710}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<1x1x240x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<1x1x240x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x240xf32>, %3: tensor<1x1x240x528xf32>, %7: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<1x1x240x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n  return %ret : tensor<512x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x240xf32>, %arg1: tensor<1x1x240x528xf32>, %arg2: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x240x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x528xf32>\n    memref.copy %2, %alloc : memref<512x7x7x528xf32> to memref<512x7x7x528xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 240 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x240xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x240x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x528xf32>\n    return %3 : tensor<512x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x240xf32>) -> tensor<512x14x14x240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x240x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x240x528xf32>) -> tensor<1x1x240x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x240xf32>, tensor<1x1x240x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 11621307145}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<1x1x38x368xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<1x1x38x368xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x38xf32>, %3: tensor<1x1x38x368xf32>, %7: tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<1x1x38x368xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n  return %ret : tensor<512x1x1x368xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x38xf32>, %arg1: tensor<1x1x38x368xf32>, %arg2: tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x38x368xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x38xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x368xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x368xf32>\n    memref.copy %2, %alloc : memref<512x1x1x368xf32> to memref<512x1x1x368xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 368 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 38 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x38xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x38x368xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x368xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x368xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x368xf32>\n    return %3 : tensor<512x1x1x368xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x368xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x38x368xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x38x368xf32>) -> tensor<1x1x38x368xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x38xf32>, tensor<1x1x38x368xf32>) outs(%7 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x368xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x368xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 368, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 38, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17822533}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<1x1x1760x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<1x1x1760x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1760xf32>, %3: tensor<1x1x1760x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<1x1x1760x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1760xf32>, %arg1: tensor<1x1x1760x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1760x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1760xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1760 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1760xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1760x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1760xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1760xf32>) -> tensor<512x7x7x1760xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1760x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1760x128xf32>) -> tensor<1x1x1760x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1760xf32>, tensor<1x1x1760x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1760, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21265774443}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<1x1x32x128xf32>, %7: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n  return %ret : tensor<512x56x56x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<1x1x32x128xf32>, %arg2: tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x128xf32>\n    memref.copy %2, %alloc : memref<512x56x56x128xf32> to memref<512x56x56x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x128xf32>\n    return %3 : tensor<512x56x56x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x128xf32>) -> tensor<1x1x32x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x128xf32>) outs(%7 : tensor<512x56x56x128xf32>) -> tensor<512x56x56x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18152242853}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1512xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1512xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<1x1x144x1512xf32>, %7: tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1512xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n  return %ret : tensor<512x1x1x1512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<1x1x144x1512xf32>, %arg2: tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x144x1512xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1512xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1512xf32> to memref<512x1x1x1512xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x144x1512xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1512xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1512xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1512xf32>\n    return %3 : tensor<512x1x1x1512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x144x1512xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x144x1512xf32>) -> tensor<1x1x144x1512xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1512xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x1512xf32>) outs(%7 : tensor<512x1x1x1512xf32>) -> tensor<512x1x1x1512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 382425560}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x160xf32>, %3: tensor<1x1x160x160xf32>, %7: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n  return %ret : tensor<512x14x14x160xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x160xf32>, %arg1: tensor<1x1x160x160xf32>, %arg2: tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x160x160xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x160xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x160xf32>\n    memref.copy %2, %alloc : memref<512x14x14x160xf32> to memref<512x14x14x160xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 160 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x160xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x160x160xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x160xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x160xf32>\n    return %3 : tensor<512x14x14x160xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x160xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x160x160xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x160x160xf32>) -> tensor<1x1x160x160xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x160xf32>) outs(%7 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x160xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x160xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 160, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9350246073}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<1x1x576x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<1x1x576x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x576xf32>, %3: tensor<1x1x576x144xf32>, %7: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<1x1x576x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n  return %ret : tensor<512x1x1x144xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x576xf32>, %arg1: tensor<1x1x576x144xf32>, %arg2: tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x576x144xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x576xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x144xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x144xf32>\n    memref.copy %2, %alloc : memref<512x1x1x144xf32> to memref<512x1x1x144xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 144 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 576 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x576xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x576x144xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x144xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x144xf32>\n    return %3 : tensor<512x1x1x144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x576xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x576x144xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x576x144xf32>) -> tensor<1x1x576x144xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x576xf32>, tensor<1x1x576x144xf32>) outs(%7 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 144, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 576, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 157070752}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<1x1x11x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<1x1x11x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x11xf32>, %3: tensor<1x1x11x11xf32>, %7: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<1x1x11x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n  return %ret : tensor<512x56x56x11xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x11xf32>, %arg1: tensor<1x1x11x11xf32>, %arg2: tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x11x11xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x11xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x11xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x11xf32>\n    memref.copy %2, %alloc : memref<512x56x56x11xf32> to memref<512x56x56x11xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 11 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 11 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x11xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x11x11xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x11xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x11xf32>\n    return %3 : tensor<512x56x56x11xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x11xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x11xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x11x11xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x11x11xf32>) -> tensor<1x1x11x11xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x11xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x11xf32>, tensor<1x1x11x11xf32>) outs(%7 : tensor<512x56x56x11xf32>) -> tensor<512x56x56x11xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x11xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x11xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 11, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 11, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 489984715}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<1x1x14x152xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<1x1x14x152xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x14xf32>, %3: tensor<1x1x14x152xf32>, %7: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<1x1x14x152xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n  return %ret : tensor<512x1x1x152xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x14xf32>, %arg1: tensor<1x1x14x152xf32>, %arg2: tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x14x152xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x14xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x152xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x152xf32>\n    memref.copy %2, %alloc : memref<512x1x1x152xf32> to memref<512x1x1x152xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 152 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 14 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x14xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x14x152xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x152xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x152xf32>\n    return %3 : tensor<512x1x1x152xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x152xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x14xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x14xf32>) -> tensor<512x1x1x14xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x14x152xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x14x152xf32>) -> tensor<1x1x14x152xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x152xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x14xf32>, tensor<1x1x14x152xf32>) outs(%7 : tensor<512x1x1x152xf32>) -> tensor<512x1x1x152xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x152xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x152xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 152, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 14, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2062977}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<1x1x72x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<1x1x72x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x72xf32>, %3: tensor<1x1x72x8xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<1x1x72x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x72xf32>, %arg1: tensor<1x1x72x8xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x72x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x72xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 72 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x72xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x72x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x72xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x72xf32>) -> tensor<512x1x1x72xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x72x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x72x8xf32>) -> tensor<1x1x72x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x72xf32>, tensor<1x1x72x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 72, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 871523}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x299x299x3xf32>, %3: tensor<3x3x3x32xf32>, %7: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n  return %ret : tensor<512x149x149x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x299x299x3xf32>, %arg1: tensor<3x3x3x32xf32>, %arg2: tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x299x299x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x149x149x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x149x149x32xf32>\n    memref.copy %2, %alloc : memref<512x149x149x32xf32> to memref<512x149x149x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 149 {\n        affine.for %arg5 = 0 to 149 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x299x299x3xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<3x3x3x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x149x149x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x149x149x32xf32>\n    return %3 : tensor<512x149x149x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x149x149x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x299x299x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x299x299x3xf32>) -> tensor<512x299x299x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x3x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x3x32xf32>) -> tensor<3x3x3x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x149x149x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x299x299x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x149x149x32xf32>) -> tensor<512x149x149x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x149x149x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x149x149x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 149, 1], ["%arg5", 0, 149, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 28978723292}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<4x4x3x96xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<4x4x3x96xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x224x224x3xf32>, %3: tensor<4x4x3x96xf32>, %7: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<4x4x3x96xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n  return %ret : tensor<512x56x56x96xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x224x224x3xf32>, %arg1: tensor<4x4x3x96xf32>, %arg2: tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<4x4x3x96xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x96xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x96xf32>\n    memref.copy %2, %alloc : memref<512x56x56x96xf32> to memref<512x56x56x96xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 96 {\n            affine.for %arg7 = 0 to 4 {\n              affine.for %arg8 = 0 to 4 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x224x224x3xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<4x4x3x96xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x96xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x96xf32>\n    return %3 : tensor<512x56x56x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x224x224x3xf32>) -> tensor<512x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<4x4x3x96xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<4x4x3x96xf32>) -> tensor<4x4x3x96xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x96xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<4x4x3x96xf32>) outs(%7 : tensor<512x56x56x96xf32>) -> tensor<512x56x56x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 96, 1], ["%arg7", 0, 4, 1], ["%arg8", 0, 4, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 24529612924}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<1x1x704x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<1x1x704x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x704xf32>, %3: tensor<1x1x704x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<1x1x704x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x704xf32>, %arg1: tensor<1x1x704x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x704x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x704xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 704 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x704xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x704x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x704xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x704xf32>) -> tensor<512x7x7x704xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x704x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x704x128xf32>) -> tensor<1x1x704x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x704xf32>, tensor<1x1x704x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 704, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8451325123}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<1x1x16x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<1x1x16x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x16xf32>, %3: tensor<1x1x16x128xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<1x1x16x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x16xf32>, %arg1: tensor<1x1x16x128xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x16x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x16xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 16 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x16xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x16x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x16xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x16x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x16x128xf32>) -> tensor<1x1x16x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x16xf32>, tensor<1x1x16x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 16, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2218297}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1x1x308x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1x1x308x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x308xf32>, %3: tensor<1x1x308x1232xf32>, %7: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1x1x308x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n  return %ret : tensor<512x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x308xf32>, %arg1: tensor<1x1x308x1232xf32>, %arg2: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x308x1232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x308xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1232xf32> to memref<512x1x1x1232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 308 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x308xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x308x1232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1232xf32>\n    return %3 : tensor<512x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x308xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x308xf32>) -> tensor<512x1x1x308xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x308x1232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x308x1232xf32>) -> tensor<1x1x308x1232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x308xf32>, tensor<1x1x308x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 308, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 702456460}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<1x1x1440x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<1x1x1440x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1440xf32>, %3: tensor<1x1x1440x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<1x1x1440x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1440xf32>, %arg1: tensor<1x1x1440x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1440x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1440xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1440x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1440xf32>) -> tensor<512x7x7x1440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1440x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1440x128xf32>) -> tensor<1x1x1440x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1440xf32>, tensor<1x1x1440x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 17390854815}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x24xf32>, %3: tensor<1x1x24x56xf32>, %7: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n  return %ret : tensor<512x56x56x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x24xf32>, %arg1: tensor<1x1x24x56xf32>, %arg2: tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x24x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x56xf32>\n    memref.copy %2, %alloc : memref<512x56x56x56xf32> to memref<512x56x56x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x24xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x24x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x56xf32>\n    return %3 : tensor<512x56x56x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x24x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x24x56xf32>) -> tensor<1x1x24x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x56x56x56xf32>) -> tensor<512x56x56x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 5684956034}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x168xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x168xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x112x112x32xf32>, %3: tensor<1x1x32x168xf32>, %7: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x168xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n  return %ret : tensor<512x56x56x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x112x112x32xf32>, %arg1: tensor<1x1x32x168xf32>, %arg2: tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x32x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x112x112x32xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x168xf32>\n    memref.copy %2, %alloc : memref<512x56x56x168xf32> to memref<512x56x56x168xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 32 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x112x112x32xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x32x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x168xf32>\n    return %3 : tensor<512x56x56x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x112x112x32xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x112x112x32xf32>) -> tensor<512x112x112x32xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x32x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x32x168xf32>) -> tensor<1x1x32x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x112x112x32xf32>, tensor<1x1x32x168xf32>) outs(%7 : tensor<512x56x56x168xf32>) -> tensor<512x56x56x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 32, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 23985081288}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<1x1x1232x112xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<1x1x1232x112xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x1232xf32>, %3: tensor<1x1x1232x112xf32>, %7: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<1x1x1232x112xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n  return %ret : tensor<512x1x1x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x1232xf32>, %arg1: tensor<1x1x1232x112xf32>, %arg2: tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1232x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x1232xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x112xf32>\n    memref.copy %2, %alloc : memref<512x1x1x112xf32> to memref<512x1x1x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1232 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x1232xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1232x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x112xf32>\n    return %3 : tensor<512x1x1x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1232x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1232x112xf32>) -> tensor<1x1x1232x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x1232xf32>, tensor<1x1x1232x112xf32>) outs(%7 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1232, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 264072347}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<1x1x768x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<1x1x768x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x768xf32>, %3: tensor<1x1x768x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<1x1x768x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x768xf32>, %arg1: tensor<1x1x768x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x768x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x768xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x768x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x768xf32>) -> tensor<512x7x7x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x768x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x768x128xf32>) -> tensor<1x1x768x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x768xf32>, tensor<1x1x768x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 9227458287}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<1x1x1504x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<1x1x1504x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1504xf32>, %3: tensor<1x1x1504x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<1x1x1504x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1504xf32>, %arg1: tensor<1x1x1504x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1504x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1504xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1504 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1504xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1504x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1504xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1504xf32>) -> tensor<512x7x7x1504xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1504x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1504x128xf32>) -> tensor<1x1x1504x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1504xf32>, tensor<1x1x1504x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1504, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18164155290}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x576xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x576xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x144xf32>, %3: tensor<1x1x144x576xf32>, %7: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x576xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n  return %ret : tensor<512x1x1x576xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x144xf32>, %arg1: tensor<1x1x144x576xf32>, %arg2: tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x144x576xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x144xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x576xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x576xf32>\n    memref.copy %2, %alloc : memref<512x1x1x576xf32> to memref<512x1x1x576xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 576 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 144 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x144xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x144x576xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x576xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x576xf32>\n    return %3 : tensor<512x1x1x576xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x576xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x144xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x144xf32>) -> tensor<512x1x1x144xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x144x576xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x144x576xf32>) -> tensor<1x1x144x576xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x576xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x144xf32>, tensor<1x1x144x576xf32>) outs(%7 : tensor<512x1x1x576xf32>) -> tensor<512x1x1x576xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x576xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x576xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 576, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 144, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 146955063}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x24xf32>, %3: tensor<1x1x24x56xf32>, %7: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n  return %ret : tensor<512x28x28x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x24xf32>, %arg1: tensor<1x1x24x56xf32>, %arg2: tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x24x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x24xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x56xf32>\n    memref.copy %2, %alloc : memref<512x28x28x56xf32> to memref<512x28x28x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 24 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x24xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x24x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x56xf32>\n    return %3 : tensor<512x28x28x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x24xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x24xf32>) -> tensor<512x56x56x24xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x24x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x24x56xf32>) -> tensor<1x1x24x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x24xf32>, tensor<1x1x24x56xf32>) outs(%7 : tensor<512x28x28x56xf32>) -> tensor<512x28x28x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 24, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1425254746}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x512xf32>, %3: tensor<1x1x512x128xf32>, %7: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n  return %ret : tensor<512x1x1x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x512xf32>, %arg1: tensor<1x1x512x128xf32>, %arg2: tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x512x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x128xf32>\n    memref.copy %2, %alloc : memref<512x1x1x128xf32> to memref<512x1x1x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 512 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x512xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x512x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x128xf32>\n    return %3 : tensor<512x1x1x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x512xf32>) -> tensor<512x1x1x512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x512x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x512x128xf32>) -> tensor<1x1x512x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x512xf32>, tensor<1x1x512x128xf32>) outs(%7 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 123590439}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<1x1x1792x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<1x1x1792x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1792xf32>, %3: tensor<1x1x1792x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<1x1x1792x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1792xf32>, %arg1: tensor<1x1x1792x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1792x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1792xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1792 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1792xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1792x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1792xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1792xf32>) -> tensor<512x7x7x1792xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1792x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1792x128xf32>) -> tensor<1x1x1792x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1792xf32>, tensor<1x1x1792x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1792, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21658905243}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x168xf32>, %3: tensor<1x1x168x8xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x168xf32>, %arg1: tensor<1x1x168x8xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x168x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x168xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x168x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x168x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x168x8xf32>) -> tensor<1x1x168x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2350810}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<1x1x768x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<1x1x768x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x768xf32>, %3: tensor<1x1x768x80xf32>, %7: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<1x1x768x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n  return %ret : tensor<512x1x1x80xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x768xf32>, %arg1: tensor<1x1x768x80xf32>, %arg2: tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x768x80xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x80xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x80xf32>\n    memref.copy %2, %alloc : memref<512x1x1x80xf32> to memref<512x1x1x80xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 80 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 768 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x768xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x768x80xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x80xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x80xf32>\n    return %3 : tensor<512x1x1x80xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x80xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x768xf32>) -> tensor<512x1x1x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x768x80xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x768x80xf32>) -> tensor<1x1x768x80xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x80xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x768xf32>, tensor<1x1x768x80xf32>) outs(%7 : tensor<512x1x1x80xf32>) -> tensor<512x1x1x80xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x80xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x80xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 80, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 116986327}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<1x1x48x104xf32>, %7: tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n  return %ret : tensor<512x56x56x104xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<1x1x48x104xf32>, %arg2: tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x104xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x104xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x104xf32>\n    memref.copy %2, %alloc : memref<512x56x56x104xf32> to memref<512x56x56x104xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 104 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x104xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x104xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x104xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x104xf32>\n    return %3 : tensor<512x56x56x104xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x104xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x104xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x104xf32>) -> tensor<1x1x48x104xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x104xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x104xf32>) outs(%7 : tensor<512x56x56x104xf32>) -> tensor<512x56x56x104xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x104xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x104xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 104, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 25065352248}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<1x1x672x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<1x1x672x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x672xf32>, %3: tensor<1x1x672x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<1x1x672x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x672xf32>, %arg1: tensor<1x1x672x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x672x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 672 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x672xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x672x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x672xf32>) -> tensor<512x7x7x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x672x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x672x128xf32>) -> tensor<1x1x672x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x672xf32>, tensor<1x1x672x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8060611507}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x52xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x52xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<1x1x440x52xf32>, %7: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x52xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n  return %ret : tensor<512x1x1x52xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<1x1x440x52xf32>, %arg2: tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x440x52xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x52xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x52xf32>\n    memref.copy %2, %alloc : memref<512x1x1x52xf32> to memref<512x1x1x52xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 52 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x440x52xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x52xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x52xf32>\n    return %3 : tensor<512x1x1x52xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x52xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x440x52xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x440x52xf32>) -> tensor<1x1x440x52xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x52xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x52xf32>) outs(%7 : tensor<512x1x1x52xf32>) -> tensor<512x1x1x52xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x52xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x52xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 52, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 42837909}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x38xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x38xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x368xf32>, %3: tensor<1x1x368x38xf32>, %7: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x38xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n  return %ret : tensor<512x1x1x38xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x368xf32>, %arg1: tensor<1x1x368x38xf32>, %arg2: tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x368x38xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x38xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x38xf32>\n    memref.copy %2, %alloc : memref<512x1x1x38xf32> to memref<512x1x1x38xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 38 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 368 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x368xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x368x38xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x38xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x38xf32>\n    return %3 : tensor<512x1x1x38xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x38xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x368xf32>) -> tensor<512x1x1x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x368x38xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x368x38xf32>) -> tensor<1x1x368x38xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x38xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x368xf32>, tensor<1x1x368x38xf32>) outs(%7 : tensor<512x1x1x38xf32>) -> tensor<512x1x1x38xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x38xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x38xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 38, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26022214}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<1x1x608x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<1x1x608x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x608xf32>, %3: tensor<1x1x608x64xf32>, %7: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<1x1x608x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n  return %ret : tensor<512x1x1x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x608xf32>, %arg1: tensor<1x1x608x64xf32>, %arg2: tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x608x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x64xf32>\n    memref.copy %2, %alloc : memref<512x1x1x64xf32> to memref<512x1x1x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 608 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x608xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x608x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x64xf32>\n    return %3 : tensor<512x1x1x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x608xf32>) -> tensor<512x1x1x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x608x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x608x64xf32>) -> tensor<1x1x608x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x608xf32>, tensor<1x1x608x64xf32>) outs(%7 : tensor<512x1x1x64xf32>) -> tensor<512x1x1x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 73762580}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x110xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x110xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x440xf32>, %3: tensor<1x1x440x110xf32>, %7: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x110xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n  return %ret : tensor<512x1x1x110xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x440xf32>, %arg1: tensor<1x1x440x110xf32>, %arg2: tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x440x110xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x110xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x110xf32>\n    memref.copy %2, %alloc : memref<512x1x1x110xf32> to memref<512x1x1x110xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 110 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 440 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x440xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x440x110xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x110xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x110xf32>\n    return %3 : tensor<512x1x1x110xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x110xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x440xf32>) -> tensor<512x1x1x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x440x110xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x440x110xf32>) -> tensor<1x1x440x110xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x110xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x440xf32>, tensor<1x1x440x110xf32>) outs(%7 : tensor<512x1x1x110xf32>) -> tensor<512x1x1x110xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x110xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x110xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 110, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 90661553}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<1x1x192x64xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<1x1x192x64xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x35x35x192xf32>, %3: tensor<1x1x192x64xf32>, %7: tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<1x1x192x64xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n  return %ret : tensor<512x35x35x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x35x35x192xf32>, %arg1: tensor<1x1x192x64xf32>, %arg2: tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x192x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x35x35x192xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x35x35x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x35x35x64xf32>\n    memref.copy %2, %alloc : memref<512x35x35x64xf32> to memref<512x35x35x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 35 {\n        affine.for %arg5 = 0 to 35 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 192 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x35x35x192xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x192x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x35x35x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x35x35x64xf32>\n    return %3 : tensor<512x35x35x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x35x35x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x35x35x192xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x35x35x192xf32>) -> tensor<512x35x35x192xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x192x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x192x64xf32>) -> tensor<1x1x192x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x35x35x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x35x35x192xf32>, tensor<1x1x192x64xf32>) outs(%7 : tensor<512x35x35x64xf32>) -> tensor<512x35x35x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x35x35x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x35x35x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 35, 1], ["%arg5", 0, 35, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 192, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 27932361893}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<1x1x6x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<1x1x6x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x6xf32>, %3: tensor<1x1x6x56xf32>, %7: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<1x1x6x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n  return %ret : tensor<512x1x1x56xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x6xf32>, %arg1: tensor<1x1x6x56xf32>, %arg2: tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x6x56xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x6xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x56xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x56xf32>\n    memref.copy %2, %alloc : memref<512x1x1x56xf32> to memref<512x1x1x56xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 56 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 6 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x6xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x6x56xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x56xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x56xf32>\n    return %3 : tensor<512x1x1x56xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x56xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x6xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x6xf32>) -> tensor<512x1x1x6xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x6x56xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x6x56xf32>) -> tensor<1x1x6x56xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x56xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x6xf32>, tensor<1x1x6x56xf32>) outs(%7 : tensor<512x1x1x56xf32>) -> tensor<512x1x1x56xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x56xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x56xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 56, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 6, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 266486}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<1x1x44x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<1x1x44x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x28x28x44xf32>, %3: tensor<1x1x44x44xf32>, %7: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<1x1x44x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n  return %ret : tensor<512x28x28x44xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x28x28x44xf32>, %arg1: tensor<1x1x44x44xf32>, %arg2: tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x44x44xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x28x28x44xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x28x28x44xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x28x28x44xf32>\n    memref.copy %2, %alloc : memref<512x28x28x44xf32> to memref<512x28x28x44xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 28 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 44 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 44 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x28x28x44xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x44x44xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x28x28x44xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x28x28x44xf32>\n    return %3 : tensor<512x28x28x44xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x28x28x44xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x44x44xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x44x44xf32>) -> tensor<1x1x44x44xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x28x28x44xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x28x28x44xf32>, tensor<1x1x44x44xf32>) outs(%7 : tensor<512x28x28x44xf32>) -> tensor<512x28x28x44xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x28x28x44xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x28x28x44xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 28, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 44, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 44, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 2432297450}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<1x1x336x84xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<1x1x336x84xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x336xf32>, %3: tensor<1x1x336x84xf32>, %7: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<1x1x336x84xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n  return %ret : tensor<512x1x1x84xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x336xf32>, %arg1: tensor<1x1x336x84xf32>, %arg2: tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x336x84xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x336xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x84xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x84xf32>\n    memref.copy %2, %alloc : memref<512x1x1x84xf32> to memref<512x1x1x84xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 84 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 336 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x336xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x336x84xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x84xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x84xf32>\n    return %3 : tensor<512x1x1x84xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x84xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x336xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x336xf32>) -> tensor<512x1x1x336xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x336x84xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x336x84xf32>) -> tensor<1x1x336x84xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x84xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x336xf32>, tensor<1x1x336x84xf32>) outs(%7 : tensor<512x1x1x84xf32>) -> tensor<512x1x1x84xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x84xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x84xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 84, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 336, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 52326969}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<1x1x1696x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<1x1x1696x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1696xf32>, %3: tensor<1x1x1696x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<1x1x1696x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1696xf32>, %arg1: tensor<1x1x1696x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1696x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1696xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1696 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1696xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1696x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1696xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1696xf32>) -> tensor<512x7x7x1696xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1696x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1696x128xf32>) -> tensor<1x1x1696x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1696xf32>, tensor<1x1x1696x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1696, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 20488054302}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<1x1x8x48xf32>, %7: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n  return %ret : tensor<512x1x1x48xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<1x1x8x48xf32>, %arg2: tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x8x48xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x48xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x48xf32>\n    memref.copy %2, %alloc : memref<512x1x1x48xf32> to memref<512x1x1x48xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 48 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x8x48xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x48xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x48xf32>\n    return %3 : tensor<512x1x1x48xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x48xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x8x48xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x8x48xf32>) -> tensor<1x1x8x48xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x48xf32>) outs(%7 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x48xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x48xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 48, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 329426}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<1x1x222x888xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<1x1x222x888xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x222xf32>, %3: tensor<1x1x222x888xf32>, %7: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<1x1x222x888xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n  return %ret : tensor<512x1x1x888xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x222xf32>, %arg1: tensor<1x1x222x888xf32>, %arg2: tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x222x888xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x222xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x888xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x888xf32>\n    memref.copy %2, %alloc : memref<512x1x1x888xf32> to memref<512x1x1x888xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 888 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 222 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x222xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x222x888xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x888xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x888xf32>\n    return %3 : tensor<512x1x1x888xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x888xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x222xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x222xf32>) -> tensor<512x1x1x222xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x222x888xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x222x888xf32>) -> tensor<1x1x222x888xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x888xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x222xf32>, tensor<1x1x222x888xf32>) outs(%7 : tensor<512x1x1x888xf32>) -> tensor<512x1x1x888xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x888xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x888xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 888, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 222, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 358505979}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x112xf32>, %3: tensor<1x1x112x1232xf32>, %7: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n  return %ret : tensor<512x1x1x1232xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x112xf32>, %arg1: tensor<1x1x112x1232xf32>, %arg2: tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x112x1232xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x112xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x1232xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x1232xf32>\n    memref.copy %2, %alloc : memref<512x1x1x1232xf32> to memref<512x1x1x1232xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 1232 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 112 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x112xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x112x1232xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x1232xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x1232xf32>\n    return %3 : tensor<512x1x1x1232xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x1232xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x112xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x112xf32>) -> tensor<512x1x1x112xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x112x1232xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x112x1232xf32>) -> tensor<1x1x112x1232xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x1232xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x112xf32>, tensor<1x1x112x1232xf32>) outs(%7 : tensor<512x1x1x1232xf32>) -> tensor<512x1x1x1232xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x1232xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x1232xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 1232, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 112, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 235390004}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<1x1x544x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<1x1x544x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x544xf32>, %3: tensor<1x1x544x128xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<1x1x544x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x544xf32>, %arg1: tensor<1x1x544x128xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x544x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x544xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 544 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x544xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x544x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x544xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x544xf32>) -> tensor<512x14x14x544xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x544x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x544x128xf32>) -> tensor<1x1x544x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x544xf32>, tensor<1x1x544x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 544, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26020879384}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<3x1x224x256xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<3x1x224x256xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>", "wrapped_operation": "func.func @func_call(%padded: tensor<512x10x8x224xf32>, %3: tensor<3x1x224x256xf32>, %7: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<3x1x224x256xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n  return %ret : tensor<512x8x8x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x10x8x224xf32>, %arg1: tensor<3x1x224x256xf32>, %arg2: tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x1x224x256xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x10x8x224xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x8x8x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x8x8x256xf32>\n    memref.copy %2, %alloc : memref<512x8x8x256xf32> to memref<512x8x8x256xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 8 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 224 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x10x8x224xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<3x1x224x256xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x8x8x256xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x8x8x256xf32>\n    return %3 : tensor<512x8x8x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x8x8x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_padded = bufferization.alloc_tensor() : tensor<512x10x8x224xf32>\n%padded = linalg.fill ins(%val : f32) outs(%tmp_padded : tensor<512x10x8x224xf32>) -> tensor<512x10x8x224xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x1x224x256xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x1x224x256xf32>) -> tensor<3x1x224x256xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x8x8x256xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%padded, %3 : tensor<512x10x8x224xf32>, tensor<3x1x224x256xf32>) outs(%7 : tensor<512x8x8x256xf32>) -> tensor<512x8x8x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x8x8x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x8x8x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 8, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 224, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21130217112}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<1x1x1632x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<1x1x1632x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1632xf32>, %3: tensor<1x1x1632x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<1x1x1632x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1632xf32>, %arg1: tensor<1x1x1632x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1632x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1632xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1632 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1632xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1632x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1632xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1632xf32>) -> tensor<512x7x7x1632xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1632x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1632x128xf32>) -> tensor<1x1x1632x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1632xf32>, tensor<1x1x1632x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1632, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 19707134318}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<1x1x208x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<1x1x208x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x208xf32>, %3: tensor<1x1x208x440xf32>, %7: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<1x1x208x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n  return %ret : tensor<512x7x7x440xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x208xf32>, %arg1: tensor<1x1x208x440xf32>, %arg2: tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x208x440xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x208xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x440xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x440xf32>\n    memref.copy %2, %alloc : memref<512x7x7x440xf32> to memref<512x7x7x440xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 440 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 208 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x208xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x208x440xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x440xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x440xf32>\n    return %3 : tensor<512x7x7x440xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x440xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x208xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x208xf32>) -> tensor<512x14x14x208xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x208x440xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x208x440xf32>) -> tensor<1x1x208x440xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x440xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x208xf32>, tensor<1x1x208x440xf32>) outs(%7 : tensor<512x7x7x440xf32>) -> tensor<512x7x7x440xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x440xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x440xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 440, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 208, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 8338338562}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x168xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x168xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x8xf32>, %3: tensor<1x1x8x168xf32>, %7: tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x168xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n  return %ret : tensor<512x1x1x168xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x8xf32>, %arg1: tensor<1x1x8x168xf32>, %arg2: tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x8x168xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x8xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x168xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x168xf32>\n    memref.copy %2, %alloc : memref<512x1x1x168xf32> to memref<512x1x1x168xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 168 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 8 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x8xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x8x168xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x168xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x168xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x168xf32>\n    return %3 : tensor<512x1x1x168xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x168xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x8x168xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x8x168xf32>) -> tensor<1x1x8x168xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x8xf32>, tensor<1x1x8x168xf32>) outs(%7 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x168xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x168xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 168, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 8, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 1122148}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<1x1x1568x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<1x1x1568x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x1568xf32>, %3: tensor<1x1x1568x128xf32>, %7: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<1x1x1568x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n  return %ret : tensor<512x7x7x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x1568xf32>, %arg1: tensor<1x1x1568x128xf32>, %arg2: tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x1568x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x1568xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x128xf32>\n    memref.copy %2, %alloc : memref<512x7x7x128xf32> to memref<512x7x7x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 1568 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x1568xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x1568x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x128xf32>\n    return %3 : tensor<512x7x7x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x1568xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x1568xf32>) -> tensor<512x7x7x1568xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x1568x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x1568x128xf32>) -> tensor<1x1x1568x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x1568xf32>, tensor<1x1x1568x128xf32>) outs(%7 : tensor<512x7x7x128xf32>) -> tensor<512x7x7x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 1568, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 18932875019}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x112xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x112xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x48xf32>, %3: tensor<1x1x48x112xf32>, %7: tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x112xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n  return %ret : tensor<512x56x56x112xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x48xf32>, %arg1: tensor<1x1x48x112xf32>, %arg2: tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x112xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x112xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x112xf32>\n    memref.copy %2, %alloc : memref<512x56x56x112xf32> to memref<512x56x56x112xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 112 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x112xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x112xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x112xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x112xf32>\n    return %3 : tensor<512x56x56x112xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x112xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x48xf32>) -> tensor<512x56x56x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x112xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x112xf32>) -> tensor<1x1x48x112xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x112xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x48xf32>, tensor<1x1x48x112xf32>) outs(%7 : tensor<512x56x56x112xf32>) -> tensor<512x56x56x112xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x112xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x112xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 112, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26980838525}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x168xf32>, %3: tensor<1x1x168x42xf32>, %7: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n  return %ret : tensor<512x1x1x42xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x168xf32>, %arg1: tensor<1x1x168x42xf32>, %arg2: tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x168x42xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x168xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x42xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x42xf32>\n    memref.copy %2, %alloc : memref<512x1x1x42xf32> to memref<512x1x1x42xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 42 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 168 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x168xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x168x42xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x42xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x42xf32>\n    return %3 : tensor<512x1x1x42xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x42xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x168xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x168xf32>) -> tensor<512x1x1x168xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x168x42xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x168x42xf32>) -> tensor<1x1x168x42xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x42xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x168xf32>, tensor<1x1x168x42xf32>) outs(%7 : tensor<512x1x1x42xf32>) -> tensor<512x1x1x42xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x42xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x42xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 42, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 168, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 12565645}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<1x1x64x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<1x1x64x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x56x56x64xf32>, %3: tensor<1x1x64x64xf32>, %7: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<1x1x64x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n  return %ret : tensor<512x56x56x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x56x56x64xf32>, %arg1: tensor<1x1x64x64xf32>, %arg2: tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x64x64xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x56x56x64xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x56x56x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x56x56x64xf32>\n    memref.copy %2, %alloc : memref<512x56x56x64xf32> to memref<512x56x56x64xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 56 {\n        affine.for %arg5 = 0 to 56 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 64 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x56x56x64xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x64x64xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x56x56x64xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x56x56x64xf32>\n    return %3 : tensor<512x56x56x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x56x56x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x64x64xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x64x64xf32>) -> tensor<1x1x64x64xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x56x56x64xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x56x56x64xf32>, tensor<1x1x64x64xf32>) outs(%7 : tensor<512x56x56x64xf32>) -> tensor<512x56x56x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x56x56x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x56x56x64xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 56, 1], ["%arg5", 0, 56, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 64, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21903592228}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x48xf32>, %3: tensor<1x1x48x8xf32>, %7: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n  return %ret : tensor<512x1x1x8xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x48xf32>, %arg1: tensor<1x1x48x8xf32>, %arg2: tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x48x8xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x48xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x8xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x8xf32>\n    memref.copy %2, %alloc : memref<512x1x1x8xf32> to memref<512x1x1x8xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 8 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 48 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x48xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x48x8xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x8xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x8xf32>\n    return %3 : tensor<512x1x1x8xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x8xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x48xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x48xf32>) -> tensor<512x1x1x48xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x48x8xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x48x8xf32>) -> tensor<1x1x48x8xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x8xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x48xf32>, tensor<1x1x48x8xf32>) outs(%7 : tensor<512x1x1x8xf32>) -> tensor<512x1x1x8xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x8xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x8xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 8, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 48, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 488268}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x224x224x3xf32>, %3: tensor<3x3x3x32xf32>, %7: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n  return %ret : tensor<512x111x111x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x224x224x3xf32>, %arg1: tensor<3x3x3x32xf32>, %arg2: tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<3x3x3x32xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x224x224x3xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x111x111x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x111x111x32xf32>\n    memref.copy %2, %alloc : memref<512x111x111x32xf32> to memref<512x111x111x32xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 111 {\n        affine.for %arg5 = 0 to 111 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                affine.for %arg9 = 0 to 3 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x224x224x3xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<3x3x3x32xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x111x111x32xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x111x111x32xf32>\n    return %3 : tensor<512x111x111x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x111x111x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x224x224x3xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x224x224x3xf32>) -> tensor<512x224x224x3xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<3x3x3x32xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<3x3x3x32xf32>) -> tensor<3x3x3x32xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x111x111x32xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x224x224x3xf32>, tensor<3x3x3x32xf32>) outs(%7 : tensor<512x111x111x32xf32>) -> tensor<512x111x111x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x111x111x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x111x111x32xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 111, 1], ["%arg5", 0, 111, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1], ["%arg9", 0, 3, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 16069005745}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<1x1x448x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<1x1x448x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x448xf32>, %3: tensor<1x1x448x128xf32>, %7: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<1x1x448x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n  return %ret : tensor<512x14x14x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x448xf32>, %arg1: tensor<1x1x448x128xf32>, %arg2: tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x448x128xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x448xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x128xf32>\n    memref.copy %2, %alloc : memref<512x14x14x128xf32> to memref<512x14x14x128xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 448 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x448xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x448x128xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x128xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x128xf32>\n    return %3 : tensor<512x14x14x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x448xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x448xf32>) -> tensor<512x14x14x448xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x448x128xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x448x128xf32>) -> tensor<1x1x448x128xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x128xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x448xf32>, tensor<1x1x448x128xf32>) outs(%7 : tensor<512x14x14x128xf32>) -> tensor<512x14x14x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 448, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 21361081003}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x384xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x384xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x14x14x160xf32>, %3: tensor<1x1x160x384xf32>, %7: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x384xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n  return %ret : tensor<512x14x14x384xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x14x14x160xf32>, %arg1: tensor<1x1x160x384xf32>, %arg2: tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x160x384xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x14x14x160xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x14x14x384xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x14x14x384xf32>\n    memref.copy %2, %alloc : memref<512x14x14x384xf32> to memref<512x14x14x384xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 14 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 384 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 160 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x14x14x160xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x160x384xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x14x14x384xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x14x14x384xf32>\n    return %3 : tensor<512x14x14x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x14x14x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x14x14x160xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x14x14x160xf32>) -> tensor<512x14x14x160xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x160x384xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x160x384xf32>) -> tensor<1x1x160x384xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x14x14x384xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x14x14x160xf32>, tensor<1x1x160x384xf32>) outs(%7 : tensor<512x14x14x384xf32>) -> tensor<512x14x14x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x14x14x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x14x14x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 14, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 384, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 160, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 22228367078}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x16xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x16xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x1x1x128xf32>, %3: tensor<1x1x128x16xf32>, %7: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x16xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n  return %ret : tensor<512x1x1x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x1x1x128xf32>, %arg1: tensor<1x1x128x16xf32>, %arg2: tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x128x16xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x1x1x128xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x1x1x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x1x1x16xf32>\n    memref.copy %2, %alloc : memref<512x1x1x16xf32> to memref<512x1x1x16xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 1 {\n        affine.for %arg5 = 0 to 1 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 128 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x1x1x128xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x128x16xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x1x1x16xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x1x1x16xf32>\n    return %3 : tensor<512x1x1x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x1x1x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x1x1x128xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x1x1x128xf32>) -> tensor<512x1x1x128xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x128x16xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x128x16xf32>) -> tensor<1x1x128x16xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x1x1x16xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x1x1x128xf32>, tensor<1x1x128x16xf32>) outs(%7 : tensor<512x1x1x16xf32>) -> tensor<512x1x1x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x1x1x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x1x1x16xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 1, 1], ["%arg5", 0, 1, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 128, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 3556124}], ["linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<1x1x528x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", {"operation": "linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<1x1x528x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<512x7x7x528xf32>, %3: tensor<1x1x528x528xf32>, %7: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n  %ret = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<1x1x528x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n  return %ret : tensor<512x7x7x528xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<512x7x7x528xf32>, %arg1: tensor<1x1x528x528xf32>, %arg2: tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1x528x528xf32>\n    %1 = bufferization.to_memref %arg0 : memref<512x7x7x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<512x7x7x528xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<512x7x7x528xf32>\n    memref.copy %2, %alloc : memref<512x7x7x528xf32> to memref<512x7x7x528xf32>\n    affine.for %arg3 = 0 to 512 {\n      affine.for %arg4 = 0 to 7 {\n        affine.for %arg5 = 0 to 7 {\n          affine.for %arg6 = 0 to 528 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                affine.for %arg9 = 0 to 528 {\n                  %4 = affine.apply #map(%arg4, %arg7)\n                  %5 = affine.apply #map(%arg5, %arg8)\n                  %6 = affine.load %1[%arg3, %4, %5, %arg9] : memref<512x7x7x528xf32>\n                  %7 = affine.load %0[%arg7, %arg8, %arg9, %arg6] : memref<1x1x528x528xf32>\n                  %8 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                  %9 = arith.mulf %6, %7 : f32\n                  %10 = arith.addf %8, %9 : f32\n                  affine.store %10, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<512x7x7x528xf32>\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<512x7x7x528xf32>\n    return %3 : tensor<512x7x7x528xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<512x7x7x528xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1x528x528xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1x528x528xf32>) -> tensor<1x1x528x528xf32>\n%tmp_7 = bufferization.alloc_tensor() : tensor<512x7x7x528xf32>\n%7 = linalg.fill ins(%val : f32) outs(%tmp_7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.conv_2d_nhwc_hwcf {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%1, %3 : tensor<512x7x7x528xf32>, tensor<1x1x528x528xf32>) outs(%7 : tensor<512x7x7x528xf32>) -> tensor<512x7x7x528xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<512x7x7x528xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<512x7x7x528xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 512, 1], ["%arg4", 0, 7, 1], ["%arg5", 0, 7, 1], ["%arg6", 0, 528, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1], ["%arg9", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4 + %arg7", "%arg5 + %arg8", "%arg9"], ["%arg7", "%arg8", "%arg9", "%arg6"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": ["%arg3", "%arg4", "%arg5", "%arg6"]}, "execution_time": 26028296621}]]