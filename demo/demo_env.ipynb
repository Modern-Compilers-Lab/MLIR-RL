{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hiearchy_simple_ppo_env import ParallelEnv\n",
    "from utils.ppo_model import HiearchyModel as MyModel\n",
    "from utils.consts import (\n",
    "    MAX_NUM_STORES_LOADS,\n",
    "    MAX_NUM_LOOPS,\n",
    "    MAX_NUM_LOAD_STORE_DIM\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 19.58it/s]\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'len_trajectory': 64,\n",
    "    'ppo_batch_size': 64,\n",
    "    'steps':10000,\n",
    "    'ppo_epochs':4,\n",
    "    'logs':True,\n",
    "    'entropy_coef':0.01,\n",
    "    'lr':0.001,\n",
    "    'truncate':5,\n",
    "    'json_file':\"generated_data/bigger_input_nn_(32x230x230x3)operations.json\",\n",
    "}\n",
    "\n",
    "env = ParallelEnv(\n",
    "    json_file=CONFIG[\"json_file\"],\n",
    "    num_env=1,\n",
    "    truncate=CONFIG[\"truncate\"],\n",
    "    reset_repeat=1,\n",
    "    step_repeat=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 411\n"
     ]
    }
   ],
   "source": [
    "# Define the model:\n",
    "input_dim = MAX_NUM_LOOPS + MAX_NUM_LOOPS*MAX_NUM_LOAD_STORE_DIM*MAX_NUM_STORES_LOADS + MAX_NUM_LOOPS*MAX_NUM_LOAD_STORE_DIM + 5 + \\\n",
    "    MAX_NUM_LOOPS*3*CONFIG[\"truncate\"]\n",
    "print('input_dim:', input_dim)\n",
    "\n",
    "model = MyModel(\n",
    "    input_dim=input_dim,\n",
    "    num_loops=MAX_NUM_LOOPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_state, batch_obs = env.reset()\n",
    "obs = torch.cat(batch_obs).to(device)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        action_index, action_log_p, values, entropy = model.sample(obs)\n",
    "    \n",
    "\n",
    "    batch_next_obs, batch_reward, batch_terminated, batch_truncated, batch_next_state, batch_final_state = env.step(batch_state, action_index, model)\n",
    "    \n",
    "    # print(batch_next_state[0].actions)\n",
    "\n",
    "    for i in range(env.num_env):\n",
    "        done     = batch_terminated[i] or batch_truncated[i]\n",
    "        final_state = batch_final_state[i]\n",
    "        # print(done)\n",
    "        if done:\n",
    "            speedup_metric = final_state.root_exec_time /  final_state.exec_time\n",
    "            print('-'*70)\n",
    "            print(final_state.operation_id)\n",
    "            print('speedup:', speedup_metric)\n",
    "            print('-'*70)                \n",
    "\n",
    "    batch_state = batch_next_state\n",
    "    obs = torch.cat(batch_next_obs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(\n",
    "    input_dim=input_dim,\n",
    "    num_loops=MAX_NUM_LOOPS\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load('models/demo_model_bigger_nn.pt'))\n",
    "model.load_state_dict(torch.load('models/demo_ppo_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_operations = [\n",
    "    (1, 'linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>'),\n",
    "    (4, 'linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>'),\n",
    "    (15, 'linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>'),\n",
    "    (11, 'linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>'),\n",
    "    (0, 'linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<32x64x112x112xf32>'),\n",
    "    (2, 'linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  %cst_1 = arith.constant 0.000000e+00 : f32\\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\\n  %47 = arith.select %46, %in, %cst_1 : f32\\n  linalg.yield %47 : f32\\n} -> tensor<32x64x112x112xf32>'),\n",
    "    (3, 'linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<32x16x52x52xf32>'),\n",
    "    (6, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<10816x120xf32>'),\n",
    "    (8, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\\n^bb0(%in: f32, %in_1: f32, %out: f32):\\n  %46 = arith.addf %in, %in_1 : f32\\n  linalg.yield %46 : f32\\n} -> tensor<32x120xf32>'),\n",
    "    (9, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  %cst_1 = arith.constant 0.000000e+00 : f32\\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\\n  %47 = arith.select %46, %in, %cst_1 : f32\\n  linalg.yield %47 : f32\\n} -> tensor<32x120xf32>'),\n",
    "    (10, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<120x84xf32>'),\n",
    "    (12, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\\n^bb0(%in: f32, %in_1: f32, %out: f32):\\n  %46 = arith.addf %in, %in_1 : f32\\n  linalg.yield %46 : f32\\n} -> tensor<32x84xf32>'),\n",
    "    (13, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  %cst_1 = arith.constant 0.000000e+00 : f32\\n  %46 = arith.cmpf ugt, %in, %cst_1 : f32\\n  %47 = arith.select %46, %in, %cst_1 : f32\\n  linalg.yield %47 : f32\\n} -> tensor<32x84xf32>'),\n",
    "    (14, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<84x10xf32>'),\n",
    "    (16, 'linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\\n^bb0(%in: f32, %in_1: f32, %out: f32):\\n  %46 = arith.addf %in, %in_1 : f32\\n  linalg.yield %46 : f32\\n} -> tensor<32x10xf32>')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation (0): linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2 : tensor<64xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  linalg.yield %in : f32\n",
      "} -> tensor<32x64x112x112xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 28, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 24161533.0\n",
      "New execution time: 40568356\n",
      "speedup: 0.5955758473426924\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (1): linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 28, 14, 0, 7, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 7, 7, 3, 7, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 1, 7, 3, 1, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t True\u001b[0m\n",
      "Base execution time: 13560142681.0\n",
      "New execution time: 2296394794\n",
      "speedup: 5.90497013685531\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (2): linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%13 : tensor<32x64x112x112xf32>) outs(%11 : tensor<32x64x112x112xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  %cst_1 = arith.constant 0.000000e+00 : f32\n",
      "  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n",
      "  %47 = arith.select %46, %in, %cst_1 : f32\n",
      "  linalg.yield %47 : f32\n",
      "} -> tensor<32x64x112x112xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 28, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 24161533.0\n",
      "New execution time: 40478977\n",
      "speedup: 0.5968908996884975\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (3): linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%14, %17 : tensor<32x64x112x112xf32>, tensor<2x2xf32>) outs(%16 : tensor<32x64x56x56xf32>) -> tensor<32x64x56x56xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 8, 0, 2, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 4, 4, 2, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 0, 1, 0, 1, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t True\u001b[0m\n",
      "Base execution time: 72947384.5\n",
      "New execution time: 94561566\n",
      "speedup: 0.7714274158699952\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (4): linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  linalg.yield %in : f32\n",
      "} -> tensor<32x16x52x52xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 4, 26, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace.\n",
      "Stack dump:\n",
      "0.\tProgram arguments: /scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt -loop-invariant-code-motion -cse -canonicalize -cse -eliminate-empty-tensors -empty-tensor-to-alloc-tensor \"-one-shot-bufferize=bufferize-function-boundaries function-boundary-type-conversion=identity-layout-map\" -buffer-deallocation -convert-linalg-to-loops -scf-foreach-thread-lowering -convert-vector-to-scf -convert-scf-to-openmp -canonicalize -lower-affine -expand-strided-metadata -finalize-memref-to-llvm -convert-scf-to-cf -lower-affine -convert-arith-to-llvm -convert-openmp-to-llvm -convert-vector-to-llvm -convert-cf-to-llvm -convert-func-to-llvm -convert-math-to-llvm -reconcile-unrealized-casts /scratch/nb3891/Script/MLIR_RL_2/examples/temp_mlir.mlir\n",
      "1.\tMLIR Parser: custom op parser 'func.func'\u0000\n",
      "2.\tMLIR Parser: custom op parser 'scf.forall'\u0000\n",
      "3.\tMLIR Parser: custom op parser 'vector.extract'\u0000\n",
      " #0 0x0000555556c4569f llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x16f169f)\n",
      " #1 0x0000555556c4214b llvm::sys::RunSignalHandlers() (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x16ee14b)\n",
      " #2 0x0000555556c42295 SignalHandler(int) Signals.cpp:0:0\n",
      " #3 0x000015555511ddd0 __restore_rt sigaction.c:0:0\n",
      " #4 0x000055555c847391 mlir::Lexer::lexToken() (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x72f3391)\n",
      " #5 0x000055555c848561 mlir::detail::Parser::consumeToken(mlir::Token::Kind) (.isra.0) Parser.cpp:0:0\n",
      " #6 0x000055555c84b5b5 mlir::detail::Parser::parseToken(mlir::Token::Kind, llvm::Twine const&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x72f75b5)\n",
      " #7 0x000055555c84b71a mlir::detail::AsmParserImpl<mlir::OpAsmParser>::parseColon() (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x72f771a)\n",
      " #8 0x0000555558c6974f mlir::vector::ExtractOp::parse(mlir::OpAsmParser&, mlir::OperationState&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x371574f)\n",
      " #9 0x000055555c859750 (anonymous namespace)::OperationParser::parseOperation() Parser.cpp:0:0\n",
      "#10 0x000055555c85cba0 (anonymous namespace)::OperationParser::parseRegionBody(mlir::Region&, llvm::SMLoc, llvm::ArrayRef<mlir::OpAsmParser::Argument>, bool) Parser.cpp:0:0\n",
      "#11 0x000055555c85d301 (anonymous namespace)::CustomOpAsmParser::parseRegion(mlir::Region&, llvm::ArrayRef<mlir::OpAsmParser::Argument>, bool) Parser.cpp:0:0\n",
      "#12 0x00005555582b6482 mlir::scf::ForallOp::parse(mlir::OpAsmParser&, mlir::OperationState&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x2d62482)\n",
      "#13 0x000055555c859750 (anonymous namespace)::OperationParser::parseOperation() Parser.cpp:0:0\n",
      "#14 0x000055555c85cba0 (anonymous namespace)::OperationParser::parseRegionBody(mlir::Region&, llvm::SMLoc, llvm::ArrayRef<mlir::OpAsmParser::Argument>, bool) Parser.cpp:0:0\n",
      "#15 0x000055555c85d06f (anonymous namespace)::OperationParser::parseRegion(mlir::Region&, llvm::ArrayRef<mlir::OpAsmParser::Argument>, bool) Parser.cpp:0:0\n",
      "#16 0x000055555c85d10d (anonymous namespace)::CustomOpAsmParser::parseOptionalRegion(mlir::Region&, llvm::ArrayRef<mlir::OpAsmParser::Argument>, bool) Parser.cpp:0:0\n",
      "#17 0x000055555c964d93 mlir::function_interface_impl::parseFunctionOp(mlir::OpAsmParser&, mlir::OperationState&, bool, mlir::StringAttr, llvm::function_ref<mlir::Type (mlir::Builder&, llvm::ArrayRef<mlir::Type>, llvm::ArrayRef<mlir::Type>, mlir::function_interface_impl::VariadicFlag, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>&)>, mlir::StringAttr, mlir::StringAttr) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x7410d93)\n",
      "#18 0x000055555725be93 mlir::func::FuncOp::parse(mlir::OpAsmParser&, mlir::OperationState&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x1d07e93)\n",
      "#19 0x000055555c859750 (anonymous namespace)::OperationParser::parseOperation() Parser.cpp:0:0\n",
      "#20 0x000055555c85b10c mlir::parseAsmSourceFile(llvm::SourceMgr const&, mlir::Block*, mlir::ParserConfig const&, mlir::AsmParserState*, mlir::AsmParserCodeCompleteContext*) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x730710c)\n",
      "#21 0x0000555559800e1f performActions(llvm::raw_ostream&, std::shared_ptr<llvm::SourceMgr> const&, mlir::MLIRContext*, mlir::MlirOptMainConfig const&) MlirOptMain.cpp:0:0\n",
      "#22 0x00005555598032f3 processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::MlirOptMainConfig const&, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0\n",
      "#23 0x0000555559803448 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0\n",
      "#24 0x000055555990ec43 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x43bac43)\n",
      "#25 0x00005555597fc650 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x42a8650)\n",
      "#26 0x0000555559803590 mlir::MlirOptMain(int, char**, llvm::StringRef, llvm::StringRef, mlir::DialectRegistry&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x42af590)\n",
      "#27 0x0000555559803a5d mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x42afa5d)\n",
      "#28 0x0000555556b53f40 main (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x15fff40)\n",
      "#29 0x00001555546036a3 __libc_start_main (/lib64/libc.so.6+0x236a3)\n",
      "#30 0x0000555556c15a16 _start (/scratch/nb3891/Script/MLIR_RL_2/llvm-project/build/bin/mlir-opt+0x16c1a16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m[ERROR]\t EVAL ERROR:no_transformation [0] [('parallelization', [4, 4, 26, 0]), ('no_transformation', [0])]\u001b[0m\n",
      "Base execution time: 1167705.5\n",
      "New execution time: 1167705.5\n",
      "speedup: 1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (5): linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%18, %3 : tensor<32x64x56x56xf32>, tensor<16x64x5x5xf32>) outs(%20 : tensor<32x16x52x52xf32>) -> tensor<32x16x52x52xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 26, 13, 0, 5, 0]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: entry point not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 26, 13, 8, 5, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 1, 13, 8, 1, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t True\u001b[0m\n",
      "Base execution time: 8356379721.0\n",
      "New execution time: 1305524306\n",
      "speedup: 6.400784483747482\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (6): linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21 : tensor<32x16x52x52xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  %cst_1 = arith.constant 0.000000e+00 : f32\n",
      "  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n",
      "  %47 = arith.select %46, %in, %cst_1 : f32\n",
      "  linalg.yield %47 : f32\n",
      "} -> tensor<32x16x52x52xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 26, 13]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 1291713.0\n",
      "New execution time: 2416902\n",
      "speedup: 0.5344498866731047\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (7): linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%22, %17 : tensor<32x16x52x52xf32>, tensor<2x2xf32>) outs(%24 : tensor<32x16x26x26xf32>) -> tensor<32x16x26x26xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 26, 0, 0, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 2, 13, 13, 2, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t tiling [0, 0, 1, 0, 1, 0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t True\u001b[0m\n",
      "Base execution time: 4064276.0\n",
      "New execution time: 4120902\n",
      "speedup: 0.986258833624289\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (8): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%5 : tensor<120x10816xf32>) outs(%26 : tensor<10816x120xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  linalg.yield %in : f32\n",
      "} -> tensor<10816x120xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [0, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 4656410.0\n",
      "New execution time: 741958\n",
      "speedup: 6.2758404114518616\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (9): linalg.matmul ins(%collapsed, %27 : tensor<32x10816xf32>, tensor<10816x120xf32>) outs(%29 : tensor<32x120xf32>) -> tensor<32x120xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 26]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t interchange [0, 2, 1]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t interchange [0, 2, 1]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t interchange [0, 2, 1]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t interchange [0, 2, 1]\u001b[0m\n",
      "Base execution time: 158641869.0\n",
      "New execution time: 25614141\n",
      "speedup: 6.193526810053868\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (10): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%30, %6 : tensor<32x120xf32>, tensor<120xf32>) outs(%28 : tensor<32x120xf32>) {\n",
      "^bb0(%in: f32, %in_1: f32, %out: f32):\n",
      "  %46 = arith.addf %in, %in_1 : f32\n",
      "  linalg.yield %46 : f32\n",
      "} -> tensor<32x120xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 3229.0\n",
      "New execution time: 61766\n",
      "speedup: 0.05227795227147622\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (11): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%31 : tensor<32x120xf32>) outs(%28 : tensor<32x120xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  %cst_1 = arith.constant 0.000000e+00 : f32\n",
      "  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n",
      "  %47 = arith.select %46, %in, %cst_1 : f32\n",
      "  linalg.yield %47 : f32\n",
      "} -> tensor<32x120xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 3024.5\n",
      "New execution time: 110408\n",
      "speedup: 0.02739384827186436\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (12): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%7 : tensor<84x120xf32>) outs(%33 : tensor<120x84xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  linalg.yield %in : f32\n",
      "} -> tensor<120x84xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [0, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 12623.5\n",
      "New execution time: 97553\n",
      "speedup: 0.12940145356882926\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (13): linalg.matmul ins(%32, %34 : tensor<32x120xf32>, tensor<120x84xf32>) outs(%36 : tensor<32x84xf32>) -> tensor<32x84xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 20]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 1080776.0\n",
      "New execution time: 230434\n",
      "speedup: 4.690175928899381\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (14): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%37, %8 : tensor<32x84xf32>, tensor<84xf32>) outs(%35 : tensor<32x84xf32>) {\n",
      "^bb0(%in: f32, %in_1: f32, %out: f32):\n",
      "  %46 = arith.addf %in, %in_1 : f32\n",
      "  linalg.yield %46 : f32\n",
      "} -> tensor<32x84xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 2383.0\n",
      "New execution time: 68940\n",
      "speedup: 0.03456628952712504\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (15): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%38 : tensor<32x84xf32>) outs(%35 : tensor<32x84xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  %cst_1 = arith.constant 0.000000e+00 : f32\n",
      "  %46 = arith.cmpf ugt, %in, %cst_1 : f32\n",
      "  %47 = arith.select %46, %in, %cst_1 : f32\n",
      "  linalg.yield %47 : f32\n",
      "} -> tensor<32x84xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 2294.0\n",
      "New execution time: 55605\n",
      "speedup: 0.041255282798309506\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (16): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1, d0)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%9 : tensor<10x84xf32>) outs(%40 : tensor<84x10xf32>) {\n",
      "^bb0(%in: f32, %out: f32):\n",
      "  linalg.yield %in : f32\n",
      "} -> tensor<84x10xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [0, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 676.5\n",
      "New execution time: 47699\n",
      "speedup: 0.014182687268076899\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (17): linalg.matmul ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2, 14]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 84347.5\n",
      "New execution time: 119355\n",
      "speedup: 0.7066943152779523\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operation (18): linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = [\"parallel\", \"parallel\"]} ins(%44, %10 : tensor<32x10xf32>, tensor<10xf32>) outs(%42 : tensor<32x10xf32>) {\n",
      "^bb0(%in: f32, %in_1: f32, %out: f32):\n",
      "  %46 = arith.addf %in, %in_1 : f32\n",
      "  linalg.yield %46 : f32\n",
      "} -> tensor<32x10xf32>\n",
      "\u001b[92m[SUCCESS]\t parallelization [4, 2]\u001b[0m\n",
      "\u001b[92m[SUCCESS]\t no_transformation [0]\u001b[0m\n",
      "Base execution time: 363.0\n",
      "New execution time: 48451\n",
      "speedup: 0.007492105426100596\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i, operation in benchmark_operations:\n",
    "for i in range(len(env.env.operations_files)):    \n",
    "    \n",
    "    print(f'Operation ({i}):', env.env.operations_files[i][0])\n",
    "    \n",
    "    # Reset the environement with the specific operation\n",
    "    state, obs = env.reset(i)\n",
    "    obs = torch.cat(obs).to(device)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select the action using the model\n",
    "            action, action_log_p, values, entropy = model.sample(obs)\n",
    "\n",
    "        # Apply the action and get the next state\n",
    "        next_obs, reward, terminated, truncated, next_state, final_state = env.step(state, action, model)\n",
    "        \n",
    "        done = terminated[0] or truncated[0]\n",
    "        if done:\n",
    "            final_state = final_state[0]\n",
    "            speedup_metric = final_state.root_exec_time /  final_state.exec_time\n",
    "            print('Base execution time:', final_state.root_exec_time)\n",
    "            print('New execution time:', final_state.exec_time)\n",
    "            print('speedup:', speedup_metric)\n",
    "            break             \n",
    "\n",
    "        state = next_state\n",
    "        obs = torch.cat(next_obs).to(device)\n",
    "\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusion_utils.transforms import *\n",
    "from utils.observation_utils import *\n",
    "from data_generation.data_generation_from_model import transform_wrapper\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base execution time: 0.00132489 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New execution time: 2e-07 ms\n",
      "Speedup: 6624.45\n"
     ]
    }
   ],
   "source": [
    "operation = \"\"\"linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"], tag = \"operation_0\"} ins(%4 : tensor<16xf32>) outs(%19 : tensor<32x16x52x52xf32>) {\\n^bb0(%in: f32, %out: f32):\\n  linalg.yield %in : f32\\n} -> tensor<32x16x52x52xf32>\"\"\"\n",
    "\n",
    "code = transform_wrapper(operation)\n",
    "\n",
    "old_exec_time = evaluate_code(code)\n",
    "print('Base execution time:', old_exec_time / 1e9, 'ms')\n",
    "\n",
    "new_code = code\n",
    "new_code = transform_dialect_TP(new_code, 'operation_0', [4, 4, 0, 0])\n",
    "new_code = transform_dialect_tile(new_code, 'operation_0', [2, 16, 4, 4])\n",
    "new_code = transform_dialect_vectorise(new_code, 'operation_0')\n",
    "\n",
    "\n",
    "new_exec_time = evaluate_code(new_code)\n",
    "print('New execution time:', new_exec_time / 1e9, 'ms')\n",
    "\n",
    "speedup = old_exec_time / new_exec_time\n",
    "print('Speedup:', speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base execution time: 0.000108674 ms\n",
      "New execution time: 1.3e-07 ms\n",
      "Speedup: 835.9538461538461\n"
     ]
    }
   ],
   "source": [
    "operation = \"\"\"linalg.matmul {tag = \"operation_0\"} ins(%39, %41 : tensor<32x84xf32>, tensor<84x10xf32>) outs(%43 : tensor<32x10xf32>) -> tensor<32x10xf32>\"\"\"\n",
    "\n",
    "code = transform_wrapper(operation)\n",
    "\n",
    "old_exec_time = evaluate_code(code)\n",
    "print('Base execution time:', old_exec_time / 1e9, 'ms')\n",
    "\n",
    "new_code = code\n",
    "new_code = transform_dialect_TP(new_code, 'operation_0', [4, 5, 0])\n",
    "new_code = transform_dialect_interchange(new_code, 'operation_0', [1, 0, 2])\n",
    "new_code = transform_dialect_vectorise(new_code, 'operation_0')\n",
    "\n",
    "\n",
    "new_exec_time = evaluate_code(new_code)\n",
    "print('New execution time:', new_exec_time / 1e9, 'ms')\n",
    "\n",
    "speedup = old_exec_time / new_exec_time\n",
    "print('Speedup:', speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base execution time: 17.573441457 ms\n",
      "New execution time: 2.590573458 ms\n",
      "Speedup: 6.78361055647008\n"
     ]
    }
   ],
   "source": [
    "operation = \"\"\"linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>, tag = \"operation_0\"} ins(%0, %1 : tensor<32x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%12 : tensor<32x64x112x112xf32>) -> tensor<32x64x112x112xf32>\"\"\"\n",
    "\n",
    "code = transform_wrapper(operation)\n",
    "\n",
    "old_exec_time = evaluate_code_2(code)\n",
    "print('Base execution time:', old_exec_time / 1e9, 'ms')\n",
    "\n",
    "new_code = code\n",
    "\n",
    "new_code = transform_dialect_TP(new_code, 'operation_0', [4, 8, 0, 0, 0, 0, 0])\n",
    "new_code = transform_dialect_tile(new_code, 'operation_0', [2, 8, 0, 0, 3, 7, 7])\n",
    "new_code = transform_dialect_tile(new_code, 'operation_0', [2, 0, 1, 0, 0, 1, 0])\n",
    "new_code = apply_conv2d_decomposition(new_code, 'operation_0')\n",
    "new_code = transform_dialect_vectorise(new_code, 'operation_0')\n",
    "\n",
    "new_exec_time = evaluate_code_2(new_code)\n",
    "print('New execution time:', new_exec_time / 1e9, 'ms')\n",
    "\n",
    "speedup = old_exec_time / new_exec_time\n",
    "print('Speedup:', speedup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
